{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9-ZzQhCL-Lt"
      },
      "source": [
        "# Computer vision project\n",
        "\n",
        "- Carlotta Anna Maria Ciani 1881291\n",
        "- Michela Fuselli 1883535\n",
        "- Simone Federico Laganà 1946083\n",
        "\n",
        "Project 4 car plate recognition\n",
        "\n",
        "Folder structure in this notebook\n",
        " -  imports: contains the imports and the necessary libaries for executing the whole code\n",
        " - globals: it includes the global variables\n",
        " - utils: contains various functions as well as the definition of the evaluator class, and the function to perform baseline plate detection\n",
        " - network: defines the networks for PDLPR and the CNN CTC network\n",
        " - data: defining the dataset class\n",
        " - train: this is divided into the training code for YOLO, for PDLPR and for CNN + CTC under the name Baseline method where there is also the implementation fo the traditional plate detection method.\n",
        " - test: Here there is the testing for the single components YOLO, PDLPR and CNN + CTC as well as the pipeline for yolo + pdlpr and the baseline pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRoLLAHCMhTG"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "z-7qkQbJXRL0",
        "outputId": "57e308af-8136-4790-80e0-7f7a4dd69679"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ultralytics in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (8.3.169)\n",
            "Requirement already satisfied: numpy>=1.23.0 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ultralytics) (3.9.2)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ultralytics) (11.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ultralytics) (1.15.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ultralytics) (2.5.1)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ultralytics) (0.20.1)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ultralytics) (4.67.0)\n",
            "Requirement already satisfied: psutil in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ultralytics) (6.1.0)\n",
            "Requirement already satisfied: py-cpuinfo in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ultralytics) (2.2.3)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ultralytics) (2.0.14)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: filelock in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: networkx in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: easyocr in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.7.2)\n",
            "Requirement already satisfied: torch in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from easyocr) (2.5.1)\n",
            "Requirement already satisfied: torchvision>=0.5 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from easyocr) (0.20.1)\n",
            "Requirement already satisfied: opencv-python-headless in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from easyocr) (4.12.0.88)\n",
            "Requirement already satisfied: scipy in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from easyocr) (1.15.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from easyocr) (2.2.2)\n",
            "Requirement already satisfied: Pillow in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from easyocr) (11.0.0)\n",
            "Requirement already satisfied: scikit-image in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from easyocr) (0.25.1)\n",
            "Requirement already satisfied: python-bidi in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from easyocr) (0.6.6)\n",
            "Requirement already satisfied: PyYAML in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from easyocr) (6.0.2)\n",
            "Requirement already satisfied: Shapely in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from easyocr) (2.1.1)\n",
            "Requirement already satisfied: pyclipper in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from easyocr) (1.3.0.post6)\n",
            "Requirement already satisfied: ninja in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from easyocr) (1.11.1.4)\n",
            "Requirement already satisfied: filelock in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->easyocr) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->easyocr) (4.12.2)\n",
            "Requirement already satisfied: networkx in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->easyocr) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->easyocr) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->easyocr) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->easyocr) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy==1.13.1->torch->easyocr) (1.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-image->easyocr) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-image->easyocr) (2025.1.10)\n",
            "Requirement already satisfied: packaging>=21 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-image->easyocr) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-image->easyocr) (0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch->easyocr) (3.0.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gdown) (4.67.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests[socks]->gdown) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\carlo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->gdown) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install ultralytics\n",
        "%pip install easyocr\n",
        "%pip install gdown "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TWWIQ0MMiuN",
        "outputId": "3f01d2aa-9783-4da9-8168-df355ec63110"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1PR8ygH66VKKDOaFpoxzR7aLqc_H5VUtC\n",
            "From (redirected): https://drive.google.com/uc?id=1PR8ygH66VKKDOaFpoxzR7aLqc_H5VUtC&confirm=t&uuid=6c481d8d-77f3-4398-b609-5a54032eb986\n",
            "To: c:\\Users\\carlo\\Desktop\\CV_project_2\\CV_project\\models\\pdlpr_10_0.0001_16.pt\n",
            "100%|██████████| 129M/129M [00:16<00:00, 7.93MB/s] \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'models/pdlpr_10_0.0001_16.pt'"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#from google.colab import drive\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import torch.optim as optim\n",
        "from torch.optim import Adam, SGD\n",
        "\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from ultralytics import YOLO\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import easyocr\n",
        "import json\n",
        "import gdown\n",
        "\n",
        "#drive.mount('/content/drive')\n",
        "#path_to_shared_folder = '/content/drive/MyDrive/cv_project_folder/'\n",
        "\n",
        "#download the best pdlpr model from google drive, since the file is too big to be stored on the repository\n",
        "url = \"https://drive.google.com/uc?id=1PR8ygH66VKKDOaFpoxzR7aLqc_H5VUtC\"\n",
        "output = \"models/pdlpr_10_0.0001_16.pt\"\n",
        "gdown.download(url, output, quiet=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7_eAV_pMhbO"
      },
      "source": [
        "## Globals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnPEcperJPWq"
      },
      "source": [
        "initialization of all the global variables used in this notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "CiORVTBjMjX2"
      },
      "outputs": [],
      "source": [
        "#General\n",
        "\n",
        "#character mapping used for the chinese plates\n",
        "PROVINCES = [\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\", \"O\"]\n",
        "ALPHABETS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'O']\n",
        "ADS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n",
        "\n",
        "#sorted list of all the unique characters that could appear in a plate\n",
        "CHAR_LIST = sorted(set(PROVINCES + ALPHABETS +ADS))\n",
        "\n",
        "#dictionary containing the character and the corresponding index\n",
        "CHAR_IDX = {}\n",
        "IDX_CHAR = {}\n",
        "for idx, char in enumerate(CHAR_LIST):\n",
        "    CHAR_IDX[char] = idx + 1  # start from 1\n",
        "    IDX_CHAR[idx + 1] = char\n",
        "IDX_CHAR[0] = '_'  # blank character for CTC\n",
        "\n",
        "DATASET_PATH_Y      = f\"dataset\"\n",
        "IMAGE_SIZE_Y        = 640\n",
        "\n",
        "#YOLOv5\n",
        "IOU_THRESHOLD = 0.7\n",
        "BATCH_SIZE_TRAIN_Y  = 20        # paper:50\n",
        "BATCH_SIZE_TEST_Y   = 4\n",
        "EPOCHS_TRAIN_Y      = 25        # paper: 300\n",
        "LR_INIT_Y           = 0.001     # initial learning rate\n",
        "\n",
        "\n",
        "#PDLPR\n",
        "BATCH_SIZE_PDLPR = 16\n",
        "LR_PDLPR = 1e-4 #0.00001, mostly used\n",
        "NUM_EPOCHS_PDLPR = 5\n",
        "WEIGHT_DECAY_PDLPR = 0.0001\n",
        "\n",
        "#CNN ctc\n",
        "#(here there are the hyperparameters with the best performances among the ones tried)\n",
        "BATCH_SIZE_CNN = 32\n",
        "LR_CNN = 0.001\n",
        "NUM_EPOCHS_CNN = 60\n",
        "WEIGHT_DECAY_CNN = 0.0001\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNFOfY6GMhfe"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVydzOv9YmT9"
      },
      "source": [
        "### useful functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "qPC1-L-fMj3t"
      },
      "outputs": [],
      "source": [
        "base_dir = Path(DATASET_PATH_Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e9SEG4d1vcZ"
      },
      "source": [
        "This code extracts the labels and puts them into folders, structured in a similar way of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "anTWin7gX6sW"
      },
      "outputs": [],
      "source": [
        "def initialize_labels():\n",
        "    #initialize the labels and creating the folders\n",
        "    splits = [\"train\", \"val\", \"test\"]\n",
        "    for split in splits:\n",
        "        # It is just a safe and readable way to say: go to datasets/ccpd/images/train (or val, or test), depending on which split you're processing.\n",
        "        image_dir = base_dir / \"images\" / split\n",
        "        label_dir = base_dir / \"labels\" / split\n",
        "        crops_dir = base_dir / \"crops\" / split\n",
        "        label_pdlpr_dir = base_dir / \"labels_pdlpr\" / split\n",
        "\n",
        "        label_dir.mkdir(parents=True, exist_ok=True)    # creates the folder if it does not exist\n",
        "        crops_dir.mkdir(parents=True, exist_ok=True)\n",
        "        label_pdlpr_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Loop through all .jpg images in the current image directory\n",
        "        #the tqdm library is useful to plot the loading bar\n",
        "        for image_path in tqdm(list(image_dir.glob(\"*.jpg\")), desc=f\"Processing - {split}\", unit=\"img\"):\n",
        "            #print(f\"Found image: {image_path}\")\n",
        "            #print(f\"Processing: {image_path.name}\")\n",
        "\n",
        "            # Parse bounding box from filename: example => \"XXXXX&x1_x2_y1_y2&...\"\n",
        "            try:\n",
        "                fields = image_path.stem.split(\"-\")    # image_path.stem is the filename without .jpg\n",
        "\n",
        "                # Field 2 (index 2) is bbox: format is \"x1&y1_x2&y2\"\n",
        "                bbox_part = fields[2]\n",
        "                corners = bbox_part.split(\"_\")\n",
        "                x1, y1 = map(int, corners[0].split(\"&\"))\n",
        "                x2, y2 = map(int, corners[1].split(\"&\"))\n",
        "\n",
        "                # Define min/max values\n",
        "                x_min = min(x1, x2)\n",
        "                x_max = max(x1, x2)\n",
        "                y_min = min(y1, y2)\n",
        "                y_max = max(y1, y2)\n",
        "\n",
        "\n",
        "                #extracting the information about the plate to create the labels for pdlpr\n",
        "                #the plate is in this format 0_0_22_27_27_33_16\n",
        "                plate_number = fields[4]\n",
        "                character_id_list = plate_number.split(\"_\")\n",
        "                #get the number for the province and for the letter\n",
        "                province_id = int(character_id_list[0])\n",
        "                alphabet_id = int(character_id_list[1])\n",
        "                #get the actual character for both and join them\n",
        "                province_char = PROVINCES[province_id]\n",
        "                alphabet_char = ALPHABETS[alphabet_id]\n",
        "                plate = province_char + alphabet_char\n",
        "\n",
        "                for i in range(2, 8):\n",
        "                    #for the remaining 5 characters we do the mapping from the ADS\n",
        "                    ads_index = int(character_id_list[i])\n",
        "                    plate += ADS[ads_index]\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping {image_path.name}: {e}\")\n",
        "                continue\n",
        "            # Read the image to get image size (needed to normalize the coordinates)\n",
        "            img = Image.open(image_path)\n",
        "\n",
        "            img_width, img_height = img.size\n",
        "\n",
        "            #crop the image according to the bounding box coordinates\n",
        "            cropped_img = img.crop((x_min, y_min, x_max, y_max))\n",
        "\n",
        "            #Adding crops so cut images into a separate folder\n",
        "            crops_path = crops_dir / (image_path.stem + \".jpg\")\n",
        "\n",
        "            #saving the image into the crops folder\n",
        "            cropped_img.save(crops_path)\n",
        "\n",
        "            img.close()\n",
        "\n",
        "\n",
        "            # Normalize the bounding box for YOLO format\n",
        "            x_center = ((x_min + x_max) / 2) / img_width\n",
        "            y_center = ((y_min + y_max) / 2) / img_height\n",
        "            width = (x_max - x_min) / img_width\n",
        "            height = (y_max - y_min) / img_height\n",
        "\n",
        "            # Create YOLO label string\n",
        "            # 0 is the class ID (only one class - license plate)\n",
        "            # the rest are floats with 6 digits after the decimal point\n",
        "            label_str = f\"0 {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\"\n",
        "\n",
        "            # Save label file with same name\n",
        "            label_path = label_dir / (image_path.stem + \".txt\")\n",
        "            with open(label_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(label_str + \"\\n\")\n",
        "\n",
        "            #print(f\"Wrote label: {label_path.name}\")\n",
        "\n",
        "            #Save the label for PDLPR\n",
        "            label_pdl_pr_path = label_pdlpr_dir / (image_path.stem + \".txt\")\n",
        "            with open(label_pdl_pr_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(plate + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fYO-ULQJxmC"
      },
      "source": [
        "This are some useful functions that are used in this notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "GIoX3jszSEjk"
      },
      "outputs": [],
      "source": [
        "def yoloprediction_to_pdlpr_input(x_center, y_center, width, height, image_path):\n",
        "    #This functions takes in input the prediction from yolo and returns the cropped image (so the input for pdlpr)\n",
        "    img = Image.open(image_path)\n",
        "\n",
        "    image_width, image_height = img.size\n",
        "\n",
        "    x_center_pixel = x_center * image_width\n",
        "    y_center_pixel = y_center * image_height\n",
        "    width_pixel = width * image_width\n",
        "    height_pixel = height * image_height\n",
        "\n",
        "    x_min = int(x_center_pixel - width_pixel / 2)\n",
        "    x_max = int(x_center_pixel + width_pixel / 2)\n",
        "    y_min = int(y_center_pixel - height_pixel / 2)\n",
        "    y_max = int(y_center_pixel + height_pixel / 2)\n",
        "\n",
        "    #crop the image according to the bounding box coordinates\n",
        "    cropped_img = img.crop((x_min, y_min, x_max, y_max))\n",
        "\n",
        "    return cropped_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXsatw_VSHsD"
      },
      "source": [
        "This function computes the intersection over union given two boxes [x1, y1, x2, y2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "oYE9zft7SIME"
      },
      "outputs": [],
      "source": [
        "def compute_iou(box_1, box_2):\n",
        "    #it is a metric that involves the intersection of the two areas\n",
        "    #over the union, and returns a matching percentage\n",
        "\n",
        "    #coputing the coordinate of the intersections\n",
        "    x1 = max(box_1[0], box_2[0])\n",
        "    y1 = max(box_1[1], box_2[1])\n",
        "    x2 = min(box_1[2], box_2[2])\n",
        "    y2 = min(box_1[3], box_2[3])\n",
        "\n",
        "    interArea = max(0, x2 - x1) * max(0, y2 - y1)\n",
        "    boxAArea = (box_1[2] - box_1[0]) * (box_1[3] - box_1[1])\n",
        "    boxBArea = (box_2[2] - box_2[0]) * (box_2[3] - box_2[1])\n",
        "\n",
        "    iou = interArea / float(boxAArea + boxBArea - interArea + 1e-6)  #+1e-6 is used to avoid the division per zero\n",
        "    return iou"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bWuwEWaStOk"
      },
      "source": [
        "this function converts the id of the target into the ids that are returned by the model since it uses the ids from the sorted list of all the possible characters.\n",
        "\n",
        "index_to_target converts the index list into the plate characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "G3kkl-epS68s"
      },
      "outputs": [],
      "source": [
        "def target_to_index(target_list):\n",
        "    #this function converts the id of the target\n",
        "    #into the ids that are returned by the model\n",
        "    #since it uses the ids from the sorted list of\n",
        "    #all the possible characters\n",
        "    output = []\n",
        "    province = PROVINCES[target_list[0]]\n",
        "    alphabet = ALPHABETS[target_list[1]]\n",
        "    output.append(CHAR_IDX[province])\n",
        "    output.append(CHAR_IDX[alphabet])\n",
        "    for char_idx in range(2,8):\n",
        "        char = ADS[target_list[char_idx]]\n",
        "        output.append(CHAR_IDX[char])\n",
        "    return output\n",
        "\n",
        "\n",
        "def index_to_target(index_list):\n",
        "    output=[]\n",
        "    for idx in index_list:\n",
        "        output.append(IDX_CHAR[idx])\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIlNhOinS-Js"
      },
      "source": [
        "These funcitions aim at retrieving the coordinates of the true bounding box (ground truth), for both validation and testing. The format returned is [x1, y1, x2, y2], if not label is found or it is invalid, None is returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "l9cp_DvP2BQD"
      },
      "outputs": [],
      "source": [
        "def load_gt_box_from_label_validation(image_path):\n",
        "    label_path = Path(f\"dataset/labels/val\") / (image_path.stem + \".txt\")\n",
        "\n",
        "    if not label_path.exists():\n",
        "        print(f\"[WARN] No label found for {image_path.name}\")\n",
        "        return None\n",
        "\n",
        "    with open(label_path, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    if not lines:\n",
        "        print(f\"[WARN] Empty label file for {image_path.name}\")\n",
        "        return None\n",
        "\n",
        "    # Assume the first object only\n",
        "    try:\n",
        "        parts = list(map(float, lines[0].strip().split()))\n",
        "        _, x_center, y_center, w, h = parts\n",
        "    except Exception:\n",
        "        print(f\"[WARN] Label parse failed for {image_path.name}\")\n",
        "        return None\n",
        "\n",
        "    # Convert from normalized to absolute coordinates\n",
        "    img = plt.imread(image_path)\n",
        "    img_h, img_w = img.shape[:2]\n",
        "\n",
        "    cx, cy = x_center * img_w, y_center * img_h\n",
        "    bw, bh = w * img_w, h * img_h\n",
        "\n",
        "    x1, y1 = cx - bw / 2, cy - bh / 2\n",
        "    x2, y2 = cx + bw / 2, cy + bh / 2\n",
        "\n",
        "    return [x1, y1, x2, y2]\n",
        "\n",
        "\n",
        "def load_gt_box_from_label_test(image_path):\n",
        "    label_path = Path(f\"dataset/labels/test\") / (image_path.stem + \".txt\")\n",
        "\n",
        "    if not label_path.exists():\n",
        "        print(f\"[WARN] No label found for {image_path.name}\")\n",
        "        return None\n",
        "\n",
        "    with open(label_path, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    if not lines:\n",
        "        print(f\"[WARN] Empty label file for {image_path.name}\")\n",
        "        return None\n",
        "\n",
        "    # Assume the first object only\n",
        "    try:\n",
        "        parts = list(map(float, lines[0].strip().split()))\n",
        "        _, x_center, y_center, w, h = parts\n",
        "    except Exception:\n",
        "        print(f\"[WARN] Label parse failed for {image_path.name}\")\n",
        "        return None\n",
        "\n",
        "    # Convert from normalized to absolute coordinates\n",
        "    img = plt.imread(image_path)\n",
        "    img_h, img_w = img.shape[:2]\n",
        "\n",
        "    cx, cy = x_center * img_w, y_center * img_h\n",
        "    bw, bh = w * img_w, h * img_h\n",
        "\n",
        "    x1, y1 = cx - bw / 2, cy - bh / 2\n",
        "    x2, y2 = cx + bw / 2, cy + bh / 2\n",
        "\n",
        "    return [x1, y1, x2, y2]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWNTB7XCh6V2"
      },
      "source": [
        "function that builds the vocabulary (chinese regions) that will be used to build idx2char and char2idx, in particular the inputs are **label_folder** (str), which is the path to folder containing license plate label .txt files, **file_name** (str) is the file .json that will contain the vocabulary for later use and **include_blank** (bool) that tells whether to reserve index 0 for the CTC blank token ('-')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "wBGwecxWh4-D"
      },
      "outputs": [],
      "source": [
        "def build_vocab(label_folder, file_name, include_blank=True):\n",
        "    vocab = set()\n",
        "\n",
        "    for filename in os.listdir(label_folder):\n",
        "        if not filename.endswith(\".txt\"):\n",
        "            continue\n",
        "        with open(os.path.join(label_folder, filename), \"r\", encoding=\"utf-8\") as f:\n",
        "            label = f.read().strip().upper()\n",
        "            vocab.update(label)\n",
        "\n",
        "    # doing sorting for consinstency\n",
        "    vocab = sorted(vocab)\n",
        "    char2idx = {}\n",
        "    idx2char = {}\n",
        "    start_idx = 0\n",
        "\n",
        "    if include_blank:\n",
        "        char2idx[\"-\"] = 0  # CTC blank\n",
        "        idx2char[0] = \"-\"\n",
        "        start_idx = 1\n",
        "\n",
        "    for i, ch in enumerate(vocab, start=start_idx):\n",
        "        char2idx[ch] = i\n",
        "        idx2char[i] = ch\n",
        "\n",
        "    with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(char2idx, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"[vocab] Built vocabulary with {len(char2idx)} characters.\")\n",
        "    return char2idx, idx2char"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPsGMsn2zcqH"
      },
      "source": [
        "Function that loads the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "MFwXxgEJzbqB"
      },
      "outputs": [],
      "source": [
        "def load_vocab(path=\"vocab.json\"):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        char_idx = json.load(f)\n",
        "    idx_char = {int(v): k for k, v in char_idx.items()}\n",
        "    return char_idx, idx_char"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XES82yIgzcDk"
      },
      "source": [
        "Function that plots PDLPR metrics comparing training and validation, in particular the metrics are: character accuracy, sequence accuracy and levenshtein distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "6HnkwwZdzpTG"
      },
      "outputs": [],
      "source": [
        "def plot_metrics(train_seq, val_seq, train_char, val_char, train_lev, val_lev):\n",
        "    epochs = range(1, NUM_EPOCHS_PDLPR + 1)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, [l.detach().cpu().item() if torch.is_tensor(l) else l for l in train_seq], label=\"Train Seq Accuracy\")\n",
        "    plt.plot(epochs, [l.detach().cpu().item() if torch.is_tensor(l) else l for l in val_seq], label=\"Val Seq Accuracy\")\n",
        "    plt.title(\"Sequence Accuracy over Epochs\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(f\"metrics_images/seq_accs_plot_{NUM_EPOCHS_PDLPR}_{LR_PDLPR}_{BATCH_SIZE_PDLPR}.png\", dpi=300)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, [l.detach().cpu().item() if torch.is_tensor(l) else l for l in train_char], label=\"Train Char Accuracy\")\n",
        "    plt.plot(epochs, [l.detach().cpu().item() if torch.is_tensor(l) else l for l in val_char], label=\"Val Char Accuracy\")\n",
        "    plt.title(\"Char Accuracy over Epochs\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(f\"metrics_images/char_accs_plot{NUM_EPOCHS_PDLPR}_{LR_PDLPR}_{BATCH_SIZE_PDLPR}.png\", dpi=300)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, [l.detach().cpu().item() if torch.is_tensor(l) else l for l in train_lev], label=\"Train Levenshtein distance\")\n",
        "    plt.plot(epochs, [l.detach().cpu().item() if torch.is_tensor(l) else l for l in val_lev], label=\"Val Levenshtein distance\")\n",
        "    plt.title(\"Levenshtein distance over Epochs\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Lev distance\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(f\"metrics_images/levenshtein_plot{NUM_EPOCHS_PDLPR}_{LR_PDLPR}_{BATCH_SIZE_PDLPR}.png\", dpi=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGwGO7U4zboN"
      },
      "source": [
        "Functions for processing the batches returned from the dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "DJizNMT6za49"
      },
      "outputs": [],
      "source": [
        "# functions for pdlpr\n",
        "def custom_collate_simple(batch):\n",
        "    return batch\n",
        "\n",
        "def custom_collate(batch):\n",
        "    return {\n",
        "        \"cropped_image\": torch.stack([item[\"cropped_image\"] for item in batch]),\n",
        "        \"pdlpr_plate_string\": [item[\"pdlpr_plate_string\"] for item in batch],\n",
        "        # add other fields as needed\n",
        "    }\n",
        "\n",
        "def custom_collate_2(batch):\n",
        "    return {\n",
        "        \"cropped_image\": torch.stack([item[\"cropped_image\"] for item in batch]),\n",
        "        \"pdlpr_plate_idx\": [item[\"pdlpr_plate_idx\"] for item in batch],\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_PZ6aiSU_Jk"
      },
      "source": [
        "### Evaluator class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WC-HsHUYVDmU"
      },
      "source": [
        "Evaluator class: this class will help in computing the metrics to evaluate the models (char accuracy, sequence accuracy and lev distance). Since there are a lot of variables to look after, we found more convenient to impement a class.\n",
        "\n",
        "There are two different versions of the same methods, greedy_decode/greedy_decoede_idx  and update/update_baseline. This because the two models produce slightly different outputs so they need different treatment to decode and when it comes to compare the predictions to the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "M-CVjjooU-Al"
      },
      "outputs": [],
      "source": [
        "class Evaluator:\n",
        "    def __init__(self, idx2char={}, blank_index=0):\n",
        "        self.idx2char = idx2char\n",
        "        self.blank_index = blank_index\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.total_chars = 0\n",
        "        self.correct_chars = 0\n",
        "        self.correct_seqs = 0\n",
        "        self.total_samples = 0\n",
        "\n",
        "    def greedy_decode(self, logits):\n",
        "        # logits: [B, T, C]\n",
        "        predictions = torch.argmax(logits, dim=-1)  # [B, T]\n",
        "        decoded = []\n",
        "\n",
        "        for prediction in predictions:\n",
        "            prev = self.blank_index\n",
        "            chars = []\n",
        "            for idx in prediction:\n",
        "                idx = idx.item()\n",
        "                if idx != self.blank_index and idx != prev:\n",
        "                    chars.append(self.idx2char[idx])\n",
        "                prev = idx\n",
        "            decoded.append(\"\".join(chars))\n",
        "        return decoded\n",
        "\n",
        "    def greedy_decode_idx(self, logits):\n",
        "        predictions = torch.argmax(logits, dim=2)\n",
        "        predictions= predictions.transpose(0, 1)\n",
        "        final_predictions = []\n",
        "        #iterate for each prediction array in the batch\n",
        "        for prediction in predictions:\n",
        "            before = 0\n",
        "            reduced = []\n",
        "            for t_index in prediction:\n",
        "                t_index = t_index.item()\n",
        "                if t_index != 0 and t_index != before:\n",
        "                    #append the index only if it is not zero and it is different than before\n",
        "                    reduced.append(t_index)\n",
        "                before = t_index\n",
        "            final_predictions.append(reduced)\n",
        "        return final_predictions\n",
        "\n",
        "    def update(self, logits, target_strs):\n",
        "        # logits: [B, T, vocab_size]\n",
        "        pred_strs = self.greedy_decode(logits)\n",
        "\n",
        "        for pred, true in zip(pred_strs, target_strs):\n",
        "            self.total_samples += 1\n",
        "            self.total_chars += len(true)\n",
        "            correct = sum(p == t for p, t in zip(pred, true))\n",
        "            self.correct_chars += correct\n",
        "            if pred == true:\n",
        "                self.correct_seqs += 1\n",
        "\n",
        "    def update_baseline(self, logits, labels):\n",
        "\n",
        "        final_predictions = self.greedy_decode_idx(logits)\n",
        "        for pred_idx_list, label in zip(final_predictions, labels):\n",
        "            label_list = label.tolist()\n",
        "            if pred_idx_list == label_list:\n",
        "                self.correct_seqs +=1\n",
        "\n",
        "            self.total_samples += 1\n",
        "            self.total_chars += len(label)\n",
        "            correct = 0\n",
        "            for pred_idx, label_idx in zip(pred_idx_list, label):\n",
        "                if pred_idx == label_idx:\n",
        "                    correct += 1\n",
        "            self.correct_chars += correct\n",
        "\n",
        "    def compute(self):\n",
        "        char_acc = self.correct_chars / self.total_chars if self.total_chars > 0 else 0.0\n",
        "        seq_acc = self.correct_seqs / self.total_samples if self.total_samples > 0 else 0.0\n",
        "        return {\n",
        "            \"char_accuracy\": char_acc,\n",
        "            \"seq_accuracy\": seq_acc,\n",
        "        }\n",
        "\n",
        "    def print(self):\n",
        "        metrics = self.compute()\n",
        "        print(f\"Character accuracy:  {metrics['char_accuracy']:.4f}\")\n",
        "        print(f\"Sequence accuracy:   {metrics['seq_accuracy']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MERiSC1xedhO"
      },
      "source": [
        "### Function for baseline plate detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUrnobb6ejM-"
      },
      "source": [
        "This function is used in the \"training\" for the plate recognition phase of the baseline methods, it uses traditional techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "NV6JDfiGeh_G"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_label_yolo(label_path):\n",
        "    with open(label_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        line = f.readline().strip()\n",
        "    parts = list(map(float, line.split()))\n",
        "    # It returns x, y, width, height\n",
        "    return torch.tensor(parts[1:], dtype=torch.float32)\n",
        "\n",
        "\n",
        "def get_ground_truth_coordinates(yolo_tensor, image_width, image_height):\n",
        "        cx, cy, w, h = yolo_tensor.tolist()\n",
        "\n",
        "        x_min = (cx - w / 2) * image_width\n",
        "        y_min = (cy - h / 2) * image_height\n",
        "        x_max = (cx + w / 2) * image_width\n",
        "        y_max = (cy + h / 2) * image_height\n",
        "\n",
        "        return [x_min, y_min, x_max, y_max]\n",
        "\n",
        "\n",
        "def plate_detector(image_path, true_coordinates):\n",
        "    # Load the image\n",
        "    img = cv2.imread(str(image_path))\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Isolate green parts (the plates have a black text on a green backgroung)\n",
        "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)              # Convert from RGB to HSV (to filter colours basing on the tone)\n",
        "    lower_green = np.array([40, 40, 40])                    # HSV tone range for green\n",
        "    upper_green = np.array([80, 255, 255])\n",
        "    mask = cv2.inRange(hsv, lower_green, upper_green)       # Create a binary mask\n",
        "\n",
        "    # Morphology\n",
        "    # The edge detector returns multiple fragmented contours,\n",
        "    # I need to use morphological operations to bound together near lines\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n",
        "    cleaned_mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
        "\n",
        "    # Edge detector --> Canny\n",
        "    edges = cv2.Canny(cleaned_mask, 100, 200)\n",
        "\n",
        "    # Found the contours (borders)\n",
        "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # Geometric filter + OCR check to see if there are numbers/letters\n",
        "    # Discard regiorns where there are no plates\n",
        "    candidates = []\n",
        "    for cnt in contours:\n",
        "        x, y, w, h = cv2.boundingRect(cnt)\n",
        "        aspect_ratio = w / float(h)\n",
        "        area = w * h\n",
        "        if not (2.5 < aspect_ratio < 6 and 1000 < area < 40000):\n",
        "            continue\n",
        "\n",
        "        roi = img_rgb[y:y+h, x:x+w]\n",
        "        result = reader.readtext(roi)\n",
        "\n",
        "        if result:\n",
        "            text = result[0][1]\n",
        "            conf = result[0][2]\n",
        "            clean_text = text.strip().replace(\" \", \"\").replace(\"\\n\", \"\")\n",
        "        else:\n",
        "            clean_text = \"\"\n",
        "            conf = 0.0\n",
        "\n",
        "        iou = compute_iou([x, y, x+w, y+h], true_coordinates)\n",
        "        ocr_score = len(clean_text) if len(clean_text) >= 4 else 0\n",
        "\n",
        "        # score = iou + 1 * ocr_score     # OCR weights more\n",
        "        score = 1.5 * (ocr_score / 8.0) + 0.5 * iou     # OCR weights more but I still consider iou\n",
        "\n",
        "        candidates.append({\n",
        "            \"bbox\": [x, y, x+w, y+h],\n",
        "            \"text\": clean_text,\n",
        "            \"score\": score\n",
        "        })\n",
        "\n",
        "    if not candidates:\n",
        "        return None\n",
        "\n",
        "    best = max(candidates, key=lambda c: c[\"score\"])\n",
        "    return best[\"bbox\"], best[\"text\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mR_Eir2Mhju"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3KrWDAZH8Gx"
      },
      "source": [
        "Definition of the dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "kYNQqdkrMkWd"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CCPDDataset(Dataset):\n",
        "    #This class helps to manage the elements from the CCPDD dataset\n",
        "    #and also initialized the batchloaders used during the training test and validation phases\n",
        "\n",
        "    def __init__(self, base_dir, transform=None):\n",
        "\n",
        "        self.base_dir = Path(base_dir)\n",
        "        self.transform = transform\n",
        "\n",
        "    def get_dataset(self, split):\n",
        "        #this is used to set the split from which we are initializing the dataset\n",
        "        #like train, test, val\n",
        "\n",
        "        self.image_dir = self.base_dir / \"images\" / split\n",
        "        self.label_yolo_dir = self.base_dir / \"labels\" / split\n",
        "        #directories for the labels and cropped images that are going to be used\n",
        "        #in the character recognition part with pdlpr\n",
        "        self.crops_dir = self.base_dir / \"crops\" / split\n",
        "        self.label_pdlpr_dir = self.base_dir / \"labels_pdlpr\" / split\n",
        "\n",
        "        # List all image files in the current split's image directory\n",
        "        self.image_files = sorted(list(self.image_dir.glob(\"*.jpg\")))\n",
        "\n",
        "        # List the cropped image files if PDLPR needs them directly\n",
        "        self.cropped_image_files = sorted(list(self.crops_dir.glob(\"*.jpg\")))\n",
        "\n",
        "        # Basic validation to ensure files exist\n",
        "        if not self.image_files:\n",
        "            raise FileNotFoundError(f\"No .jpg images found in {self.image_dir}\")\n",
        "        if not self.cropped_image_files:\n",
        "            raise FileNotFoundError(f\"No cropped .jpg images found in {self.crops_dir}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        # Returns the total number of samples in the dataset\n",
        "        return len(self.image_files)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Retrieves a single data sample given the index\n",
        "\n",
        "        #initializes the paths\n",
        "\n",
        "        img_path = self.image_files[index]\n",
        "        yolo_label_path = self.label_yolo_dir / (img_path.stem + \".txt\")\n",
        "        cropped_img_path = self.cropped_image_files[index]\n",
        "        pdlpr_label_path = self.label_pdlpr_dir / (img_path.stem + \".txt\")\n",
        "\n",
        "        img_name = img_path.name\n",
        "\n",
        "        # Open images\n",
        "        # Ensure 'RGB' conversion if images might be grayscale to be consistent for models\n",
        "        # This helps with GPU optimization as models typically expect 3 channels\n",
        "        full_image = Image.open(img_path).convert(\"RGB\")\n",
        "        cropped_image = Image.open(cropped_img_path).convert(\"RGB\")\n",
        "\n",
        "        # Read YOLO label (bounding box) from the text file\n",
        "        with open(yolo_label_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            yolo_label_str = f.readline().strip()\n",
        "\n",
        "        # Check if the label file is empty or malformed\n",
        "        if not yolo_label_str:\n",
        "            raise ValueError(f\"Empty label in {yolo_label_path}\")\n",
        "\n",
        "        # Assuming YOLO format: \"class_id x_center y_center width height\"\n",
        "        # We only have one class (0), so we can discard it or keep it\n",
        "\n",
        "        parts = list(map(float, yolo_label_str.split()))\n",
        "        # parts is a list of floats like [0.0, 0.5, 0.4, 0.3, 0.1]\n",
        "        class_id = int(parts[0])\n",
        "        # discard the first element (class) --> [x_center, y_center, width, height]\n",
        "        yolo_bbox = torch.tensor(parts[1:], dtype=torch.float32)\n",
        "        # convert the list of floats into a PyTorch tensor\n",
        "\n",
        "        # Read PDLPR label (license plate string)\n",
        "        with open(pdlpr_label_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            pdlpr_plate_str = f.readline().strip()\n",
        "\n",
        "        #Extracting the pdlpr index label that is going to be used by\n",
        "        #the CNNCTC model\n",
        "        fields = img_path.name.split(\"-\")\n",
        "        plate_number = fields[4]\n",
        "        character_id_list = plate_number.split(\"_\")\n",
        "        plate_id = []\n",
        "        for c in character_id_list:\n",
        "            plate_id.append(int(c))\n",
        "\n",
        "        #converting the index from the name to the index from the\n",
        "        #unified vocabulary\n",
        "        plate_id= target_to_index(plate_id)\n",
        "\n",
        "        pdlpr_label_idx = torch.tensor(plate_id, dtype=torch.long)\n",
        "\n",
        "        # apply the transformations\n",
        "        if self.transform:\n",
        "            full_image_original = full_image\n",
        "            full_image = self.transform(full_image)\n",
        "            cropped_image = self.transform(cropped_image)\n",
        "\n",
        "        return {\n",
        "            'full_image_original': full_image_original,\n",
        "            'full_image': full_image,\n",
        "            'cropped_image': cropped_image,\n",
        "            'yolo_bbox_label': yolo_bbox,\n",
        "            'pdlpr_plate_string': pdlpr_plate_str,\n",
        "            'pdlpr_plate_idx': pdlpr_label_idx,\n",
        "            'image_name': img_name\n",
        "        }\n",
        "\n",
        "\n",
        "    def get_dataloaders(base_dir, batch_size = 8, transform = None, collate_fn=None):\n",
        "        #this functions initializes the different dataloaders and returns them\n",
        "        ds = CCPDDataset(base_dir=base_dir, transform=transform)\n",
        "\n",
        "        train_loader = DataLoader(ds.get_dataset(\"train\"), batch_size=batch_size, shuffle=True, collate_fn = collate_fn)\n",
        "        val_loader = DataLoader(ds.get_dataset(\"val\"), batch_size=batch_size, shuffle=False, collate_fn = collate_fn)\n",
        "        test_loader = DataLoader(ds.get_dataset(\"test\"), batch_size=batch_size, shuffle=False, collate_fn = collate_fn)\n",
        "\n",
        "        return train_loader, val_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1KTU3FkMhmW"
      },
      "source": [
        "## Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gHQ8bVQxCkG"
      },
      "source": [
        "### Network for PDLPR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA_c11Er4uEj"
      },
      "source": [
        "#### IGFE Feature extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNd8YxFd5UyN"
      },
      "source": [
        "ADD COMMENTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "2VLvPcJPxGxw"
      },
      "outputs": [],
      "source": [
        "class FocusStructure(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=64):\n",
        "        super(FocusStructure, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels * 4, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        patch_tl = x[..., ::2, ::2]\n",
        "        patch_tr = x[..., ::2, 1::2]\n",
        "        patch_bl = x[..., 1::2, ::2]\n",
        "        patch_br = x[..., 1::2, 1::2]\n",
        "        x = torch.cat([patch_tl, patch_tr, patch_bl, patch_br], dim=1)  # [B, 4C, H/2, W/2]\n",
        "        return self.conv(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "rXcmvg-Y5E6c"
      },
      "outputs": [],
      "source": [
        "class ConvDownSampling(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ConvDownSampling, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.1) #here we use as activation function LeakyReLU, which is more used in car plate detection\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "dXq3ji7B5MZl"
      },
      "outputs": [],
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(channels),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(channels),\n",
        "        )\n",
        "        self.relu = nn.LeakyReLU(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.relu(x + self.block(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "-asTp22z5PYR"
      },
      "outputs": [],
      "source": [
        "class IGFE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(IGFE, self).__init__()\n",
        "        self.focus = FocusStructure(3, 64)         # From [3,48,144] to [64,24,72]\n",
        "        self.down1 = ConvDownSampling(64, 128)     # [128,12,36]\n",
        "        self.res1 = ResBlock(128)\n",
        "        self.res2 = ResBlock(128)\n",
        "        self.down2 = ConvDownSampling(128, 256)    # [256,6,18]\n",
        "        self.res3 = ResBlock(256)\n",
        "        self.res4 = ResBlock(256)\n",
        "        self.final_conv = nn.Conv2d(256, 512, kernel_size=1)  # [512,6,18]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.focus(x)\n",
        "        x = self.down1(x)\n",
        "        x = self.res1(x)\n",
        "        x = self.res2(x)\n",
        "        x = self.down2(x)\n",
        "        x = self.res3(x)\n",
        "        x = self.res4(x)\n",
        "        x = self.final_conv(x)\n",
        "        return x  # [B, 512, 6, 18]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnbuZU1I5ZGa"
      },
      "source": [
        "#### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "ZEMr0_S25b7Z"
      },
      "outputs": [],
      "source": [
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, dim, max_len=108):\n",
        "        super().__init__()\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, max_len, dim))  # [1, 108, 512]\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pos_embed  # broadcasting over batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "7StAVo9j5gTi"
      },
      "outputs": [],
      "source": [
        "# Encoder block\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, dim=512, inner_dim=1024, n_heads=8):\n",
        "        super().__init__()\n",
        "        self.expand = nn.Conv1d(dim, inner_dim, kernel_size=1)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=inner_dim, num_heads=n_heads, batch_first=True)\n",
        "        self.reduce = nn.Conv1d(inner_dim, dim, kernel_size=1)\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, 108, 512]\n",
        "        x_in = x\n",
        "        x = x.transpose(1, 2)  # [B, 512, 108]\n",
        "        x = self.expand(x)     # [B, 1024, 108]\n",
        "        x = x.transpose(1, 2)  # [B, 108, 1024]\n",
        "\n",
        "        attn_out, _ = self.attn(x, x, x)  # self-attention\n",
        "        x = self.reduce(attn_out.transpose(1, 2)).transpose(1, 2)  # back to [B, 108, 512]\n",
        "        x = self.norm(x + x_in)  # residual + norm\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "iBuGaLHG5jOj"
      },
      "outputs": [],
      "source": [
        "# Encoder (3 blocchi)\n",
        "class PDLPR_Encoder(nn.Module):\n",
        "    def __init__(self, dim=512, n_heads=8, depth=3):\n",
        "        super().__init__()\n",
        "        self.pos_enc = PositionalEncoding(dim)\n",
        "        self.blocks = nn.Sequential(*[EncoderBlock(dim, 1024, n_heads) for _ in range(depth)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, 512, 6, 18]\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.view(B, C, H * W)              # [B, 512, 108] -> shape [B, C, N] (Convolutional layers expects this)\n",
        "        # reorder dimentions for Transformer\n",
        "        x = x.permute(0, 2, 1)               # [B, 108, 512] -> changes to shape [B, N, C] (Transformer expects this)\n",
        "        x = self.pos_enc(x)                  # Add positional encoding\n",
        "        x = self.blocks(x)                   # Encoder blocks\n",
        "        return x  # [B, 108, 512]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hnmq-soL5osZ"
      },
      "source": [
        "#### Parallel decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "l4YqALNu5qtO"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "7aujTQj35u8-"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, dim, n_heads=8):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(dim, n_heads, batch_first=True)\n",
        "        self.cross_attn = nn.MultiheadAttention(dim, n_heads, batch_first=True)\n",
        "        self.ff = FeedForward(dim)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.norm3 = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None):\n",
        "        # tgt: [B, T, dim], memory: [B, S, dim]\n",
        "        x = tgt\n",
        "\n",
        "        # masked self attention\n",
        "        attn_out, _ = self.self_attn(x, x, x, attn_mask=tgt_mask)\n",
        "        x = self.norm1(x + attn_out)\n",
        "\n",
        "        # cross attention\n",
        "        attn_out, _ = self.cross_attn(x, memory, memory)\n",
        "        x = self.norm2(x + attn_out)\n",
        "\n",
        "        # feedforward neural network\n",
        "        ff_out = self.ff(x)\n",
        "        x = self.norm3(x + ff_out)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "rr-WDgHV5yP5"
      },
      "outputs": [],
      "source": [
        "class ParallelDecoder(nn.Module):\n",
        "    def __init__(self, dim=512, vocab_size=70, num_heads=8, num_blocks=3, seq_len=18):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.char_embed = nn.Parameter(torch.randn(1, seq_len, dim))\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dim = dim\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            DecoderBlock(dim, num_heads) for _ in range(num_blocks)\n",
        "        ])\n",
        "        self.classifier = nn.Linear(dim, vocab_size)\n",
        "\n",
        "    def update_vocab_size(self, new_vocab_size):\n",
        "\n",
        "         if new_vocab_size != self.vocab_size:\n",
        "            print(f\"Updating vocab size from {self.vocab_size} to {new_vocab_size}\")\n",
        "            # Save old weights\n",
        "            old_classifier = self.classifier\n",
        "            old_out_features = old_classifier.out_features\n",
        "\n",
        "            # Create new classifier\n",
        "            new_classifier = nn.Linear(self.dim, new_vocab_size)\n",
        "            new_classifier = new_classifier.to(old_classifier.weight.device)\n",
        "\n",
        "            # Copy overlapping weights\n",
        "            num_to_copy = min(old_out_features, new_vocab_size)\n",
        "            with torch.no_grad():\n",
        "                new_classifier.weight[:num_to_copy] = old_classifier.weight[:num_to_copy]\n",
        "                new_classifier.bias[:num_to_copy] = old_classifier.bias[:num_to_copy]\n",
        "\n",
        "            self.classifier = new_classifier\n",
        "            self.vocab_size = new_vocab_size\n",
        "\n",
        "    def generate_mask(self, size):\n",
        "        # mask future tokens\n",
        "        mask = torch.triu(torch.ones(size, size), diagonal=1).bool()\n",
        "        return mask\n",
        "\n",
        "    def forward(self, memory):\n",
        "        # memory: [B, S, dim] → encoder output (B, 108, 512])\n",
        "        B = memory.size(0)\n",
        "        x = self.char_embed.expand(B, -1, -1)  # [B, T, dim]\n",
        "        tgt_mask = self.generate_mask(self.seq_len).to(memory.device)  # [T, T]\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x, memory, tgt_mask)\n",
        "\n",
        "        logits = self.classifier(x)  # [B, T, vocab_size]\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUYEDzyHxIDn"
      },
      "source": [
        "### Network for character recognition of baseline method CNN CTC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdPvkbRgTM6L"
      },
      "source": [
        "This is the model class for the CNN ctc network used for the second part of the baseline method, there are two dropout layers to decrease the overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "n0Vq_hemxBPR"
      },
      "outputs": [],
      "source": [
        "class CNN_CTC_model(nn.Module):\n",
        "    def __init__(self, num_char, hidden_size):\n",
        "        super(CNN_CTC_model, self).__init__()\n",
        "        self.num_char = num_char\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            # 1 Because we use grayscale images\n",
        "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(128, self.hidden_size, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        #two linear layers to do the final classification\n",
        "        self.linear = nn.Linear(self.hidden_size * 12, 256)  # 256×12 = concatenazione H dim\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.classifier = nn.Linear(self.hidden_size, self.num_char)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #the input size is:  Batch, 1, 48 x 144\n",
        "        x = self.features(x)   #output size: Batch, 256, 12x36\n",
        "\n",
        "        #since we have 4 elements, the CTC wants the width first so we have to\n",
        "        #put it into the first position\n",
        "        x = x.permute(3, 0, 1, 2)  # 36, batch, 256, 12\n",
        "        #the width so the frames must be more than the number of total characters that\n",
        "        #we want to encode, so T = width = 36\n",
        "        x = x.flatten(2)          # 36, batch , 256×12\n",
        "        x = self.linear(x)\n",
        "        x = self.dropout(x)           # 36, batch, 256\n",
        "        x = self.classifier(x)    # 36, batch, num_char\n",
        "\n",
        "        return x  #returns a tensor of size [numchar] for each one of the 36 positions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hf_xoanzUlgj"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMrIfoyDbvDt"
      },
      "source": [
        "### Train YOLO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "w4CMqjffb3-0",
        "outputId": "e6a438d4-8378-44ba-8bba-7e412aff5b98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.169  Python-3.11.5 torch-2.5.1+cpu CPU (AMD Ryzen 7 6800H with Radeon Graphics)\n",
            "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=20, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=ccpd.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=25, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov5s.yaml, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov5_epochs25_bs20_lr0.001_imgs6402, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/train, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\train\\yolov5_epochs25_bs20_lr0.001_imgs6402, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      3520  ultralytics.nn.modules.conv.Conv             [3, 32, 6, 2, 2]              \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     18816  ultralytics.nn.modules.block.C3              [64, 64, 1]                   \n",
            "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  4                  -1  2    115712  ultralytics.nn.modules.block.C3              [128, 128, 2]                 \n",
            "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  6                  -1  3    625152  ultralytics.nn.modules.block.C3              [256, 256, 3]                 \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1182720  ultralytics.nn.modules.block.C3              [512, 512, 1]                 \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1    131584  ultralytics.nn.modules.conv.Conv             [512, 256, 1, 1]              \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    361984  ultralytics.nn.modules.block.C3              [512, 256, 1, False]          \n",
            " 14                  -1  1     33024  ultralytics.nn.modules.conv.Conv             [256, 128, 1, 1]              \n",
            " 15                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 16             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 17                  -1  1     90880  ultralytics.nn.modules.block.C3              [256, 128, 1, False]          \n",
            " 18                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 19            [-1, 14]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 20                  -1  1    296448  ultralytics.nn.modules.block.C3              [256, 256, 1, False]          \n",
            " 21                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 22            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 23                  -1  1   1182720  ultralytics.nn.modules.block.C3              [512, 512, 1, False]          \n",
            " 24        [17, 20, 23]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
            "YOLOv5s summary: 153 layers, 9,122,579 parameters, 9,122,563 gradients, 24.0 GFLOPs\n",
            "\n",
            "Freezing layer 'model.24.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.20.1 ms, read: 185.354.7 MB/s, size: 76.8 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\carlo\\Desktop\\CV_project_2\\CV_project\\dataset\\labels\\train... 3595 images, 0 backgrounds, 0 corrupt:  62%|██████▏   | 3595/5769 [00:01<00:01, 1967.71it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\data\\dataset.py:172\u001b[39m, in \u001b[36mYOLODataset.get_labels\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m cache[\u001b[33m\"\u001b[39m\u001b[33mversion\u001b[39m\u001b[33m\"\u001b[39m] == DATASET_CACHE_VERSION  \u001b[38;5;66;03m# matches current version\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m cache[\u001b[33m\"\u001b[39m\u001b[33mhash\u001b[39m\u001b[33m\"\u001b[39m] == get_hash(\u001b[38;5;28mself\u001b[39m.label_files + \u001b[38;5;28mself\u001b[39m.im_files)  \u001b[38;5;66;03m# identical hash\u001b[39;00m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mFileNotFoundError\u001b[39;00m, \u001b[38;5;167;01mAssertionError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m):\n",
            "\u001b[31mAssertionError\u001b[39m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\pool.py:856\u001b[39m, in \u001b[36mIMapIterator.next\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    855\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m856\u001b[39m     item = \u001b[38;5;28mself\u001b[39m._items.popleft()\n\u001b[32m    857\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
            "\u001b[31mIndexError\u001b[39m: pop from an empty deque",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[125]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m best_model_path\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# TRAIN\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m train_model_path = \u001b[43mtrain_yolo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m run_name = get_run_name()\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# VALIDATION after training\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Load and use the best model best.pt --> create a model instance initializzed with the trained weights\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[125]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mtrain_yolo\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Create an untrained model based on the configuration params\u001b[39;00m\n\u001b[32m     18\u001b[39m model = YOLO(\u001b[33m\"\u001b[39m\u001b[33myolov5s.yaml\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mccpd.yaml\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# path to .yaml file for the configuration\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m  \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS_TRAIN_Y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE_TRAIN_Y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr0\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mLR_INIT_Y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mIMAGE_SIZE_Y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                                        \u001b[49m\u001b[38;5;66;43;03m# save the training checkpoints and weigths of the final model\u001b[39;49;00m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m  \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                                       \u001b[49m\u001b[38;5;66;43;03m# TO BE CHANGED ACCORDING TO PC --> \"cpu\"\u001b[39;49;00m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mruns/train\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# directory where to save the outputs of training\u001b[39;49;00m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.pt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# create a subdir in the project folder, where to save training logs and outputs\u001b[39;49;00m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                                        \u001b[49m\u001b[38;5;66;43;03m# run validation here to create results.csv and .png\u001b[39;49;00m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplots\u001b[49m\u001b[43m   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     32\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m model.save(model_name)\n\u001b[32m     36\u001b[39m best_model_path = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mruns/train/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/weights/best.pt\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\engine\\model.py:799\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    796\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.trainer.model\n\u001b[32m    798\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer.hub_session = \u001b[38;5;28mself\u001b[39m.session  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m799\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:227\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    224\u001b[39m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:348\u001b[39m, in \u001b[36mBaseTrainer._do_train\u001b[39m\u001b[34m(self, world_size)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m world_size > \u001b[32m1\u001b[39m:\n\u001b[32m    347\u001b[39m     \u001b[38;5;28mself\u001b[39m._setup_ddp(world_size)\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    350\u001b[39m nb = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.train_loader)  \u001b[38;5;66;03m# number of batches\u001b[39;00m\n\u001b[32m    351\u001b[39m nw = \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mself\u001b[39m.args.warmup_epochs * nb), \u001b[32m100\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.warmup_epochs > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m -\u001b[32m1\u001b[39m  \u001b[38;5;66;03m# warmup iterations\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:307\u001b[39m, in \u001b[36mBaseTrainer._setup_train\u001b[39m\u001b[34m(self, world_size)\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[38;5;66;03m# Dataloaders\u001b[39;00m\n\u001b[32m    306\u001b[39m batch_size = \u001b[38;5;28mself\u001b[39m.batch_size // \u001b[38;5;28mmax\u001b[39m(world_size, \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m \u001b[38;5;28mself\u001b[39m.train_loader = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_dataloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLOCAL_RANK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    309\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n\u001b[32m    311\u001b[39m     \u001b[38;5;66;03m# Note: When training DOTA dataset, double batch size could get OOM on images with >2000 objects.\u001b[39;00m\n\u001b[32m    312\u001b[39m     \u001b[38;5;28mself\u001b[39m.test_loader = \u001b[38;5;28mself\u001b[39m.get_dataloader(\n\u001b[32m    313\u001b[39m         \u001b[38;5;28mself\u001b[39m.data.get(\u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data.get(\u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    314\u001b[39m         batch_size=batch_size \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.task == \u001b[33m\"\u001b[39m\u001b[33mobb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m batch_size * \u001b[32m2\u001b[39m,\n\u001b[32m    315\u001b[39m         rank=-\u001b[32m1\u001b[39m,\n\u001b[32m    316\u001b[39m         mode=\u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    317\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\models\\yolo\\detect\\train.py:84\u001b[39m, in \u001b[36mDetectionTrainer.get_dataloader\u001b[39m\u001b[34m(self, dataset_path, batch_size, rank, mode)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m}, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMode must be \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mval\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch_distributed_zero_first(rank):  \u001b[38;5;66;03m# init dataset *.cache only once if DDP\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     dataset = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuild_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m shuffle = mode == \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(dataset, \u001b[33m\"\u001b[39m\u001b[33mrect\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m shuffle:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\models\\yolo\\detect\\train.py:67\u001b[39m, in \u001b[36mDetectionTrainer.build_dataset\u001b[39m\u001b[34m(self, img_path, mode, batch)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[33;03mBuild YOLO Dataset for training or validation.\u001b[39;00m\n\u001b[32m     57\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m \u001b[33;03m    (Dataset): YOLO dataset object configured for the specified mode.\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     66\u001b[39m gs = \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mint\u001b[39m(de_parallel(\u001b[38;5;28mself\u001b[39m.model).stride.max() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m), \u001b[32m32\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_yolo_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrect\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mval\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\data\\build.py:127\u001b[39m, in \u001b[36mbuild_yolo_dataset\u001b[39m\u001b[34m(cfg, img_path, batch, data, mode, rect, stride, multi_modal)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Build and return a YOLO dataset based on configuration parameters.\"\"\"\u001b[39;00m\n\u001b[32m    126\u001b[39m dataset = YOLOMultiModalDataset \u001b[38;5;28;01mif\u001b[39;00m multi_modal \u001b[38;5;28;01melse\u001b[39;00m YOLODataset\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# augmentation\u001b[39;49;00m\n\u001b[32m    132\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# TODO: probably add a get_hyps_from_cfg function\u001b[39;49;00m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrect\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrect\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# rectangular batches\u001b[39;49;00m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43msingle_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43msingle_cls\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolorstr\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmode\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfraction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfraction\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\data\\dataset.py:88\u001b[39m, in \u001b[36mYOLODataset.__init__\u001b[39m\u001b[34m(self, data, task, *args, **kwargs)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28mself\u001b[39m.data = data\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.use_segments \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_keypoints), \u001b[33m\"\u001b[39m\u001b[33mCan not use both segments and keypoints.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchannels\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\data\\base.py:116\u001b[39m, in \u001b[36mBaseDataset.__init__\u001b[39m\u001b[34m(self, img_path, imgsz, cache, augment, hyp, prefix, rect, batch_size, stride, pad, single_cls, classes, fraction, channels)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28mself\u001b[39m.cv2_flag = cv2.IMREAD_GRAYSCALE \u001b[38;5;28;01mif\u001b[39;00m channels == \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m cv2.IMREAD_COLOR\n\u001b[32m    115\u001b[39m \u001b[38;5;28mself\u001b[39m.im_files = \u001b[38;5;28mself\u001b[39m.get_img_files(\u001b[38;5;28mself\u001b[39m.img_path)\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m \u001b[38;5;28mself\u001b[39m.labels = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[38;5;28mself\u001b[39m.update_labels(include_class=classes)  \u001b[38;5;66;03m# single_cls and include_class\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[38;5;28mself\u001b[39m.ni = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.labels)  \u001b[38;5;66;03m# number of images\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\data\\dataset.py:174\u001b[39m, in \u001b[36mYOLODataset.get_labels\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    172\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m cache[\u001b[33m\"\u001b[39m\u001b[33mhash\u001b[39m\u001b[33m\"\u001b[39m] == get_hash(\u001b[38;5;28mself\u001b[39m.label_files + \u001b[38;5;28mself\u001b[39m.im_files)  \u001b[38;5;66;03m# identical hash\u001b[39;00m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mFileNotFoundError\u001b[39;00m, \u001b[38;5;167;01mAssertionError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     cache, exists = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcache_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_path\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# run cache ops\u001b[39;00m\n\u001b[32m    176\u001b[39m \u001b[38;5;66;03m# Display cache\u001b[39;00m\n\u001b[32m    177\u001b[39m nf, nm, ne, nc, n = cache.pop(\u001b[33m\"\u001b[39m\u001b[33mresults\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# found, missing, empty, corrupt, total\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\data\\dataset.py:125\u001b[39m, in \u001b[36mYOLODataset.cache_labels\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m    111\u001b[39m results = pool.imap(\n\u001b[32m    112\u001b[39m     func=verify_image_label,\n\u001b[32m    113\u001b[39m     iterable=\u001b[38;5;28mzip\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    122\u001b[39m     ),\n\u001b[32m    123\u001b[39m )\n\u001b[32m    124\u001b[39m pbar = TQDM(results, desc=desc, total=total)\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mim_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeypoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnm_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnf_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mne_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnc_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnm\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mnm_f\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnf\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mnf_f\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\pool.py:861\u001b[39m, in \u001b[36mIMapIterator.next\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    859\u001b[39m     \u001b[38;5;28mself\u001b[39m._pool = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    860\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m861\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    863\u001b[39m     item = \u001b[38;5;28mself\u001b[39m._items.popleft()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:320\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    322\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "def get_model_name():\n",
        "    return f\"yolov5_epochs{EPOCHS_TRAIN_Y}_bs{BATCH_SIZE_TRAIN_Y}_lr{LR_INIT_Y}_imgs{IMAGE_SIZE_Y}.pt\"\n",
        "\n",
        "def get_run_name():\n",
        "    # Create a unique name for the current training run, using the hyperparams used in the model\n",
        "    return get_model_name().replace(\".pt\", \"\")\n",
        "\n",
        "\n",
        "def train_yolo():\n",
        "    model_name = get_model_name()\n",
        "    run_name = get_run_name()\n",
        "\n",
        "    if os.path.exists(model_name):\n",
        "        print(f\"[INFO] Model {model_name} already exists ---> SKIP training!!\")\n",
        "        return YOLO(model_name)\n",
        "\n",
        "    # Create an untrained model based on the configuration params\n",
        "    model = YOLO(\"yolov5s.yaml\")\n",
        "\n",
        "    model.train(\n",
        "        data    = f\"ccpd.yaml\",         # path to .yaml file for the configuration\n",
        "        epochs  = EPOCHS_TRAIN_Y,\n",
        "        batch   = BATCH_SIZE_TRAIN_Y,\n",
        "        lr0     = LR_INIT_Y,\n",
        "        imgsz   = IMAGE_SIZE_Y,\n",
        "        save    = True,                                        # save the training checkpoints and weigths of the final model\n",
        "        device  = \"mps\",                                       # TO BE CHANGED ACCORDING TO PC --> \"cpu\"\n",
        "        project = f\"runs/train\",        # directory where to save the outputs of training\n",
        "        name    = model_name.replace(\".pt\", \"\"),               # create a subdir in the project folder, where to save training logs and outputs\n",
        "        val     = True,                                        # run validation here to create results.csv and .png\n",
        "        plots   = True\n",
        "    )\n",
        "\n",
        "    model.save(model_name)\n",
        "\n",
        "    best_model_path = f\"runs/train/{run_name}/weights/best.pt\"\n",
        "\n",
        "    return best_model_path\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TRAIN\n",
        "train_model_path = train_yolo()\n",
        "\n",
        "run_name = get_run_name()\n",
        "\n",
        "# VALIDATION after training\n",
        "# Load and use the best model best.pt --> create a model instance initializzed with the trained weights\n",
        "best_model = YOLO(train_model_path, verbose = False)\n",
        "# best_model = YOLO(\"/Users/michelafuselli/Desktop/Michi/Università/Magistrale/Computer Vision/Project/CV_project/runs/train/yolov5_epochs20_bs8_lr0.001_imgs6402/weights/best.pt\", verbose = False)\n",
        "\n",
        "# Inside results: mAP@0.5, mAP@0.5:0.95. precision, recall, confusion matrix, curva PR, curva f1, ... --> are saved in runs/detect\n",
        "results = best_model.val(\n",
        "    data    = f\"ccpd.yaml\",\n",
        "    split   = 'val',\n",
        "    iou     = IOU_THRESHOLD,\n",
        "    device  = \"cpu\",\n",
        "    name    = f\"{run_name}_VAL_iou{int(IOU_THRESHOLD*100)}\",\n",
        ")\n",
        "\n",
        "image_dir = Path(f\"dataset/images/val\")\n",
        "output_dir = Path(f\"runs/val\") / f\"{get_run_name()}_VAL_iou{int(IOU_THRESHOLD * 100)}\"\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "iou_list = []\n",
        "\n",
        "# Loop over images\n",
        "for image_path in sorted(image_dir.glob(\"*.jpg\")):\n",
        "    # Predict\n",
        "    result = best_model(image_path, max_det=5, verbose = False)[0]\n",
        "    predictions = result.boxes.xyxy.cpu().numpy()  # shape: (N, 4)\n",
        "\n",
        "    real_box = load_gt_box_from_label_validation(image_path)\n",
        "    if real_box is None:\n",
        "        # skip image if no GT or invalid\n",
        "        continue\n",
        "\n",
        "    # Compute IoU between every predicted box and the true one\n",
        "    for predicted_box in predictions:\n",
        "        iou = compute_iou(predicted_box, real_box)\n",
        "        iou_list.append(iou)\n",
        "\n",
        "# Compute average among all iou values\n",
        "if iou_list:\n",
        "    mean_iou = sum(iou_list) / len(iou_list)\n",
        "else:\n",
        "    mean_iou = 0.0\n",
        "\n",
        "# Save in .txt\n",
        "txt_path = output_dir / \"mean_iou.txt\"\n",
        "with open(txt_path, \"w\") as f:\n",
        "    f.write(f\"Mean IoU over validation set: {mean_iou:.4f}\\n\")\n",
        "\n",
        "print(f\"[INFO] Mean IoU saved to {txt_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8HBAOmm6Otb"
      },
      "source": [
        "### PDLPR training function - IGFE, Encoder, Parallel decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tpAVEZy99zv"
      },
      "source": [
        "Validation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "kfcbi0KMKTuY"
      },
      "outputs": [],
      "source": [
        "def validate(model_parts, evaluator, val_loader, char_idx, idx_char, device):\n",
        "    igfe, encoder, decoder = model_parts\n",
        "\n",
        "    igfe.eval()\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    evaluator = Evaluator(idx_char)\n",
        "    pbar = tqdm(val_loader, desc=f\"Validating\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in pbar:\n",
        "            images = batch[\"cropped_image\"].to(device)\n",
        "            label_strs = batch[\"pdlpr_plate_string\"]\n",
        "\n",
        "            unknown = set(c for s in label_strs for c in s if c not in char_idx)\n",
        "            if unknown:\n",
        "                # updating vocabulary\n",
        "                for c in ''.join(label_strs):\n",
        "                    if c not in char_idx:\n",
        "                        idx = len(char_idx)\n",
        "                        char_idx[c] = idx\n",
        "                        idx_char[idx] = c\n",
        "                print(f\"unknown character {c}. Vocabulary updated\")\n",
        "                decoder.update_vocab_size(len(char_idx))\n",
        "\n",
        "            # Forward\n",
        "            features = igfe(images)\n",
        "            encoded = encoder(features)\n",
        "            logits = decoder(encoded)\n",
        "\n",
        "            evaluator.update(logits, label_strs)\n",
        "\n",
        "    metrics = evaluator.compute()\n",
        "    evaluator.print()\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9A4ZOTHKW_D"
      },
      "source": [
        "Training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "_2xL6_eu68TY"
      },
      "outputs": [],
      "source": [
        "def train(model_parts, evaluator, train_loader, val_loader, char_idx, idx_char, num_epochs, optimizer ,device):\n",
        "\n",
        "    igfe, encoder, decoder = model_parts\n",
        "    total_loss = 0\n",
        "\n",
        "    train_losses = []\n",
        "    train_seq_accs = []\n",
        "    train_char_accs = []\n",
        "\n",
        "    val_char_accs = []\n",
        "    val_seq_accs = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        evaluator = Evaluator(idx2char=idx_char)\n",
        "\n",
        "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        for batch in loop:\n",
        "            images = batch[\"cropped_image\"].to(device)\n",
        "            label_strs = batch[\"pdlpr_plate_string\"]\n",
        "\n",
        "            # update the vocabulary if unkwon character found\n",
        "            unknown = set(c for s in label_strs for c in s if c not in char_idx)\n",
        "            if unknown:\n",
        "                # updating vocabulary\n",
        "                for c in ''.join(label_strs):\n",
        "                    if c not in char_idx:\n",
        "                        idx = len(char_idx)\n",
        "                        char_idx[c] = idx\n",
        "                        idx_char[idx] = c\n",
        "                print(f\"unknown character {c}. Vocabulary updated\")\n",
        "                # update the decoder with the new vocabulary size but keeping the old weights\n",
        "                decoder.update_vocab_size(len(char_idx))\n",
        "\n",
        "            # lable encoding in ordr to compute loss\n",
        "            targets = torch.tensor([char_idx[c] for s in label_strs for c in s], dtype=torch.long).to(device)\n",
        "            target_lengths = torch.tensor([len(s) for s in label_strs], dtype=torch.long).to(device)\n",
        "            input_lengths = torch.full((images.size(0),), 18, dtype=torch.long).to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            features = igfe(images)\n",
        "            encoded = encoder(features)\n",
        "            logits = decoder(encoded)\n",
        "            log_probs = logits.log_softmax(2).permute(1, 0, 2)  # [T, B, C]\n",
        "\n",
        "            loss = ctc_loss(log_probs, targets, input_lengths, target_lengths)\n",
        "            total_loss += loss.item()\n",
        "            train_losses.append(loss)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # metrics updating using the evaluator\n",
        "            evaluator.update(logits, label_strs)\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        print(f\"Metrics at epoch {epoch+1}\")\n",
        "        evaluator.print()\n",
        "        metrics = evaluator.compute()\n",
        "        train_seq_accs.append(metrics[\"seq_accuracy\"])\n",
        "        train_char_accs.append(metrics[\"char_accuracy\"])\n",
        "\n",
        "\n",
        "        #saving the new vocabulary\n",
        "        with open(f\"vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(char_idx, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        # VALIDATION\n",
        "        val_evaluator = Evaluator(idx_char)\n",
        "        val_metrics = validate(\n",
        "        model_parts=(igfe, encoder, decoder),\n",
        "        evaluator=val_evaluator,\n",
        "        val_loader=val_loader,\n",
        "        char_idx=char_idx,\n",
        "        idx_char=idx_char,\n",
        "        device=device\n",
        "    )\n",
        "        val_seq_accs.append(val_metrics[\"seq_accuracy\"])\n",
        "        val_char_accs.append(val_metrics[\"char_accuracy\"])\n",
        "\n",
        "    #Saving the model for testing, the models will have as input the images cropped by YOLO\n",
        "    torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'igfe_state_dict': igfe.state_dict(),\n",
        "            'encoder_state_dict': encoder.state_dict(),\n",
        "            'decoder_state_dict': decoder.state_dict(),\n",
        "            'loss': total_loss,\n",
        "            'train_losses': train_losses,\n",
        "            'train_seq_accs': train_seq_accs,\n",
        "            'train_char_accs': train_char_accs\n",
        "        }, f'models/pdlpr_{NUM_EPOCHS_PDLPR}_{LR_PDLPR}_{BATCH_SIZE_PDLPR}.pt')\n",
        "    print(f\"Model saved in models/pdlpr_final.pt\")\n",
        "\n",
        "    print(\"END OF TRAINING, results:\\n\")\n",
        "\n",
        "    print(f\"number of epochs: {num_epochs}\")\n",
        "    print(f\"learning rate: {LR_PDLPR}\")\n",
        "    print(f\"batch size: {BATCH_SIZE_PDLPR}\")\n",
        "    print(f\"Loss: {total_loss / len(train_loader):.4f}\")\n",
        "    evaluator.print()\n",
        "\n",
        "\n",
        "    train_seq_accuracy = metrics['seq_accuracy']\n",
        "    val_seq_accuracy = val_metrics[\"seq_accuracy\"]\n",
        "    train_char_accuracy = metrics['char_accuracy']\n",
        "    val_char_accuracy = metrics['char_accuracy']\n",
        "\n",
        "    with open(f\"results/PDLPR-{NUM_EPOCHS_PDLPR}_{LR_PDLPR}_{BATCH_SIZE_PDLPR}.txt\", \"w\") as f:\n",
        "        f.write(f\"Final train accuracy: {train_seq_accuracy:.4f}\\n\")\n",
        "        f.write(f\"Final validation accuracy: {val_seq_accuracy:.4f}\\n\")\n",
        "        f.write(f\"Final character train accuracy: {train_char_accuracy:.4f}\\n\")\n",
        "        f.write(f\"Final character validation accuracy: {val_char_accuracy:.4f}\\n\")\n",
        "\n",
        "    print(f\"results saved in results/PDLPR-{NUM_EPOCHS_PDLPR}_{LR_PDLPR}_{BATCH_SIZE_PDLPR}.txt\")\n",
        "\n",
        "    # plot loss over epochs\n",
        "    epochs = range(1, len(train_losses)+1)\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, [l.detach().cpu().item() if torch.is_tensor(l) else l for l in train_losses], label=\"Train Loss\")\n",
        "    plt.title(\"Loss over Epochs\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"CTC Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(f\"metrics_images/loss_plot_{num_epochs}_{LR_PDLPR}_{BATCH_SIZE_PDLPR}.png\", dpi=300)\n",
        "\n",
        "    # plot train and validation metrics\n",
        "    print(\"Plotting metrics.........\")\n",
        "    plot_metrics(train_seq_accs, val_seq_accs, train_char_accs, val_char_accs)\n",
        "\n",
        "    return train_losses, train_seq_accs, train_char_accs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co8kt5kR-Fu-"
      },
      "source": [
        "Defining transformations to apply on the images and load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "4BREUnDd-E-c"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((48, 144)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
        "    ])\n",
        "\n",
        "    # loading data\n",
        "\n",
        "    dataset = CCPDDataset(base_dir=f\"dataset\", transform=transform)\n",
        "    train_loader, val_loader, test_loader = CCPDDataset.get_dataloaders(\n",
        "        base_dir=f\"dataset\",\n",
        "        batch_size=BATCH_SIZE_PDLPR,\n",
        "        transform=transform,\n",
        "        collate_fn= custom_collate\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXB6dwfp-ezK"
      },
      "source": [
        "Create the vocabulary if it does not already exist and defining mapping char to index and index to char"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loU_5ZrT-O7L",
        "outputId": "f0a83c41-2646-4726-86b4-0edb5ab57fcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocab.json found — loading...\n"
          ]
        }
      ],
      "source": [
        "# loading vocabulary\n",
        "if os.path.exists(f'vocab.json'):\n",
        "    print(f\"vocab.json found — loading...\")\n",
        "    char_idx, idx_char = load_vocab(f'vocab.json')\n",
        "else:\n",
        "    print(\"vocab.json not found — building it from labels...\")\n",
        "    char_idx, idx_char = build_vocab(f\"dataset/labels_pdlpr/train\", \"vocab.json\")\n",
        "\n",
        "vocab_size = len(char_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vfur3BaB-sO8"
      },
      "source": [
        "Defining training arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "H4BHpAS3-rTy"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "evaluator = Evaluator(idx_char)\n",
        "ctc_loss = nn.CTCLoss(blank=0)\n",
        "decoder_seq_len = 18  # From ParallelDecoder\n",
        "decoder = ParallelDecoder(dim=512, vocab_size=vocab_size, seq_len=decoder_seq_len).to(device).train()\n",
        "encoder = PDLPR_Encoder().to(device).train()\n",
        "igfe = IGFE().to(device).train()\n",
        "params = list(igfe.parameters()) + list(encoder.parameters()) + list(decoder.parameters())\n",
        "optimizer = optim.Adam(params, lr=LR_PDLPR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1jEU-ER_B6m"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "cnxUdYfW_Dym",
        "outputId": "fe1888a7-1a59-4b11-e01e-1d4a612a9a25"
      },
      "outputs": [],
      "source": [
        "print(\"Starting training..........\")\n",
        "train_char_accs, train_seq_accs, train_losses = train(\n",
        "    model_parts=(igfe, encoder, decoder),\n",
        "    evaluator=evaluator,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    char_idx=char_idx,\n",
        "    idx_char=idx_char,\n",
        "    num_epochs=NUM_EPOCHS_PDLPR,\n",
        "    optimizer=optimizer,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOa4Xuipb0wd"
      },
      "source": [
        "### train Baseline method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MSm6h7qV867"
      },
      "source": [
        "Training of the second part of character recognition, since the first part of licence plate detection was implemented using traditional techniques it doesn't make sense to distinguish between train and testing.\n",
        "\n",
        "So this is the code for the first part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "collapsed": true,
        "id": "r_n1gOR9WiE8",
        "outputId": "6b368e7d-b037-4e8b-d348-f79ae7537803"
      },
      "outputs": [],
      "source": [
        "# Initialize the reader just once: simplified chinese and english\n",
        "reader = easyocr.Reader(['ch_sim', 'en'])\n",
        "\n",
        "images_dir = Path(f\"dataset/images/train/\")\n",
        "labels_dir = Path(f\"dataset/labels/train/\")\n",
        "\n",
        "diff_results_dir = Path(\"results\")\n",
        "diff_results_dir.mkdir(parents=True, exist_ok=True)\n",
        "diff_results_txt = diff_results_dir / f\"BL_iou_ocr6.txt\"\n",
        "open(diff_results_txt, \"w\").close()\n",
        "\n",
        "total_iou = 0.0\n",
        "num_iou = 0\n",
        "num_passed_iou = 0      # counts values >= 0.7\n",
        "\n",
        "\n",
        "for image_path in tqdm(images_dir.glob(\"*.jpg\"), desc=\"Processing images\", unit=\"img\"):\n",
        "    image_name = image_path.name\n",
        "    label_path = labels_dir / (image_path.stem + \".txt\")\n",
        "\n",
        "    if not label_path.exists():\n",
        "        print(f\"NO label for {image_name}\")\n",
        "        continue\n",
        "\n",
        "    pil_image = Image.open(image_path).convert(\"RGB\")\n",
        "    yolo_tensor = get_label_yolo(label_path)\n",
        "    width, height = pil_image.size\n",
        "\n",
        "    true_coordinates = get_ground_truth_coordinates(yolo_tensor, width, height)\n",
        "\n",
        "    candidate_bounding_box = plate_detector(image_path, true_coordinates)\n",
        "\n",
        "    if not candidate_bounding_box:\n",
        "        with open(diff_results_txt, \"a\") as f:\n",
        "            f.write(f\"{image_name}\\n\")\n",
        "            f.write(\"IoU: 0.000\\n\")\n",
        "            f.write(\"OCR: NONE\\n\")\n",
        "            f.write(f\"Box GT: {true_coordinates}\\n\")\n",
        "            f.write(\"Box Pred: NONE\\n\")\n",
        "            f.write(\"---\\n\")\n",
        "\n",
        "        # count it as missing\n",
        "        total_iou += 0.0\n",
        "        num_iou += 1\n",
        "        continue\n",
        "\n",
        "    predict_bbox, ocr_text  = candidate_bounding_box\n",
        "    iou_diff = compute_iou(predict_bbox, true_coordinates)\n",
        "\n",
        "    total_iou += iou_diff\n",
        "    num_iou += 1\n",
        "    if iou_diff >= IOU_THRESHOLD:\n",
        "        num_passed_iou += 1\n",
        "\n",
        "    with open(diff_results_txt,  \"a\", encoding = \"utf-8\") as f:\n",
        "        f.write(f\"{image_name}\\n\")\n",
        "        f.write(f\"IoU: {iou_diff:.3f}\\n\")\n",
        "        f.write(f\"OCR: {ocr_text}\\n\")\n",
        "        f.write(f\"Box GT: {true_coordinates}\\n\")\n",
        "        f.write(f\"Box Pred: {predict_bbox}\\n\")\n",
        "        f.write(\"---\\n\")\n",
        "\n",
        "if num_iou > 0:\n",
        "    avg_iou = total_iou / num_iou\n",
        "    pass_rate = (num_passed_iou / num_iou) * 100\n",
        "else:\n",
        "    avg_iou = 0.0\n",
        "    pass_rate = 0.0\n",
        "\n",
        "\n",
        "with open(diff_results_txt, \"a\") as f:\n",
        "    f.write(f\"\\n AVERAGE IoU over {num_iou} predictions: {avg_iou:0.4f}\")\n",
        "    f.write(f\"\\n IoU pass rate (>= 0.7) {pass_rate:0.2f}\")\n",
        "\n",
        "print(f\"\\n AVERAGE IoU over {num_iou} predictions: {avg_iou:0.4f}\")\n",
        "print(f\"\\n IoU pass rate (>= 0.7) {pass_rate:0.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80HVEI5TWiw0"
      },
      "source": [
        "In the training of CNN + CTC we try different hyperparameters combinations to select the best ones. The resulting images and txt files are saved in local folders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "collapsed": true,
        "id": "NRlmVpiWUrqC",
        "outputId": "7ee0389d-359a-407d-968b-aabc887165cf"
      },
      "outputs": [],
      "source": [
        "#Hyperparameters combination\n",
        "batch_sizes = [64, 32]  \n",
        "learning_rates = [0.001]\n",
        "weight_decays = [1e-4, 5e-4]\n",
        "epochs = [40, 60]\n",
        "\n",
        "CHAR_LIST = sorted(set(PROVINCES+ALPHABETS+ADS))\n",
        "PLATE_LENGTH = 8\n",
        "\n",
        "NUM_CHAR = len(CHAR_LIST) + 1 #since we include the blank character\n",
        "\n",
        "combinations = product(batch_sizes, learning_rates, weight_decays, epochs)\n",
        "\n",
        "#executing the training and testing for all the possible combinations to get the best one\n",
        "for bs, lr, wd, ne in combinations:\n",
        "\n",
        "    #Hyperparameters\n",
        "    BATCH_SIZE = bs\n",
        "    LR = lr\n",
        "    WEIGHT_DECAY = wd\n",
        "    NUM_EPOCHS = ne\n",
        "\n",
        "    SAVE_NAME = f\"n_epochs_{NUM_EPOCHS}_bs_{BATCH_SIZE}_LR_{LR}_wd_{WEIGHT_DECAY}\"\n",
        "\n",
        "    print(f\"training with {SAVE_NAME}\")\n",
        "\n",
        "    model = CNN_CTC_model(num_char=NUM_CHAR, hidden_size=256)\n",
        "    ctc_loss = nn.CTCLoss(blank=0)\n",
        "\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Grayscale(),              # converte in 1 canale\n",
        "        transforms.Resize((48, 144)),       # adatta a H=48, W=144\n",
        "        transforms.ToTensor(),              # [C, H, W]\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "\n",
        "    train_dataloader, val_dataloader, test_dataloader = CCPDDataset.get_dataloaders(base_dir=DATASET_PATH_Y, batch_size=BATCH_SIZE, transform=preprocess, collate_fn = custom_collate_2)\n",
        "     #this optimizer uses stochastic gradient descent and has in input the parameters (weights) from\n",
        "    #the pretrained model\n",
        "    optimizer = Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "    #optimizer = SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    #initialize the device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    accuracy_val=[]\n",
        "    accuracy_train=[]\n",
        "    total_train_loss=[]\n",
        "    char_accuracy_train =[]\n",
        "    char_accuracy_val =[]\n",
        "\n",
        "    epsilon = 1e-6\n",
        "    #TRAIN LOOP we are doing fine tuning on the task of recognizing plate\n",
        "    for e in range(NUM_EPOCHS):\n",
        "        evaluator = Evaluator()\n",
        "        model.train()\n",
        "        train_loss= 0.0\n",
        "        train_acc = []\n",
        "        val_acc = []\n",
        "        train_char_acc =[]\n",
        "        val_char_acc = []\n",
        "        B_size = 0\n",
        "        i=0\n",
        "        #does the for loop for all the items in the same batch\n",
        "        for batch in train_dataloader:\n",
        "            print(f\"Batch {i + 1}/{len(train_dataloader)}\")\n",
        "            images = batch[\"cropped_image\"]\n",
        "            labels = batch[\"pdlpr_plate_idx\"]\n",
        "\n",
        "            images = [img.to(device) for img in images]\n",
        "            labels = [lab.to(device) for lab in labels]\n",
        "\n",
        "            # Stack per batch processing\n",
        "            images = torch.stack(images)\n",
        "            labels = torch.stack(labels)\n",
        "\n",
        "            #Ctc loss expects a simple list not a 2 dimensional tensor, so all the batch\n",
        "            #index are flattened into one single list\n",
        "            flat_labels_list = labels.view(-1)\n",
        "            #we get the output of the models and apply softmax to turn it into probability\n",
        "            output_logits = model(images)\n",
        "            output_probabilities = F.log_softmax(output_logits, dim=2)\n",
        "            #the output of the model are T vectors for the batch size\n",
        "            T = output_logits.size(0)\n",
        "            #get the current batch size\n",
        "            B_size = images.size(0)\n",
        "            #creates a tensor the length of the batch size filled with the dimention of the input\n",
        "            #and the dimension of the output, since ctc requires the lengths because it uses one big\n",
        "            #vector\n",
        "            input_lengths = torch.full((B_size,), T, dtype=torch.long).to(device)\n",
        "            target_lengths = torch.full((B_size,), PLATE_LENGTH, dtype=torch.long).to(device)\n",
        "\n",
        "            #CTC loss\n",
        "            loss = ctc_loss(output_probabilities, flat_labels_list, input_lengths, target_lengths)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            evaluator.reset()\n",
        "            evaluator.update_baseline(output_logits, labels)\n",
        "\n",
        "            ##we take the index of words with the highest probabilities\n",
        "            metrics = evaluator.compute()\n",
        "            #metrics for the whole batch\n",
        "            mean_batch_train_char_acc = metrics[\"char_accuracy\"]\n",
        "            mean_batch_train_acc = metrics[\"seq_accuracy\"]\n",
        "            #print(mean_batch_train_char_acc)\n",
        "            #print(mean_batch_train_acc)\n",
        "            train_acc.append(mean_batch_train_acc)\n",
        "            train_char_acc.append(mean_batch_train_char_acc)\n",
        "\n",
        "            i+=1\n",
        "\n",
        "        #compute the mean of the full and character accuracy for training\n",
        "        #for the whole epoch\n",
        "        mean_train_acc = sum(train_acc)/len(train_acc)\n",
        "        mean_train_char_acc = sum(train_char_acc)/len(train_char_acc)\n",
        "        train_loss = train_loss/B_size\n",
        "\n",
        "        #append the result to lists in order to plot them\n",
        "        accuracy_train.append(mean_train_acc)\n",
        "        char_accuracy_train.append(mean_train_char_acc)\n",
        "        total_train_loss.append(train_loss)\n",
        "\n",
        "        j=0\n",
        "        #Validation phase\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in val_dataloader:\n",
        "                print(f\"Batch {j + 1}/{len(val_dataloader)}\")\n",
        "                images = batch[\"cropped_image\"]\n",
        "                labels = batch[\"pdlpr_plate_idx\"]\n",
        "\n",
        "                images = [img.to(device) for img in images]\n",
        "                labels = [lab.to(device) for lab in labels]\n",
        "\n",
        "                images = torch.stack(images)\n",
        "                labels = torch.stack(labels)\n",
        "\n",
        "                flat_labels_list = labels.view(-1)\n",
        "\n",
        "                output_logits = model(images)\n",
        "\n",
        "                evaluator.reset()\n",
        "                evaluator.update_baseline(output_logits, labels)\n",
        "                metrics = evaluator.compute()\n",
        "\n",
        "                #metrics for the whole batch\n",
        "                mean_batch_val_char_acc = metrics[\"char_accuracy\"]\n",
        "                mean_batch_val_acc = metrics[\"seq_accuracy\"]\n",
        "                #print(mean_batch_val_char_acc)\n",
        "                #print(mean_batch_val_acc)\n",
        "                val_acc.append(mean_batch_val_acc)\n",
        "                val_char_acc.append(mean_batch_val_char_acc)\n",
        "\n",
        "                j+=1\n",
        "\n",
        "        #compute the mean of the iou validation score\n",
        "        mean_val_acc = sum(val_acc)/len(val_acc)\n",
        "        mean_val_char_acc = sum(val_char_acc)/len(val_char_acc)\n",
        "\n",
        "        accuracy_val.append(mean_val_acc)\n",
        "        char_accuracy_val.append(mean_val_char_acc)\n",
        "\n",
        "        print(f\"Epoch {e +1}/{NUM_EPOCHS} - train loss: {train_loss} - train acc: {mean_train_acc} - train char acc: {mean_train_char_acc} - val acc: {mean_val_acc} --  val char acc: {mean_val_char_acc}\" )\n",
        "\n",
        "    #Saving the model\n",
        "    torch.save(model.state_dict(), f\"models/CNNCTC-{SAVE_NAME}.pth\")\n",
        "\n",
        "    #Plotting the figure for the train and validation\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(range(1, NUM_EPOCHS + 1), accuracy_train, label=\"train acc\", marker='o')\n",
        "    plt.plot(range(1, NUM_EPOCHS + 1), accuracy_val, label=\"validation acc\", marker='s')\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(\"accuracy\")\n",
        "    plt.title(\"Train and validation plate accuracy per epoch\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    #plt.show()\n",
        "    plt.savefig(f\"metrics_images/train_validation_CNNCTC-{SAVE_NAME}.png\")\n",
        "\n",
        "    #Plotting the figure for the train and validation\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(range(1, NUM_EPOCHS + 1), char_accuracy_train, label=\"char train acc\", marker='o')\n",
        "    plt.plot(range(1, NUM_EPOCHS + 1), char_accuracy_val, label=\"char validation acc\", marker='s')\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(\"accuracy\")\n",
        "    plt.title(\"Train and validation character accuracy per epoch\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    #plt.show()\n",
        "    plt.savefig(f\"metrics_images/char_train_validation_CNNCTC-{SAVE_NAME}.png\")\n",
        "\n",
        "    #getting the last iou value for train and validation\n",
        "    final_train_acc = accuracy_train[-1]\n",
        "    final_val_acc = accuracy_val[-1]\n",
        "\n",
        "    final_char_train_acc = char_accuracy_train[-1]\n",
        "    final_char_val_acc = char_accuracy_val[-1]\n",
        "\n",
        "    with open(f\"results/CNNCTC-{SAVE_NAME}.txt\", \"w\") as f:\n",
        "        f.write(f\"Final train accuracy: {final_train_acc:.4f}\\n\")\n",
        "        f.write(f\"Final validation accuracy: {final_val_acc:.4f}\\n\")\n",
        "        f.write(f\"Final character train accuracy: {final_char_train_acc:.4f}\\n\")\n",
        "        f.write(f\"Final character validation accuracy: {final_char_val_acc:.4f}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cncz11IbUnpg"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrQVCVnYb770"
      },
      "source": [
        "### test Yolo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "LR08RINVUuUa",
        "outputId": "07e05e6d-7360-4fa6-f868-37562d58c361"
      },
      "outputs": [],
      "source": [
        "def get_model_name():\n",
        "    return f\"yolov5_epochs{EPOCHS_TRAIN_Y}_bs{BATCH_SIZE_TRAIN_Y}_lr{LR_INIT_Y}_imgs{IMAGE_SIZE_Y}.pt\"\n",
        "\n",
        "\n",
        "def get_run_name():\n",
        "    return get_model_name().replace(\".pt\", \"\")\n",
        "\n",
        "\n",
        "def test_yolo():\n",
        "    model_name = get_model_name()\n",
        "    # Load the actual trained model weights --> this ensures that testing is run on the best version of the model\n",
        "    model_path = Path(f\"runs/train\") / model_name.replace(\".pt\", \"\") / \"weights\" / \"best.pt\"\n",
        "\n",
        "    if not model_path.exists():\n",
        "        raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
        "\n",
        "\n",
        "    # Retrive the name of the training run\n",
        "    run_name = get_run_name()\n",
        "\n",
        "    output_name = f\"{Path(model_path).stem}_TEST_iou{int(IOU_THRESHOLD * 100)}\"\n",
        "    output_dir = Path(f\"runs/test\") / output_name\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    model = YOLO(model_path)\n",
        "\n",
        "    results = model.val(\n",
        "        data    = f\"ccpd.yaml\",\n",
        "        split   = \"test\",\n",
        "        iou     = IOU_THRESHOLD,\n",
        "        device  = \"cpu\",\n",
        "        name    = f\"{run_name}_TEST_iou70\",\n",
        "        project = f\"runs/test\"\n",
        "    )\n",
        "\n",
        "    return results, model, output_dir\n",
        "\n",
        "\n",
        "print(f\"\")\n",
        "model_path = f\"runs/train/yolov5_epochs30_bs12_lr0.001_imgs640/weights\"\n",
        "\n",
        "run_name = Path(model_path).parent.parent.name\n",
        "\n",
        "output_name = f\"{Path(model_path).stem}_TEST_iou{int(IOU_THRESHOLD * 100)}\"\n",
        "\n",
        "# TESTING\n",
        "results, model_test, test_output_dir = test_yolo()\n",
        "\n",
        "# Save metrics in a file\n",
        "metrics_path = test_output_dir / \"test_metrics.txt\"\n",
        "test_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "with open(metrics_path, \"w\") as f:\n",
        "    f.write(f\"Model: {model_path}\\n\")\n",
        "    f.write(f\"IoU Threshold: {IOU_THRESHOLD}\\n\\n\")\n",
        "    f.write(f\"mAP@0.5:      {results.box.map50:.4f}\\n\")\n",
        "    f.write(f\"mAP@0.5:0.95: {results.box.map:.4f}\\n\")\n",
        "    f.write(f\"Precision:    {results.box.mp:.4f}\\n\")\n",
        "    f.write(f\"Recall:       {results.box.mr:.4f}\\n\")\n",
        "\n",
        "\n",
        "# Compute IoU\n",
        "image_dir = Path(f\"dataset/images/test\")\n",
        "\n",
        "iou_list = []\n",
        "\n",
        "# Loop over images\n",
        "for image_path in sorted(image_dir.glob(\"*.jpg\")):\n",
        "    # Predict\n",
        "    result = model_test(str(image_path), max_det=5, verbose = False)[0]\n",
        "    predictions = result.boxes.xyxy.cpu().numpy()  # shape: (N, 4)\n",
        "\n",
        "    real_box = load_gt_box_from_label_test(image_path)\n",
        "    if real_box is None:\n",
        "        # skip image if no GT or invalid\n",
        "        continue\n",
        "\n",
        "    # Compute IoU between every predicted box and the true one\n",
        "    for predicted_box in predictions:\n",
        "        iou = compute_iou(predicted_box, real_box)\n",
        "        iou_list.append(iou)\n",
        "\n",
        "# Compute average among all iou values\n",
        "if iou_list:\n",
        "    mean_iou = sum(iou_list) / len(iou_list)\n",
        "else:\n",
        "    mean_iou = 0.0\n",
        "\n",
        "# Save in test_metrics.txt\n",
        "with open(metrics_path, \"a\") as f:\n",
        "    f.write(f\"Mean IoU over test set: {mean_iou:.4f}\\n\")\n",
        "\n",
        "print(f\"[INFO] Mean IoU saved to {metrics_path}\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n TESTING complete!\")\n",
        "print(f\"Results saved to: runs/test/{test_output_dir}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W58PEoMpSRgV"
      },
      "source": [
        "### Test PDLPR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2sOXzwhWLdU"
      },
      "source": [
        "Initializing parameters and load trained pdlpr model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AO0S5XkUV10v",
        "outputId": "06511b3a-7876-4a48-b872-f87dc9719de8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocab.json found — loading...\n",
            "checkpoint found. Loading state dict......\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\carlo\\AppData\\Local\\Temp\\ipykernel_28108\\958279428.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load( f'models/pdlpr_10_0.0001_16.pt', map_location=device)\n"
          ]
        }
      ],
      "source": [
        "# loading vocabulary\n",
        "if os.path.exists(f'vocab.json'):\n",
        "    print(f\"vocab.json found — loading...\")\n",
        "    char_idx, idx_char = load_vocab(f'vocab.json')\n",
        "else:\n",
        "    print(\"vocab.json not found — building it from labels...\")\n",
        "    char_idx, idx_char = build_vocab(f\"dataset/labels_pdlpr/train\", \"vocab.json\")\n",
        "\n",
        "vocab_size = len(char_idx)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "evaluator = Evaluator(idx_char)\n",
        "ctc_loss = nn.CTCLoss(blank=0)\n",
        "decoder_seq_len = 18  # From ParallelDecoder\n",
        "decoder = ParallelDecoder(dim=512, vocab_size=vocab_size, seq_len=decoder_seq_len).to(device).train()\n",
        "encoder = PDLPR_Encoder().to(device).train()\n",
        "igfe = IGFE().to(device).train()\n",
        "\n",
        "# load pre trained model if needed\n",
        "if os.path.exists(f'models/pdlpr_10_0.0001_16.pt'):\n",
        "    print(\"checkpoint found. Loading state dict......\")\n",
        "    checkpoint = torch.load( f'models/pdlpr_10_0.0001_16.pt', map_location=device)\n",
        "    igfe.load_state_dict(checkpoint[\"igfe_state_dict\"])\n",
        "    encoder.load_state_dict(checkpoint[\"encoder_state_dict\"])\n",
        "    decoder.load_state_dict(checkpoint[\"decoder_state_dict\"])\n",
        "    #optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "\n",
        "else:\n",
        "    print(\"checkpoint not found. Please train the model first\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmNnKtGJOCXa"
      },
      "source": [
        "Test function PDLPR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "7XTJIp2bOBxW"
      },
      "outputs": [],
      "source": [
        "def test(model_parts, evaluator, test_loader, char_idx, idx_char, device):\n",
        "    igfe, encoder, decoder = model_parts\n",
        "\n",
        "    # keep track of metrics for plot\n",
        "    test_seq_accs = []\n",
        "    test_char_accs = []\n",
        "\n",
        "    igfe.eval()\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    evaluator = Evaluator(idx_char)\n",
        "    pbar = tqdm(test_loader, desc=f\"Testing\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in pbar:\n",
        "            images = batch[\"cropped_image\"].to(device)\n",
        "            label_strs = batch[\"pdlpr_plate_string\"]\n",
        "\n",
        "            unknown = set(c for s in label_strs for c in s if c not in char_idx)\n",
        "            if unknown:\n",
        "                # updating vocabulary\n",
        "                for c in ''.join(label_strs):\n",
        "                    if c not in char_idx:\n",
        "                        idx = len(char_idx)\n",
        "                        char_idx[c] = idx\n",
        "                        idx_char[idx] = c\n",
        "                print(f\"unknown character {c}. Vocabulary updated\")\n",
        "                # update the decoder with the new vocabulary size but keeping the old weights\n",
        "                decoder.update_vocab_size(len(char_idx))\n",
        "\n",
        "\n",
        "            # Forward\n",
        "            features = igfe(images)\n",
        "            encoded = encoder(features)\n",
        "            logits = decoder(encoded)\n",
        "\n",
        "            evaluator.update(logits, label_strs)\n",
        "            metrics = evaluator.compute()\n",
        "            test_seq_accs.append(metrics[\"seq_accuracy\"])\n",
        "            test_char_accs.append(metrics[\"char_accuracy\"])\n",
        "\n",
        "\n",
        "    #saving the new vocabulary\n",
        "    with open(\"vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(char_idx, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    metrics = evaluator.compute()\n",
        "    evaluator.print()\n",
        "    return metrics, test_char_accs, test_seq_accs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_01CMcIO-q9"
      },
      "source": [
        "Start testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "V9n9ag1GPCqB",
        "outputId": "b27a93d2-f0de-48ce-dcd4-6bf2297d209f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting testing............\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing:   3%|▎         | 9/313 [00:04<02:35,  1.96it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[128]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting testing............\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m test_metrics, test_char_accs, test_seq_accs = \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_parts\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43migfe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchar_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchar_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43midx_char\u001b[49m\u001b[43m=\u001b[49m\u001b[43midx_char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[127]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mtest\u001b[39m\u001b[34m(model_parts, evaluator, test_loader, char_idx, idx_char, device)\u001b[39m\n\u001b[32m     34\u001b[39m features = igfe(images)\n\u001b[32m     35\u001b[39m encoded = encoder(features)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m logits = \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m evaluator.update(logits, label_strs)\n\u001b[32m     39\u001b[39m metrics = evaluator.compute()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[123]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mParallelDecoder.forward\u001b[39m\u001b[34m(self, memory)\u001b[39m\n\u001b[32m     44\u001b[39m tgt_mask = \u001b[38;5;28mself\u001b[39m.generate_mask(\u001b[38;5;28mself\u001b[39m.seq_len).to(memory.device)  \u001b[38;5;66;03m# [T, T]\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     x = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.classifier(x)  \u001b[38;5;66;03m# [B, T, vocab_size]\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[122]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mDecoderBlock.forward\u001b[39m\u001b[34m(self, tgt, memory, tgt_mask)\u001b[39m\n\u001b[32m     18\u001b[39m x = \u001b[38;5;28mself\u001b[39m.norm1(x + attn_out)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# cross attention\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m attn_out, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcross_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m x = \u001b[38;5;28mself\u001b[39m.norm2(x + attn_out)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# feedforward neural network\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1368\u001b[39m, in \u001b[36mMultiheadAttention.forward\u001b[39m\u001b[34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[39m\n\u001b[32m   1342\u001b[39m     attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[32m   1343\u001b[39m         query,\n\u001b[32m   1344\u001b[39m         key,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1365\u001b[39m         is_causal=is_causal,\n\u001b[32m   1366\u001b[39m     )\n\u001b[32m   1367\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1368\u001b[39m     attn_output, attn_output_weights = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1384\u001b[39m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1387\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1388\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[32m   1390\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m), attn_output_weights\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:6097\u001b[39m, in \u001b[36mmulti_head_attention_forward\u001b[39m\u001b[34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[39m\n\u001b[32m   6093\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_separate_proj_weight:\n\u001b[32m   6094\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m   6095\u001b[39m         in_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   6096\u001b[39m     ), \u001b[33m\"\u001b[39m\u001b[33muse_separate_proj_weight is False but in_proj_weight is None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m6097\u001b[39m     q, k, v = \u001b[43m_in_projection_packed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6098\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6099\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m   6100\u001b[39m         q_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   6101\u001b[39m     ), \u001b[33m\"\u001b[39m\u001b[33muse_separate_proj_weight is True but q_proj_weight is None\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:5519\u001b[39m, in \u001b[36m_in_projection_packed\u001b[39m\u001b[34m(q, k, v, w, b)\u001b[39m\n\u001b[32m   5517\u001b[39m     b_q, b_kv = b.split([E, E * \u001b[32m2\u001b[39m])\n\u001b[32m   5518\u001b[39m q_proj = linear(q, w_q, b_q)\n\u001b[32m-> \u001b[39m\u001b[32m5519\u001b[39m kv_proj = \u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_kv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_kv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5520\u001b[39m \u001b[38;5;66;03m# reshape to 2, E and not E, 2 is deliberate for better memory coalescing and keeping same order as chunk()\u001b[39;00m\n\u001b[32m   5521\u001b[39m kv_proj = (\n\u001b[32m   5522\u001b[39m     kv_proj.unflatten(-\u001b[32m1\u001b[39m, (\u001b[32m2\u001b[39m, E))\n\u001b[32m   5523\u001b[39m     .unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m   (...)\u001b[39m\u001b[32m   5526\u001b[39m     .contiguous()\n\u001b[32m   5527\u001b[39m )\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "print(\"Starting testing............\")\n",
        "test_metrics, test_char_accs, test_seq_accs = test(\n",
        "    model_parts=(igfe, encoder, decoder),\n",
        "    evaluator=evaluator,\n",
        "    test_loader=test_loader,\n",
        "    char_idx=char_idx,\n",
        "    idx_char=idx_char,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7HqGhcYSZXr"
      },
      "source": [
        "### Test pipeline yolo + PDLPR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPqPyw86SsO2"
      },
      "source": [
        "Function that crops the images using trained yolo model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPYOZqqcSqFU"
      },
      "outputs": [],
      "source": [
        "def crop_image_yolo(yolo_model, image):\n",
        "    #image = Image.open(image_path).convert(\"RGB\")\n",
        "    results = yolo_model(image, verbose=False)[0]  # Results object\n",
        "    boxes = results.boxes\n",
        "\n",
        "    if boxes is None or len(boxes) == 0:\n",
        "        print(\"No car plate detected\")\n",
        "        return None\n",
        "\n",
        "    bbox = boxes.xyxy[0].cpu().numpy().astype(int)\n",
        "    x1, y1, x2, y2 = bbox\n",
        "\n",
        "    cropped_img = image.crop((x1, y1, x2, y2))\n",
        "\n",
        "    #plt.imshow(cropped_img)\n",
        "    #plt.title(\"Targa rilevata (crop YOLO)\")\n",
        "    #plt.axis(\"off\")\n",
        "    #plt.show()\n",
        "\n",
        "    return cropped_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DerfPu2TSwxT"
      },
      "source": [
        "Function that tests the total pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Sv5fj8zSeSS"
      },
      "outputs": [],
      "source": [
        "def test(model_parts, yolo_model, transform, evaluator, test_loader, char_idx, idx_char, device):\n",
        "    igfe, encoder, decoder = model_parts\n",
        "\n",
        "    igfe.eval()\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    predicted_strings = []\n",
        "\n",
        "    evaluator = Evaluator(idx_char)\n",
        "    pbar = tqdm(test_loader, desc=f\"Testing\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in pbar:\n",
        "            # cropping images with yolo\n",
        "            sample = batch[0]\n",
        "            img = sample[\"full_image_original\"]\n",
        "            label_strs = sample[\"pdlpr_plate_string\"]\n",
        "\n",
        "            images = []\n",
        "\n",
        "            # Crop using YOLO\n",
        "            cropped_img = crop_image_yolo(yolo_model, img)\n",
        "            if cropped_img is None:\n",
        "                print(\"No plate detected — skipping image\")\n",
        "                continue\n",
        "            # Transform cropped image\n",
        "            tensor = transform(cropped_img).unsqueeze(0).to(device)\n",
        "            images.append(tensor)\n",
        "\n",
        "            if len(images) == 0:\n",
        "                continue  # skip batch if all images failed\n",
        "\n",
        "            images = torch.cat(images, dim=0)\n",
        "\n",
        "            unknown = set(c for s in label_strs for c in s if c not in char_idx)\n",
        "            if unknown:\n",
        "                # updating vocabulary\n",
        "                for c in ''.join(label_strs):\n",
        "                    if c not in char_idx:\n",
        "                        idx = len(char_idx)\n",
        "                        char_idx[c] = idx\n",
        "                        idx_char[idx] = c\n",
        "                print(f\"unknown character {c}. Vocabulary updated\")\n",
        "                # update the decoder with the new vocabulary size but keeping the old weights\n",
        "                decoder.update_vocab_size(len(char_idx))\n",
        "\n",
        "\n",
        "            # Forward\n",
        "            features = igfe(images)\n",
        "            encoded = encoder(features)\n",
        "            logits = decoder(encoded)\n",
        "\n",
        "            evaluator.update(logits, [label_strs])\n",
        "            pred_str = evaluator.greedy_decode(logits)\n",
        "            predicted_strings.append(pred_str)\n",
        "            #print(f\"traget string: {label_strs},  Predicted: {pred_str}\")\n",
        "\n",
        "        metrics = evaluator.compute()\n",
        "        evaluator.print()\n",
        "\n",
        "    return metrics, predicted_strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_GXdpR1TMoB",
        "outputId": "00ee1124-1832-4762-a052-9bbbb62cdc6d"
      },
      "outputs": [],
      "source": [
        "# loading yolo model\n",
        "yolo_model = YOLO(f\"runs/train/yolov5_epochs20_bs8_lr0.001_imgs640/weights/best.pt\")\n",
        "\n",
        "# Load data\n",
        "dataset = CCPDDataset(base_dir=f\"dataset\", transform=transform)\n",
        "_, _, test_loader = CCPDDataset.get_dataloaders(\n",
        "    base_dir=f\"dataset\",\n",
        "    batch_size=1,\n",
        "    transform=transform,\n",
        "    collate_fn=custom_collate_simple\n",
        ")\n",
        "# loading vocabulary\n",
        "if os.path.exists(f'vocab.json'):\n",
        "    print(f\"vocab.json found — loading...\")\n",
        "    char2idx, idx2char = load_vocab(f'vocab.json')\n",
        "else:\n",
        "    print(\"vocab.json not found — building it from labels...\")\n",
        "    char2idx, idx2char = build_vocab(f\"dataset/labels_pdlpr/train\", \"vocab.json\")\n",
        "\n",
        "vocab_size = len(char2idx)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "evaluator = Evaluator(idx2char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "8xrNPWYxVXL6",
        "outputId": "0ec10c1b-0115-49b0-c21f-462e9384123f"
      },
      "outputs": [],
      "source": [
        "print(\"start testing.........\")\n",
        "metrics, predicted_strings = test(\n",
        "    model_parts=(igfe, encoder, decoder),\n",
        "    yolo_model = yolo_model,\n",
        "    transform=transform,\n",
        "    evaluator=evaluator,\n",
        "    test_loader=test_loader,\n",
        "    char_idx=char2idx,\n",
        "    idx_char=idx2char,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(\"First 5 predicted strings:\", predicted_strings[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGug41Nsb-8M"
      },
      "source": [
        "### test Baseline method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbJpxW8yW50E"
      },
      "source": [
        "testing for the second part of the baseline method that uses CNN CTC, in order to execute this is important to initialize the Evaluator class which is found in Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "collapsed": true,
        "id": "2QqysWZSgry-",
        "outputId": "4e5951e6-dab9-4105-f1f2-a99cf59cf0cc"
      },
      "outputs": [],
      "source": [
        "CHAR_LIST = sorted(set(PROVINCES+ALPHABETS+ADS))\n",
        "PLATE_LENGTH = 8\n",
        "\n",
        "NUM_CHAR = len(CHAR_LIST) + 1 #since we include the blank character\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "LR = 0.001\n",
        "WEIGHT_DECAY = 0.0001\n",
        "NUM_EPOCHS = 60\n",
        "\n",
        "SAVE_NAME = f\"n_epochs_{NUM_EPOCHS}_bs_{BATCH_SIZE}_LR_{LR}_wd_{WEIGHT_DECAY}\"\n",
        "model = CNN_CTC_model(num_char=NUM_CHAR, hidden_size=256)\n",
        "ctc_loss = nn.CTCLoss(blank=0)\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Grayscale(),\n",
        "    transforms.Resize((48, 144)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "_, _, test_dataloader = CCPDDataset.get_dataloaders(base_dir=DATASET_PATH_Y, batch_size=BATCH_SIZE, transform=preprocess, collate_fn= custom_collate_2)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#TESTING PHASE\n",
        "if os.path.exists(f\"models/CNNCTC-{SAVE_NAME}.pth\"):\n",
        "    model.load_state_dict(torch.load(f\"models/CNNCTC-{SAVE_NAME}.pth\"))\n",
        "    model.to(device)\n",
        "else:\n",
        "    print(\"model not found. Please train the model first\")\n",
        "model.eval()\n",
        "test_acc = []\n",
        "char_test_acc = []\n",
        "\n",
        "evaluator = Evaluator()\n",
        "#here we just loop throught the test data and compute the accuracy scores\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        images = batch[\"cropped_image\"]\n",
        "        labels = batch[\"pdlpr_plate_idx\"]\n",
        "        images = [img.to(device) for img in images]\n",
        "        labels = [lab.to(device) for lab in labels]\n",
        "        images = torch.stack(images)\n",
        "        labels = torch.stack(labels)\n",
        "        flat_labels_list = labels.view(-1)\n",
        "\n",
        "        output_logits = model(images)\n",
        "        output_probabilities = F.log_softmax(output_logits, dim=2)\n",
        "        evaluator.reset()\n",
        "        evaluator.update_baseline(output_logits, labels)\n",
        "        metrics = evaluator.compute()\n",
        "        #metrics for the whole batch\n",
        "        mean_batch_test_char_acc = metrics[\"char_accuracy\"]\n",
        "        mean_batch_test_acc = metrics[\"seq_accuracy\"]\n",
        "        test_acc.append(mean_batch_test_acc)\n",
        "        char_test_acc.append(mean_batch_test_char_acc)\n",
        "\n",
        "mean_acc = sum(test_acc) / len(test_acc)\n",
        "mean_char_acc = sum(char_test_acc)/len(char_test_acc)\n",
        "print(f\"Test result accuracy: {mean_acc:.4f}\")\n",
        "print(f\"Test result char accuracy: {mean_char_acc:.4f}\")\n",
        "#saving the iou result of the training, validation (last step) and testing\n",
        "with open(f\"results/CNNCTC-test-{SAVE_NAME}.txt\", \"w\") as f:\n",
        "    f.write(f\"Final testing accuracy: {mean_acc:.4f}\\n\")\n",
        "    f.write(f\"Final testing character accuracy: {mean_char_acc:.4f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhGg5Gg6xSG4"
      },
      "source": [
        "### Test pipeline baseline method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxGPfANZc49u"
      },
      "source": [
        "Here we implement the pipeline testing, that tests the combination between the two parts for each method, in this code, the baseline so the function that computes the plate box using traditional methods then crops the images and does character recognition using cnn ctc model.\n",
        "\n",
        "The function plate_dectector is defined in utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cT9BfFzcxX3o",
        "outputId": "9e864cba-9524-4948-a3c2-3162ab673f10"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "reader = easyocr.Reader(['ch_sim', 'en'])\n",
        "\n",
        "CHAR_LIST = sorted(set(PROVINCES+ALPHABETS+ADS))\n",
        "PLATE_LENGTH = 8\n",
        "\n",
        "for idx, char in enumerate(CHAR_LIST):\n",
        "    CHAR_IDX[char] = idx + 1  # start from 1\n",
        "    IDX_CHAR[idx + 1] = char\n",
        "IDX_CHAR[0] = '_'  # blank character for CTC\n",
        "\n",
        "NUM_CHAR = len(CHAR_LIST) + 1 #since we include the blank character\n",
        "\n",
        "#Hyperparameters\n",
        "BATCH_SIZE = 32\n",
        "LR = 0.001\n",
        "WEIGHT_DECAY = 0.0001\n",
        "NUM_EPOCHS = 60\n",
        "SAVE_NAME = f\"n_epochs_{NUM_EPOCHS}_bs_{BATCH_SIZE}_LR_{LR}_wd_{WEIGHT_DECAY}\"\n",
        "\n",
        "cnn_ctc_model = CNN_CTC_model(num_char=NUM_CHAR, hidden_size=256)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Grayscale(),\n",
        "    transforms.Resize((48, 144)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "preprocess_dataset = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "#Load the cnnctc model for the second part\n",
        "print(f\"models/CNNCTC-{SAVE_NAME}.pth\")\n",
        "\n",
        "if os.path.exists(f\"models/CNNCTC-{SAVE_NAME}.pth\"):\n",
        "    cnn_ctc_model.load_state_dict(torch.load(f\"models/CNNCTC-{SAVE_NAME}.pth\"))\n",
        "    cnn_ctc_model.to(device)\n",
        "else:\n",
        "    print(\"model not found. Please train the model first\")\n",
        "\n",
        "#Initialize the path for the languages\n",
        "image_paths = Path(f\"dataset/images/test\")\n",
        "\n",
        "evaluator = Evaluator(idx2char=IDX_CHAR)\n",
        "cnn_ctc_model.eval()\n",
        "plate_accuracies = []\n",
        "char_accuracies = []\n",
        "iou_scores = []\n",
        "\n",
        "i=0\n",
        "for image_path in image_paths.glob(\"*.jpg\"):\n",
        "    if i % 10 == 0:\n",
        "        print(f\"processing image {i+1}/{55000}\")\n",
        "    image_name = image_path.name\n",
        "    plate_label_path = Path(f\"dataset/labels_pdlpr/test/\") / (image_path.stem + \".txt\")\n",
        "\n",
        "    with open(plate_label_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        pdlpr_plate_str = f.readline().strip()\n",
        "    #print(pdlpr_plate_str)\n",
        "\n",
        "    fields = image_path.stem.split(\"-\")\n",
        "    bbox_part = fields[2]\n",
        "    corners = bbox_part.split(\"_\")\n",
        "    x1, y1 = map(int, corners[0].split(\"&\"))\n",
        "    x2, y2 = map(int, corners[1].split(\"&\"))\n",
        "\n",
        "    true_box = [x1, y1, x2, y2]\n",
        "    #print(true_box)\n",
        "    plate_number = fields[4]\n",
        "    character_id_list = plate_number.split(\"_\")\n",
        "    plate_id = []\n",
        "    for c in character_id_list:\n",
        "        plate_id.append(int(c))\n",
        "\n",
        "    #converting the index from the name to the index from the\n",
        "    #unified vocabulary\n",
        "    plate_id= target_to_index(plate_id)\n",
        "    #print(plate_id)\n",
        "\n",
        "    true_plate_idx = torch.tensor(plate_id, dtype=torch.long).to(device)\n",
        "\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    bbx =[]\n",
        "    result_detector = plate_detector(image_path, true_box)\n",
        "    if result_detector is None:\n",
        "        print(f\"image {i}: No plate detected\")\n",
        "        i+=1\n",
        "        continue\n",
        "\n",
        "    bbx, text = result_detector\n",
        "    print(i, bbx, text)\n",
        "    iou = compute_iou(bbx, true_box)\n",
        "    print(f\"image {i}: iou score {iou}\")\n",
        "    iou_scores.append(iou)\n",
        "    x1,y1, x2, y2 = bbx\n",
        "    cropped_image = image.crop((x1, y1, x2, y2))\n",
        "\n",
        "    #import matplotlib.pyplot as plt\n",
        "    plt.imshow(cropped_image, cmap=\"gray\")\n",
        "    plt.title(\"Cropped Plate\")\n",
        "    plt.axis(\"off\")\n",
        "    #plt.show()\n",
        "\n",
        "    processed_image =preprocess(cropped_image).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        #computing the predictions wiht the cnn ctc model\n",
        "        logits_model_output = cnn_ctc_model(processed_image)\n",
        "        evaluator.reset()\n",
        "        evaluator.update_baseline(logits_model_output, [true_plate_idx])\n",
        "        metrics = evaluator.compute()\n",
        "\n",
        "        char_acc = metrics[\"char_accuracy\"]\n",
        "        plate_acc = metrics[\"seq_accuracy\"]\n",
        "        char_accuracies.append(char_acc)\n",
        "        plate_accuracies.append(plate_acc)\n",
        "        print(f\"  Char acc: {char_acc:.2f}, Seq acc: {plate_acc:.2f}\\n\")\n",
        "        plate_prediction = evaluator.greedy_decode_idx(logits_model_output)[0]\n",
        "        print(plate_prediction[:8])\n",
        "        plate_string = index_to_target(plate_prediction[:8])\n",
        "        print(f\"predicted_plate: {''.join(plate_string)}, original plate: {pdlpr_plate_str}\")\n",
        "\n",
        "    i+=1\n",
        "\n",
        "mean_char_acc = sum(char_accuracies) / len(char_accuracies)\n",
        "mean_plate_acc = sum(plate_accuracies)/len(plate_accuracies)\n",
        "mean_iou = sum(iou_scores)/len(iou_scores)\n",
        "print(f\"Pipeline test result plate accuracy: {mean_plate_acc:.4f}\")\n",
        "print(f\"Pipeline test result char accuracy: {mean_char_acc:.4f}\")\n",
        "print(f\"Pipeline test result iou score: {mean_iou:.4f}\")\n",
        "#saving the iou result of the training, validation (last step) and testing\n",
        "with open(f\"results/pipeline-baseline-test-{SAVE_NAME}.txt\", \"w\") as f:\n",
        "    f.write(f\"Final testing plate accuracy: {mean_plate_acc:.4f}\\n\")\n",
        "    f.write(f\"Final testing character accuracy: {mean_char_acc:.4f}\\n\")\n",
        "    f.write(f\"Final testing iou score: {mean_iou:.4f}\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "bRoLLAHCMhTG",
        "E_PZ6aiSU_Jk",
        "_mR_Eir2Mhju",
        "L1KTU3FkMhmW",
        "1gHQ8bVQxCkG",
        "DA_c11Er4uEj",
        "KnbuZU1I5ZGa",
        "Hnmq-soL5osZ",
        "JUYEDzyHxIDn",
        "d8HBAOmm6Otb",
        "W58PEoMpSRgV",
        "t7HqGhcYSZXr",
        "pGug41Nsb-8M",
        "AhGg5Gg6xSG4"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
