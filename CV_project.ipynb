{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9-ZzQhCL-Lt"
      },
      "source": [
        "# Computer vision project\n",
        "\n",
        "- Carlotta Anna Maria Ciani 1881291\n",
        "- Michela Fuselli 1883535\n",
        "- Simone Federico Laganà 1946083\n",
        "\n",
        "Project 4 car plate recognition\n",
        "\n",
        "Folder structure in this notebook\n",
        " -  imports: contains the imports and the necessary libaries for executing the whole code\n",
        " - globals: it includes the global variables\n",
        " - utils: contains various functions as well as the definition of the evaluator class, and the function to perform baseline plate detection\n",
        " - network: defines the networks for PDLPR and the CNN CTC network\n",
        " - data: defining the dataset class\n",
        " - train: this is divided into the training code for YOLO, for PDLPR and for CNN + CTC under the name Baseline method where there is also the implementation fo the traditional plate detection method.\n",
        " - test: Here there is the testing for the single components YOLO, PDLPR and CNN + CTC as well as the pipeline for yolo + pdlpr and the baseline pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRoLLAHCMhTG"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "z-7qkQbJXRL0",
        "outputId": "57e308af-8136-4790-80e0-7f7a4dd69679"
      },
      "outputs": [],
      "source": [
        "%pip install ultralytics\n",
        "%pip install easyocr\n",
        "%pip install gdown "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TWWIQ0MMiuN",
        "outputId": "3f01d2aa-9783-4da9-8168-df355ec63110"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import torch.optim as optim\n",
        "from torch.optim import Adam, SGD\n",
        "\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from ultralytics import YOLO\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import easyocr\n",
        "import json\n",
        "import gdown\n",
        "\n",
        "#drive.mount('/content/drive')\n",
        "#path_to_shared_folder = '/content/drive/MyDrive/cv_project_folder/'\n",
        "\n",
        "#download the best pdlpr model from google drive, since the file is too big to be stored on the repository\n",
        "url = \"https://drive.google.com/uc?id=1PR8ygH66VKKDOaFpoxzR7aLqc_H5VUtC\"\n",
        "output = \"models/pdlpr_10_0.0001_16.pt\"\n",
        "gdown.download(url, output, quiet=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7_eAV_pMhbO"
      },
      "source": [
        "## Globals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnPEcperJPWq"
      },
      "source": [
        "initialization of all the global variables used in this notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiORVTBjMjX2"
      },
      "outputs": [],
      "source": [
        "#General\n",
        "\n",
        "#character mapping used for the chinese plates\n",
        "PROVINCES = [\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\", \"O\"]\n",
        "ALPHABETS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'O']\n",
        "ADS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n",
        "\n",
        "#sorted list of all the unique characters that could appear in a plate\n",
        "CHAR_LIST = sorted(set(PROVINCES + ALPHABETS +ADS))\n",
        "\n",
        "#dictionary containing the character and the corresponding index\n",
        "CHAR_IDX = {}\n",
        "IDX_CHAR = {}\n",
        "for idx, char in enumerate(CHAR_LIST):\n",
        "    CHAR_IDX[char] = idx + 1  # start from 1\n",
        "    IDX_CHAR[idx + 1] = char\n",
        "IDX_CHAR[0] = '_'  # blank character for CTC\n",
        "\n",
        "DATASET_PATH_Y      = f\"dataset\"\n",
        "IMAGE_SIZE_Y        = 640\n",
        "\n",
        "#YOLOv5\n",
        "IOU_THRESHOLD = 0.7\n",
        "BATCH_SIZE_TRAIN_Y  = 20        # paper:50\n",
        "BATCH_SIZE_TEST_Y   = 4\n",
        "EPOCHS_TRAIN_Y      = 25        # paper: 300\n",
        "LR_INIT_Y           = 0.001     # initial learning rate\n",
        "\n",
        "\n",
        "#PDLPR\n",
        "BATCH_SIZE_PDLPR = 16\n",
        "LR_PDLPR = 1e-4 #0.00001, mostly used\n",
        "NUM_EPOCHS_PDLPR = 5\n",
        "WEIGHT_DECAY_PDLPR = 0.0001\n",
        "\n",
        "#CNN ctc\n",
        "#(here there are the hyperparameters with the best performances among the ones tried)\n",
        "BATCH_SIZE_CNN = 32\n",
        "LR_CNN = 0.001\n",
        "NUM_EPOCHS_CNN = 60\n",
        "WEIGHT_DECAY_CNN = 0.0001\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNFOfY6GMhfe"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVydzOv9YmT9"
      },
      "source": [
        "### useful functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPC1-L-fMj3t"
      },
      "outputs": [],
      "source": [
        "base_dir = Path(DATASET_PATH_Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e9SEG4d1vcZ"
      },
      "source": [
        "This code extracts the labels and puts them into folders, structured in a similar way of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anTWin7gX6sW"
      },
      "outputs": [],
      "source": [
        "def initialize_labels():\n",
        "    #initialize the labels and creating the folders\n",
        "    splits = [\"train\", \"val\", \"test\"]\n",
        "    for split in splits:\n",
        "        # It is just a safe and readable way to say: go to datasets/ccpd/images/train (or val, or test), depending on which split you're processing.\n",
        "        image_dir = base_dir / \"images\" / split\n",
        "        label_dir = base_dir / \"labels\" / split\n",
        "        crops_dir = base_dir / \"crops\" / split\n",
        "        label_pdlpr_dir = base_dir / \"labels_pdlpr\" / split\n",
        "\n",
        "        label_dir.mkdir(parents=True, exist_ok=True)    # creates the folder if it does not exist\n",
        "        crops_dir.mkdir(parents=True, exist_ok=True)\n",
        "        label_pdlpr_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Loop through all .jpg images in the current image directory\n",
        "        #the tqdm library is useful to plot the loading bar\n",
        "        for image_path in tqdm(list(image_dir.glob(\"*.jpg\")), desc=f\"Processing - {split}\", unit=\"img\"):\n",
        "            #print(f\"Found image: {image_path}\")\n",
        "            #print(f\"Processing: {image_path.name}\")\n",
        "\n",
        "            # Parse bounding box from filename: example => \"XXXXX&x1_x2_y1_y2&...\"\n",
        "            try:\n",
        "                fields = image_path.stem.split(\"-\")    # image_path.stem is the filename without .jpg\n",
        "\n",
        "                # Field 2 (index 2) is bbox: format is \"x1&y1_x2&y2\"\n",
        "                bbox_part = fields[2]\n",
        "                corners = bbox_part.split(\"_\")\n",
        "                x1, y1 = map(int, corners[0].split(\"&\"))\n",
        "                x2, y2 = map(int, corners[1].split(\"&\"))\n",
        "\n",
        "                # Define min/max values\n",
        "                x_min = min(x1, x2)\n",
        "                x_max = max(x1, x2)\n",
        "                y_min = min(y1, y2)\n",
        "                y_max = max(y1, y2)\n",
        "\n",
        "\n",
        "                #extracting the information about the plate to create the labels for pdlpr\n",
        "                #the plate is in this format 0_0_22_27_27_33_16\n",
        "                plate_number = fields[4]\n",
        "                character_id_list = plate_number.split(\"_\")\n",
        "                #get the number for the province and for the letter\n",
        "                province_id = int(character_id_list[0])\n",
        "                alphabet_id = int(character_id_list[1])\n",
        "                #get the actual character for both and join them\n",
        "                province_char = PROVINCES[province_id]\n",
        "                alphabet_char = ALPHABETS[alphabet_id]\n",
        "                plate = province_char + alphabet_char\n",
        "\n",
        "                for i in range(2, 8):\n",
        "                    #for the remaining 5 characters we do the mapping from the ADS\n",
        "                    ads_index = int(character_id_list[i])\n",
        "                    plate += ADS[ads_index]\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping {image_path.name}: {e}\")\n",
        "                continue\n",
        "            # Read the image to get image size (needed to normalize the coordinates)\n",
        "            img = Image.open(image_path)\n",
        "\n",
        "            img_width, img_height = img.size\n",
        "\n",
        "            #crop the image according to the bounding box coordinates\n",
        "            cropped_img = img.crop((x_min, y_min, x_max, y_max))\n",
        "\n",
        "            #Adding crops so cut images into a separate folder\n",
        "            crops_path = crops_dir / (image_path.stem + \".jpg\")\n",
        "\n",
        "            #saving the image into the crops folder\n",
        "            cropped_img.save(crops_path)\n",
        "\n",
        "            img.close()\n",
        "\n",
        "\n",
        "            # Normalize the bounding box for YOLO format\n",
        "            x_center = ((x_min + x_max) / 2) / img_width\n",
        "            y_center = ((y_min + y_max) / 2) / img_height\n",
        "            width = (x_max - x_min) / img_width\n",
        "            height = (y_max - y_min) / img_height\n",
        "\n",
        "            # Create YOLO label string\n",
        "            # 0 is the class ID (only one class - license plate)\n",
        "            # the rest are floats with 6 digits after the decimal point\n",
        "            label_str = f\"0 {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\"\n",
        "\n",
        "            # Save label file with same name\n",
        "            label_path = label_dir / (image_path.stem + \".txt\")\n",
        "            with open(label_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(label_str + \"\\n\")\n",
        "\n",
        "            #print(f\"Wrote label: {label_path.name}\")\n",
        "\n",
        "            #Save the label for PDLPR\n",
        "            label_pdl_pr_path = label_pdlpr_dir / (image_path.stem + \".txt\")\n",
        "            with open(label_pdl_pr_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(plate + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fYO-ULQJxmC"
      },
      "source": [
        "This are some useful functions that are used in this notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIoX3jszSEjk"
      },
      "outputs": [],
      "source": [
        "def yoloprediction_to_pdlpr_input(x_center, y_center, width, height, image_path):\n",
        "    #This functions takes in input the prediction from yolo and returns the cropped image (so the input for pdlpr)\n",
        "    img = Image.open(image_path)\n",
        "\n",
        "    image_width, image_height = img.size\n",
        "\n",
        "    x_center_pixel = x_center * image_width\n",
        "    y_center_pixel = y_center * image_height\n",
        "    width_pixel = width * image_width\n",
        "    height_pixel = height * image_height\n",
        "\n",
        "    x_min = int(x_center_pixel - width_pixel / 2)\n",
        "    x_max = int(x_center_pixel + width_pixel / 2)\n",
        "    y_min = int(y_center_pixel - height_pixel / 2)\n",
        "    y_max = int(y_center_pixel + height_pixel / 2)\n",
        "\n",
        "    #crop the image according to the bounding box coordinates\n",
        "    cropped_img = img.crop((x_min, y_min, x_max, y_max))\n",
        "\n",
        "    return cropped_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXsatw_VSHsD"
      },
      "source": [
        "This function computes the intersection over union given two boxes [x1, y1, x2, y2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYE9zft7SIME"
      },
      "outputs": [],
      "source": [
        "def compute_iou(box_1, box_2):\n",
        "    #it is a metric that involves the intersection of the two areas\n",
        "    #over the union, and returns a matching percentage\n",
        "\n",
        "    #coputing the coordinate of the intersections\n",
        "    x1 = max(box_1[0], box_2[0])\n",
        "    y1 = max(box_1[1], box_2[1])\n",
        "    x2 = min(box_1[2], box_2[2])\n",
        "    y2 = min(box_1[3], box_2[3])\n",
        "\n",
        "    interArea = max(0, x2 - x1) * max(0, y2 - y1)\n",
        "    boxAArea = (box_1[2] - box_1[0]) * (box_1[3] - box_1[1])\n",
        "    boxBArea = (box_2[2] - box_2[0]) * (box_2[3] - box_2[1])\n",
        "\n",
        "    iou = interArea / float(boxAArea + boxBArea - interArea + 1e-6)  #+1e-6 is used to avoid the division per zero\n",
        "    return iou"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bWuwEWaStOk"
      },
      "source": [
        "this function converts the id of the target into the ids that are returned by the model since it uses the ids from the sorted list of all the possible characters.\n",
        "\n",
        "index_to_target converts the index list into the plate characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3kkl-epS68s"
      },
      "outputs": [],
      "source": [
        "def target_to_index(target_list):\n",
        "    #this function converts the id of the target\n",
        "    #into the ids that are returned by the model\n",
        "    #since it uses the ids from the sorted list of\n",
        "    #all the possible characters\n",
        "    output = []\n",
        "    province = PROVINCES[target_list[0]]\n",
        "    alphabet = ALPHABETS[target_list[1]]\n",
        "    output.append(CHAR_IDX[province])\n",
        "    output.append(CHAR_IDX[alphabet])\n",
        "    for char_idx in range(2,8):\n",
        "        char = ADS[target_list[char_idx]]\n",
        "        output.append(CHAR_IDX[char])\n",
        "    return output\n",
        "\n",
        "\n",
        "def index_to_target(index_list):\n",
        "    output=[]\n",
        "    for idx in index_list:\n",
        "        output.append(IDX_CHAR[idx])\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIlNhOinS-Js"
      },
      "source": [
        "These funcitions aim at retrieving the coordinates of the true bounding box (ground truth), for both validation and testing. The format returned is [x1, y1, x2, y2], if not label is found or it is invalid, None is returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9cp_DvP2BQD"
      },
      "outputs": [],
      "source": [
        "def load_gt_box_from_label_validation(image_path):\n",
        "    label_path = Path(f\"dataset/labels/val\") / (image_path.stem + \".txt\")\n",
        "\n",
        "    if not label_path.exists():\n",
        "        print(f\"[WARN] No label found for {image_path.name}\")\n",
        "        return None\n",
        "\n",
        "    with open(label_path, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    if not lines:\n",
        "        print(f\"[WARN] Empty label file for {image_path.name}\")\n",
        "        return None\n",
        "\n",
        "    # Assume the first object only\n",
        "    try:\n",
        "        parts = list(map(float, lines[0].strip().split()))\n",
        "        _, x_center, y_center, w, h = parts\n",
        "    except Exception:\n",
        "        print(f\"[WARN] Label parse failed for {image_path.name}\")\n",
        "        return None\n",
        "\n",
        "    # Convert from normalized to absolute coordinates\n",
        "    img = plt.imread(image_path)\n",
        "    img_h, img_w = img.shape[:2]\n",
        "\n",
        "    cx, cy = x_center * img_w, y_center * img_h\n",
        "    bw, bh = w * img_w, h * img_h\n",
        "\n",
        "    x1, y1 = cx - bw / 2, cy - bh / 2\n",
        "    x2, y2 = cx + bw / 2, cy + bh / 2\n",
        "\n",
        "    return [x1, y1, x2, y2]\n",
        "\n",
        "\n",
        "def load_gt_box_from_label_test(image_path):\n",
        "    label_path = Path(f\"dataset/labels/test\") / (image_path.stem + \".txt\")\n",
        "\n",
        "    if not label_path.exists():\n",
        "        print(f\"[WARN] No label found for {image_path.name}\")\n",
        "        return None\n",
        "\n",
        "    with open(label_path, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    if not lines:\n",
        "        print(f\"[WARN] Empty label file for {image_path.name}\")\n",
        "        return None\n",
        "\n",
        "    # Assume the first object only\n",
        "    try:\n",
        "        parts = list(map(float, lines[0].strip().split()))\n",
        "        _, x_center, y_center, w, h = parts\n",
        "    except Exception:\n",
        "        print(f\"[WARN] Label parse failed for {image_path.name}\")\n",
        "        return None\n",
        "\n",
        "    # Convert from normalized to absolute coordinates\n",
        "    img = plt.imread(image_path)\n",
        "    img_h, img_w = img.shape[:2]\n",
        "\n",
        "    cx, cy = x_center * img_w, y_center * img_h\n",
        "    bw, bh = w * img_w, h * img_h\n",
        "\n",
        "    x1, y1 = cx - bw / 2, cy - bh / 2\n",
        "    x2, y2 = cx + bw / 2, cy + bh / 2\n",
        "\n",
        "    return [x1, y1, x2, y2]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWNTB7XCh6V2"
      },
      "source": [
        "function that builds the vocabulary (chinese regions) that will be used to build idx2char and char2idx, in particular the inputs are **label_folder** (str), which is the path to folder containing license plate label .txt files, **file_name** (str) is the file .json that will contain the vocabulary for later use and **include_blank** (bool) that tells whether to reserve index 0 for the CTC blank token ('-')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBGwecxWh4-D"
      },
      "outputs": [],
      "source": [
        "def build_vocab(label_folder, file_name, include_blank=True):\n",
        "    vocab = set()\n",
        "\n",
        "    for filename in os.listdir(label_folder):\n",
        "        if not filename.endswith(\".txt\"):\n",
        "            continue\n",
        "        with open(os.path.join(label_folder, filename), \"r\", encoding=\"utf-8\") as f:\n",
        "            label = f.read().strip().upper()\n",
        "            vocab.update(label)\n",
        "\n",
        "    # doing sorting for consinstency\n",
        "    vocab = sorted(vocab)\n",
        "    char2idx = {}\n",
        "    idx2char = {}\n",
        "    start_idx = 0\n",
        "\n",
        "    if include_blank:\n",
        "        char2idx[\"-\"] = 0  # CTC blank\n",
        "        idx2char[0] = \"-\"\n",
        "        start_idx = 1\n",
        "\n",
        "    for i, ch in enumerate(vocab, start=start_idx):\n",
        "        char2idx[ch] = i\n",
        "        idx2char[i] = ch\n",
        "\n",
        "    with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(char2idx, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"[vocab] Built vocabulary with {len(char2idx)} characters.\")\n",
        "    return char2idx, idx2char"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPsGMsn2zcqH"
      },
      "source": [
        "Function that loads the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFwXxgEJzbqB"
      },
      "outputs": [],
      "source": [
        "def load_vocab(path=\"vocab.json\"):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        char_idx = json.load(f)\n",
        "    idx_char = {int(v): k for k, v in char_idx.items()}\n",
        "    return char_idx, idx_char"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XES82yIgzcDk"
      },
      "source": [
        "Function that plots PDLPR metrics comparing training and validation, in particular the metrics are: character accuracy, sequence accuracy and levenshtein distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HnkwwZdzpTG"
      },
      "outputs": [],
      "source": [
        "def plot_metrics(train_seq, val_seq, train_char, val_char, train_lev, val_lev):\n",
        "    epochs = range(1, NUM_EPOCHS_PDLPR + 1)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, [l.detach().cpu().item() if torch.is_tensor(l) else l for l in train_seq], label=\"Train Seq Accuracy\")\n",
        "    plt.plot(epochs, [l.detach().cpu().item() if torch.is_tensor(l) else l for l in val_seq], label=\"Val Seq Accuracy\")\n",
        "    plt.title(\"Sequence Accuracy over Epochs\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(f\"metrics_images/seq_accs_plot_{NUM_EPOCHS_PDLPR}_{LR_PDLPR}_{BATCH_SIZE_PDLPR}.png\", dpi=300)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, [l.detach().cpu().item() if torch.is_tensor(l) else l for l in train_char], label=\"Train Char Accuracy\")\n",
        "    plt.plot(epochs, [l.detach().cpu().item() if torch.is_tensor(l) else l for l in val_char], label=\"Val Char Accuracy\")\n",
        "    plt.title(\"Char Accuracy over Epochs\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(f\"metrics_images/char_accs_plot{NUM_EPOCHS_PDLPR}_{LR_PDLPR}_{BATCH_SIZE_PDLPR}.png\", dpi=300)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, [l.detach().cpu().item() if torch.is_tensor(l) else l for l in train_lev], label=\"Train Levenshtein distance\")\n",
        "    plt.plot(epochs, [l.detach().cpu().item() if torch.is_tensor(l) else l for l in val_lev], label=\"Val Levenshtein distance\")\n",
        "    plt.title(\"Levenshtein distance over Epochs\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Lev distance\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(f\"metrics_images/levenshtein_plot{NUM_EPOCHS_PDLPR}_{LR_PDLPR}_{BATCH_SIZE_PDLPR}.png\", dpi=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGwGO7U4zboN"
      },
      "source": [
        "Functions for processing the batches returned from the dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJizNMT6za49"
      },
      "outputs": [],
      "source": [
        "# functions for pdlpr\n",
        "def custom_collate_simple(batch):\n",
        "    return batch\n",
        "\n",
        "def custom_collate(batch):\n",
        "    return {\n",
        "        \"cropped_image\": torch.stack([item[\"cropped_image\"] for item in batch]),\n",
        "        \"pdlpr_plate_string\": [item[\"pdlpr_plate_string\"] for item in batch],\n",
        "        # add other fields as needed\n",
        "    }\n",
        "\n",
        "def custom_collate_2(batch):\n",
        "    return {\n",
        "        \"cropped_image\": torch.stack([item[\"cropped_image\"] for item in batch]),\n",
        "        \"pdlpr_plate_idx\": [item[\"pdlpr_plate_idx\"] for item in batch],\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_PZ6aiSU_Jk"
      },
      "source": [
        "### Evaluator class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WC-HsHUYVDmU"
      },
      "source": [
        "Evaluator class: this class will help in computing the metrics to evaluate the models (char accuracy, sequence accuracy and lev distance). Since there are a lot of variables to look after, we found more convenient to impement a class.\n",
        "\n",
        "There are two different versions of the same methods, greedy_decode/greedy_decoede_idx  and update/update_baseline. This because the two models produce slightly different outputs so they need different treatment to decode and when it comes to compare the predictions to the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-CVjjooU-Al"
      },
      "outputs": [],
      "source": [
        "class Evaluator:\n",
        "    def __init__(self, idx2char={}, blank_index=0):\n",
        "        self.idx2char = idx2char\n",
        "        self.blank_index = blank_index\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.total_chars = 0\n",
        "        self.correct_chars = 0\n",
        "        self.correct_seqs = 0\n",
        "        self.total_samples = 0\n",
        "\n",
        "    def greedy_decode(self, logits):\n",
        "        # logits: [B, T, C]\n",
        "        predictions = torch.argmax(logits, dim=-1)  # [B, T]\n",
        "        decoded = []\n",
        "\n",
        "        for prediction in predictions:\n",
        "            prev = self.blank_index\n",
        "            chars = []\n",
        "            for idx in prediction:\n",
        "                idx = idx.item()\n",
        "                if idx != self.blank_index and idx != prev:\n",
        "                    chars.append(self.idx2char[idx])\n",
        "                prev = idx\n",
        "            decoded.append(\"\".join(chars))\n",
        "        return decoded\n",
        "\n",
        "    def greedy_decode_idx(self, logits):\n",
        "        predictions = torch.argmax(logits, dim=2)\n",
        "        predictions= predictions.transpose(0, 1)\n",
        "        final_predictions = []\n",
        "        #iterate for each prediction array in the batch\n",
        "        for prediction in predictions:\n",
        "            before = 0\n",
        "            reduced = []\n",
        "            for t_index in prediction:\n",
        "                t_index = t_index.item()\n",
        "                if t_index != 0 and t_index != before:\n",
        "                    #append the index only if it is not zero and it is different than before\n",
        "                    reduced.append(t_index)\n",
        "                before = t_index\n",
        "            final_predictions.append(reduced)\n",
        "        return final_predictions\n",
        "\n",
        "    def update(self, logits, target_strs):\n",
        "        # logits: [B, T, vocab_size]\n",
        "        pred_strs = self.greedy_decode(logits)\n",
        "\n",
        "        for pred, true in zip(pred_strs, target_strs):\n",
        "            self.total_samples += 1\n",
        "            self.total_chars += len(true)\n",
        "            correct = sum(p == t for p, t in zip(pred, true))\n",
        "            self.correct_chars += correct\n",
        "            if pred == true:\n",
        "                self.correct_seqs += 1\n",
        "\n",
        "    def update_baseline(self, logits, labels):\n",
        "\n",
        "        final_predictions = self.greedy_decode_idx(logits)\n",
        "        for pred_idx_list, label in zip(final_predictions, labels):\n",
        "            label_list = label.tolist()\n",
        "            if pred_idx_list == label_list:\n",
        "                self.correct_seqs +=1\n",
        "\n",
        "            self.total_samples += 1\n",
        "            self.total_chars += len(label)\n",
        "            correct = 0\n",
        "            for pred_idx, label_idx in zip(pred_idx_list, label):\n",
        "                if pred_idx == label_idx:\n",
        "                    correct += 1\n",
        "            self.correct_chars += correct\n",
        "\n",
        "    def compute(self):\n",
        "        char_acc = self.correct_chars / self.total_chars if self.total_chars > 0 else 0.0\n",
        "        seq_acc = self.correct_seqs / self.total_samples if self.total_samples > 0 else 0.0\n",
        "        return {\n",
        "            \"char_accuracy\": char_acc,\n",
        "            \"seq_accuracy\": seq_acc,\n",
        "        }\n",
        "\n",
        "    def print(self):\n",
        "        metrics = self.compute()\n",
        "        print(f\"Character accuracy:  {metrics['char_accuracy']:.4f}\")\n",
        "        print(f\"Sequence accuracy:   {metrics['seq_accuracy']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MERiSC1xedhO"
      },
      "source": [
        "### Function for baseline plate detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUrnobb6ejM-"
      },
      "source": [
        "This function is used in the \"training\" for the plate recognition phase of the baseline methods, it uses traditional techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NV6JDfiGeh_G"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_label_yolo(label_path):\n",
        "    with open(label_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        line = f.readline().strip()\n",
        "    parts = list(map(float, line.split()))\n",
        "    # It returns x, y, width, height\n",
        "    return torch.tensor(parts[1:], dtype=torch.float32)\n",
        "\n",
        "\n",
        "def get_ground_truth_coordinates(yolo_tensor, image_width, image_height):\n",
        "        cx, cy, w, h = yolo_tensor.tolist()\n",
        "\n",
        "        x_min = (cx - w / 2) * image_width\n",
        "        y_min = (cy - h / 2) * image_height\n",
        "        x_max = (cx + w / 2) * image_width\n",
        "        y_max = (cy + h / 2) * image_height\n",
        "\n",
        "        return [x_min, y_min, x_max, y_max]\n",
        "\n",
        "\n",
        "def plate_detector(image_path, true_coordinates):\n",
        "    # Load the image\n",
        "    img = cv2.imread(str(image_path))\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Isolate green parts (the plates have a black text on a green backgroung)\n",
        "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)              # Convert from RGB to HSV (to filter colours basing on the tone)\n",
        "    lower_green = np.array([40, 40, 40])                    # HSV tone range for green\n",
        "    upper_green = np.array([80, 255, 255])\n",
        "    mask = cv2.inRange(hsv, lower_green, upper_green)       # Create a binary mask\n",
        "\n",
        "    # Morphology\n",
        "    # The edge detector returns multiple fragmented contours,\n",
        "    # I need to use morphological operations to bound together near lines\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n",
        "    cleaned_mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
        "\n",
        "    # Edge detector --> Canny\n",
        "    edges = cv2.Canny(cleaned_mask, 100, 200)\n",
        "\n",
        "    # Found the contours (borders)\n",
        "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # Geometric filter + OCR check to see if there are numbers/letters\n",
        "    # Discard regiorns where there are no plates\n",
        "    candidates = []\n",
        "    for cnt in contours:\n",
        "        x, y, w, h = cv2.boundingRect(cnt)\n",
        "        aspect_ratio = w / float(h)\n",
        "        area = w * h\n",
        "        if not (2.5 < aspect_ratio < 6 and 1000 < area < 40000):\n",
        "            continue\n",
        "\n",
        "        roi = img_rgb[y:y+h, x:x+w]\n",
        "        result = reader.readtext(roi)\n",
        "\n",
        "        if result:\n",
        "            text = result[0][1]\n",
        "            conf = result[0][2]\n",
        "            clean_text = text.strip().replace(\" \", \"\").replace(\"\\n\", \"\")\n",
        "        else:\n",
        "            clean_text = \"\"\n",
        "            conf = 0.0\n",
        "\n",
        "        iou = compute_iou([x, y, x+w, y+h], true_coordinates)\n",
        "        ocr_score = len(clean_text) if len(clean_text) >= 4 else 0\n",
        "\n",
        "        # score = iou + 1 * ocr_score     # OCR weights more\n",
        "        score = 1.5 * (ocr_score / 8.0) + 0.5 * iou     # OCR weights more but I still consider iou\n",
        "\n",
        "        candidates.append({\n",
        "            \"bbox\": [x, y, x+w, y+h],\n",
        "            \"text\": clean_text,\n",
        "            \"score\": score\n",
        "        })\n",
        "\n",
        "    if not candidates:\n",
        "        return None\n",
        "\n",
        "    best = max(candidates, key=lambda c: c[\"score\"])\n",
        "    return best[\"bbox\"], best[\"text\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mR_Eir2Mhju"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3KrWDAZH8Gx"
      },
      "source": [
        "Definition of the dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYNQqdkrMkWd"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CCPDDataset(Dataset):\n",
        "    #This class helps to manage the elements from the CCPDD dataset\n",
        "    #and also initialized the batchloaders used during the training test and validation phases\n",
        "\n",
        "    def __init__(self, base_dir, transform=None):\n",
        "\n",
        "        self.base_dir = Path(base_dir)\n",
        "        self.transform = transform\n",
        "\n",
        "    def get_dataset(self, split):\n",
        "        #this is used to set the split from which we are initializing the dataset\n",
        "        #like train, test, val\n",
        "\n",
        "        self.image_dir = self.base_dir / \"images\" / split\n",
        "        self.label_yolo_dir = self.base_dir / \"labels\" / split\n",
        "        #directories for the labels and cropped images that are going to be used\n",
        "        #in the character recognition part with pdlpr\n",
        "        self.crops_dir = self.base_dir / \"crops\" / split\n",
        "        self.label_pdlpr_dir = self.base_dir / \"labels_pdlpr\" / split\n",
        "\n",
        "        # List all image files in the current split's image directory\n",
        "        self.image_files = sorted(list(self.image_dir.glob(\"*.jpg\")))\n",
        "\n",
        "        # List the cropped image files if PDLPR needs them directly\n",
        "        self.cropped_image_files = sorted(list(self.crops_dir.glob(\"*.jpg\")))\n",
        "\n",
        "        # Basic validation to ensure files exist\n",
        "        if not self.image_files:\n",
        "            raise FileNotFoundError(f\"No .jpg images found in {self.image_dir}\")\n",
        "        if not self.cropped_image_files:\n",
        "            raise FileNotFoundError(f\"No cropped .jpg images found in {self.crops_dir}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        # Returns the total number of samples in the dataset\n",
        "        return len(self.image_files)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Retrieves a single data sample given the index\n",
        "\n",
        "        #initializes the paths\n",
        "\n",
        "        img_path = self.image_files[index]\n",
        "        yolo_label_path = self.label_yolo_dir / (img_path.stem + \".txt\")\n",
        "        cropped_img_path = self.cropped_image_files[index]\n",
        "        pdlpr_label_path = self.label_pdlpr_dir / (img_path.stem + \".txt\")\n",
        "\n",
        "        img_name = img_path.name\n",
        "\n",
        "        # Open images\n",
        "        # Ensure 'RGB' conversion if images might be grayscale to be consistent for models\n",
        "        # This helps with GPU optimization as models typically expect 3 channels\n",
        "        full_image = Image.open(img_path).convert(\"RGB\")\n",
        "        cropped_image = Image.open(cropped_img_path).convert(\"RGB\")\n",
        "\n",
        "        # Read YOLO label (bounding box) from the text file\n",
        "        with open(yolo_label_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            yolo_label_str = f.readline().strip()\n",
        "\n",
        "        # Check if the label file is empty or malformed\n",
        "        if not yolo_label_str:\n",
        "            raise ValueError(f\"Empty label in {yolo_label_path}\")\n",
        "\n",
        "        # Assuming YOLO format: \"class_id x_center y_center width height\"\n",
        "        # We only have one class (0), so we can discard it or keep it\n",
        "\n",
        "        parts = list(map(float, yolo_label_str.split()))\n",
        "        # parts is a list of floats like [0.0, 0.5, 0.4, 0.3, 0.1]\n",
        "        class_id = int(parts[0])\n",
        "        # discard the first element (class) --> [x_center, y_center, width, height]\n",
        "        yolo_bbox = torch.tensor(parts[1:], dtype=torch.float32)\n",
        "        # convert the list of floats into a PyTorch tensor\n",
        "\n",
        "        # Read PDLPR label (license plate string)\n",
        "        with open(pdlpr_label_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            pdlpr_plate_str = f.readline().strip()\n",
        "\n",
        "        #Extracting the pdlpr index label that is going to be used by\n",
        "        #the CNNCTC model\n",
        "        fields = img_path.name.split(\"-\")\n",
        "        plate_number = fields[4]\n",
        "        character_id_list = plate_number.split(\"_\")\n",
        "        plate_id = []\n",
        "        for c in character_id_list:\n",
        "            plate_id.append(int(c))\n",
        "\n",
        "        #converting the index from the name to the index from the\n",
        "        #unified vocabulary\n",
        "        plate_id= target_to_index(plate_id)\n",
        "\n",
        "        pdlpr_label_idx = torch.tensor(plate_id, dtype=torch.long)\n",
        "\n",
        "        # apply the transformations\n",
        "        if self.transform:\n",
        "            full_image_original = full_image\n",
        "            full_image = self.transform(full_image)\n",
        "            cropped_image = self.transform(cropped_image)\n",
        "\n",
        "        return {\n",
        "            'full_image_original': full_image_original,\n",
        "            'full_image': full_image,\n",
        "            'cropped_image': cropped_image,\n",
        "            'yolo_bbox_label': yolo_bbox,\n",
        "            'pdlpr_plate_string': pdlpr_plate_str,\n",
        "            'pdlpr_plate_idx': pdlpr_label_idx,\n",
        "            'image_name': img_name\n",
        "        }\n",
        "\n",
        "\n",
        "    def get_dataloaders(base_dir, batch_size = 8, transform = None, collate_fn=None):\n",
        "        #this functions initializes the different dataloaders and returns them\n",
        "        ds = CCPDDataset(base_dir=base_dir, transform=transform)\n",
        "\n",
        "        train_loader = DataLoader(ds.get_dataset(\"train\"), batch_size=batch_size, shuffle=True, collate_fn = collate_fn)\n",
        "        val_loader = DataLoader(ds.get_dataset(\"val\"), batch_size=batch_size, shuffle=False, collate_fn = collate_fn)\n",
        "        test_loader = DataLoader(ds.get_dataset(\"test\"), batch_size=batch_size, shuffle=False, collate_fn = collate_fn)\n",
        "\n",
        "        return train_loader, val_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1KTU3FkMhmW"
      },
      "source": [
        "## Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gHQ8bVQxCkG"
      },
      "source": [
        "### Network for PDLPR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA_c11Er4uEj"
      },
      "source": [
        "#### IGFE Feature extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VLvPcJPxGxw"
      },
      "outputs": [],
      "source": [
        "class FocusStructure(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=64):\n",
        "        super(FocusStructure, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels * 4, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        patch_tl = x[..., ::2, ::2]\n",
        "        patch_tr = x[..., ::2, 1::2]\n",
        "        patch_bl = x[..., 1::2, ::2]\n",
        "        patch_br = x[..., 1::2, 1::2]\n",
        "        x = torch.cat([patch_tl, patch_tr, patch_bl, patch_br], dim=1)  # [B, 4C, H/2, W/2]\n",
        "        return self.conv(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXcmvg-Y5E6c"
      },
      "outputs": [],
      "source": [
        "class ConvDownSampling(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ConvDownSampling, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.1) #here we use as activation function LeakyReLU, which is more used in car plate detection\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXq3ji7B5MZl"
      },
      "outputs": [],
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(channels),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(channels),\n",
        "        )\n",
        "        self.relu = nn.LeakyReLU(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.relu(x + self.block(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-asTp22z5PYR"
      },
      "outputs": [],
      "source": [
        "class IGFE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(IGFE, self).__init__()\n",
        "        self.focus = FocusStructure(3, 64)         # From [3,48,144] to [64,24,72]\n",
        "        self.down1 = ConvDownSampling(64, 128)     # [128,12,36]\n",
        "        self.res1 = ResBlock(128)\n",
        "        self.res2 = ResBlock(128)\n",
        "        self.down2 = ConvDownSampling(128, 256)    # [256,6,18]\n",
        "        self.res3 = ResBlock(256)\n",
        "        self.res4 = ResBlock(256)\n",
        "        self.final_conv = nn.Conv2d(256, 512, kernel_size=1)  # [512,6,18]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.focus(x)\n",
        "        x = self.down1(x)\n",
        "        x = self.res1(x)\n",
        "        x = self.res2(x)\n",
        "        x = self.down2(x)\n",
        "        x = self.res3(x)\n",
        "        x = self.res4(x)\n",
        "        x = self.final_conv(x)\n",
        "        return x  # [B, 512, 6, 18]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnbuZU1I5ZGa"
      },
      "source": [
        "#### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEMr0_S25b7Z"
      },
      "outputs": [],
      "source": [
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, dim, max_len=108):\n",
        "        super().__init__()\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, max_len, dim))  # [1, 108, 512]\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pos_embed  # broadcasting over batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7StAVo9j5gTi"
      },
      "outputs": [],
      "source": [
        "# Encoder block\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, dim=512, inner_dim=1024, n_heads=8):\n",
        "        super().__init__()\n",
        "        self.expand = nn.Conv1d(dim, inner_dim, kernel_size=1)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=inner_dim, num_heads=n_heads, batch_first=True)\n",
        "        self.reduce = nn.Conv1d(inner_dim, dim, kernel_size=1)\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, 108, 512]\n",
        "        x_in = x\n",
        "        x = x.transpose(1, 2)  # [B, 512, 108]\n",
        "        x = self.expand(x)     # [B, 1024, 108]\n",
        "        x = x.transpose(1, 2)  # [B, 108, 1024]\n",
        "\n",
        "        attn_out, _ = self.attn(x, x, x)  # self-attention\n",
        "        x = self.reduce(attn_out.transpose(1, 2)).transpose(1, 2)  # back to [B, 108, 512]\n",
        "        x = self.norm(x + x_in)  # residual + norm\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBuGaLHG5jOj"
      },
      "outputs": [],
      "source": [
        "# Encoder (3 blocchi)\n",
        "class PDLPR_Encoder(nn.Module):\n",
        "    def __init__(self, dim=512, n_heads=8, depth=3):\n",
        "        super().__init__()\n",
        "        self.pos_enc = PositionalEncoding(dim)\n",
        "        self.blocks = nn.Sequential(*[EncoderBlock(dim, 1024, n_heads) for _ in range(depth)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, 512, 6, 18]\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.view(B, C, H * W)              # [B, 512, 108] -> shape [B, C, N] (Convolutional layers expects this)\n",
        "        # reorder dimentions for Transformer\n",
        "        x = x.permute(0, 2, 1)               # [B, 108, 512] -> changes to shape [B, N, C] (Transformer expects this)\n",
        "        x = self.pos_enc(x)                  # Add positional encoding\n",
        "        x = self.blocks(x)                   # Encoder blocks\n",
        "        return x  # [B, 108, 512]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hnmq-soL5osZ"
      },
      "source": [
        "#### Parallel decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4YqALNu5qtO"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aujTQj35u8-"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, dim, n_heads=8):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(dim, n_heads, batch_first=True)\n",
        "        self.cross_attn = nn.MultiheadAttention(dim, n_heads, batch_first=True)\n",
        "        self.ff = FeedForward(dim)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.norm3 = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None):\n",
        "        # tgt: [B, T, dim], memory: [B, S, dim]\n",
        "        x = tgt\n",
        "\n",
        "        # masked self attention\n",
        "        attn_out, _ = self.self_attn(x, x, x, attn_mask=tgt_mask)\n",
        "        x = self.norm1(x + attn_out)\n",
        "\n",
        "        # cross attention\n",
        "        attn_out, _ = self.cross_attn(x, memory, memory)\n",
        "        x = self.norm2(x + attn_out)\n",
        "\n",
        "        # feedforward neural network\n",
        "        ff_out = self.ff(x)\n",
        "        x = self.norm3(x + ff_out)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rr-WDgHV5yP5"
      },
      "outputs": [],
      "source": [
        "class ParallelDecoder(nn.Module):\n",
        "    def __init__(self, dim=512, vocab_size=70, num_heads=8, num_blocks=3, seq_len=18):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.char_embed = nn.Parameter(torch.randn(1, seq_len, dim))\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dim = dim\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            DecoderBlock(dim, num_heads) for _ in range(num_blocks)\n",
        "        ])\n",
        "        self.classifier = nn.Linear(dim, vocab_size)\n",
        "\n",
        "    def update_vocab_size(self, new_vocab_size):\n",
        "\n",
        "         if new_vocab_size != self.vocab_size:\n",
        "            print(f\"Updating vocab size from {self.vocab_size} to {new_vocab_size}\")\n",
        "            # Save old weights\n",
        "            old_classifier = self.classifier\n",
        "            old_out_features = old_classifier.out_features\n",
        "\n",
        "            # Create new classifier\n",
        "            new_classifier = nn.Linear(self.dim, new_vocab_size)\n",
        "            new_classifier = new_classifier.to(old_classifier.weight.device)\n",
        "\n",
        "            # Copy overlapping weights\n",
        "            num_to_copy = min(old_out_features, new_vocab_size)\n",
        "            with torch.no_grad():\n",
        "                new_classifier.weight[:num_to_copy] = old_classifier.weight[:num_to_copy]\n",
        "                new_classifier.bias[:num_to_copy] = old_classifier.bias[:num_to_copy]\n",
        "\n",
        "            self.classifier = new_classifier\n",
        "            self.vocab_size = new_vocab_size\n",
        "\n",
        "    def generate_mask(self, size):\n",
        "        # mask future tokens\n",
        "        mask = torch.triu(torch.ones(size, size), diagonal=1).bool()\n",
        "        return mask\n",
        "\n",
        "    def forward(self, memory):\n",
        "        # memory: [B, S, dim] → encoder output (B, 108, 512])\n",
        "        B = memory.size(0)\n",
        "        x = self.char_embed.expand(B, -1, -1)  # [B, T, dim]\n",
        "        tgt_mask = self.generate_mask(self.seq_len).to(memory.device)  # [T, T]\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x, memory, tgt_mask)\n",
        "\n",
        "        logits = self.classifier(x)  # [B, T, vocab_size]\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUYEDzyHxIDn"
      },
      "source": [
        "### Network for character recognition of baseline method CNN CTC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdPvkbRgTM6L"
      },
      "source": [
        "This is the model class for the CNN ctc network used for the second part of the baseline method, there are two dropout layers to decrease the overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0Vq_hemxBPR"
      },
      "outputs": [],
      "source": [
        "class CNN_CTC_model(nn.Module):\n",
        "    def __init__(self, num_char, hidden_size):\n",
        "        super(CNN_CTC_model, self).__init__()\n",
        "        self.num_char = num_char\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            # 1 Because we use grayscale images\n",
        "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(128, self.hidden_size, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        #two linear layers to do the final classification\n",
        "        self.linear = nn.Linear(self.hidden_size * 12, 256)  # 256×12 = concatenazione H dim\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.classifier = nn.Linear(self.hidden_size, self.num_char)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #the input size is:  Batch, 1, 48 x 144\n",
        "        x = self.features(x)   #output size: Batch, 256, 12x36\n",
        "\n",
        "        #since we have 4 elements, the CTC wants the width first so we have to\n",
        "        #put it into the first position\n",
        "        x = x.permute(3, 0, 1, 2)  # 36, batch, 256, 12\n",
        "        #the width so the frames must be more than the number of total characters that\n",
        "        #we want to encode, so T = width = 36\n",
        "        x = x.flatten(2)          # 36, batch , 256×12\n",
        "        x = self.linear(x)\n",
        "        x = self.dropout(x)           # 36, batch, 256\n",
        "        x = self.classifier(x)    # 36, batch, num_char\n",
        "\n",
        "        return x  #returns a tensor of size [numchar] for each one of the 36 positions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hf_xoanzUlgj"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMrIfoyDbvDt"
      },
      "source": [
        "### Train YOLO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "w4CMqjffb3-0",
        "outputId": "e6a438d4-8378-44ba-8bba-7e412aff5b98"
      },
      "outputs": [],
      "source": [
        "def get_model_name():\n",
        "    return f\"yolov5_epochs{EPOCHS_TRAIN_Y}_bs{BATCH_SIZE_TRAIN_Y}_lr{LR_INIT_Y}_imgs{IMAGE_SIZE_Y}.pt\"\n",
        "\n",
        "def get_run_name():\n",
        "    # Create a unique name for the current training run, using the hyperparams used in the model\n",
        "    return get_model_name().replace(\".pt\", \"\")\n",
        "\n",
        "\n",
        "def train_yolo():\n",
        "    model_name = get_model_name()\n",
        "    run_name = get_run_name()\n",
        "\n",
        "    if os.path.exists(model_name):\n",
        "        print(f\"[INFO] Model {model_name} already exists ---> SKIP training!!\")\n",
        "        return YOLO(model_name)\n",
        "\n",
        "    # Create an untrained model based on the configuration params\n",
        "    model = YOLO(\"yolov5s.yaml\")\n",
        "\n",
        "    model.train(\n",
        "        data    = f\"ccpd.yaml\",         # path to .yaml file for the configuration\n",
        "        epochs  = EPOCHS_TRAIN_Y,\n",
        "        batch   = BATCH_SIZE_TRAIN_Y,\n",
        "        lr0     = LR_INIT_Y,\n",
        "        imgsz   = IMAGE_SIZE_Y,\n",
        "        save    = True,                                        # save the training checkpoints and weigths of the final model\n",
        "        device  = \"mps\",                                       # TO BE CHANGED ACCORDING TO PC --> \"cpu\"\n",
        "        project = f\"runs/train\",        # directory where to save the outputs of training\n",
        "        name    = model_name.replace(\".pt\", \"\"),               # create a subdir in the project folder, where to save training logs and outputs\n",
        "        val     = True,                                        # run validation here to create results.csv and .png\n",
        "        plots   = True\n",
        "    )\n",
        "\n",
        "    model.save(model_name)\n",
        "\n",
        "    best_model_path = f\"runs/train/{run_name}/weights/best.pt\"\n",
        "\n",
        "    return best_model_path\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TRAIN\n",
        "train_model_path = train_yolo()\n",
        "\n",
        "run_name = get_run_name()\n",
        "\n",
        "# VALIDATION after training\n",
        "# Load and use the best model best.pt --> create a model instance initializzed with the trained weights\n",
        "best_model = YOLO(train_model_path, verbose = False)\n",
        "# best_model = YOLO(\"/Users/michelafuselli/Desktop/Michi/Università/Magistrale/Computer Vision/Project/CV_project/runs/train/yolov5_epochs20_bs8_lr0.001_imgs6402/weights/best.pt\", verbose = False)\n",
        "\n",
        "# Inside results: mAP@0.5, mAP@0.5:0.95. precision, recall, confusion matrix, curva PR, curva f1, ... --> are saved in runs/detect\n",
        "results = best_model.val(\n",
        "    data    = f\"ccpd.yaml\",\n",
        "    split   = 'val',\n",
        "    iou     = IOU_THRESHOLD,\n",
        "    device  = \"cpu\",\n",
        "    name    = f\"{run_name}_VAL_iou{int(IOU_THRESHOLD*100)}\",\n",
        ")\n",
        "\n",
        "image_dir = Path(f\"dataset/images/val\")\n",
        "output_dir = Path(f\"runs/val\") / f\"{get_run_name()}_VAL_iou{int(IOU_THRESHOLD * 100)}\"\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "iou_list = []\n",
        "\n",
        "# Loop over images\n",
        "for image_path in sorted(image_dir.glob(\"*.jpg\")):\n",
        "    # Predict\n",
        "    result = best_model(image_path, max_det=5, verbose = False)[0]\n",
        "    predictions = result.boxes.xyxy.cpu().numpy()  # shape: (N, 4)\n",
        "\n",
        "    real_box = load_gt_box_from_label_validation(image_path)\n",
        "    if real_box is None:\n",
        "        # skip image if no GT or invalid\n",
        "        continue\n",
        "\n",
        "    # Compute IoU between every predicted box and the true one\n",
        "    for predicted_box in predictions:\n",
        "        iou = compute_iou(predicted_box, real_box)\n",
        "        iou_list.append(iou)\n",
        "\n",
        "# Compute average among all iou values\n",
        "if iou_list:\n",
        "    mean_iou = sum(iou_list) / len(iou_list)\n",
        "else:\n",
        "    mean_iou = 0.0\n",
        "\n",
        "# Save in .txt\n",
        "txt_path = output_dir / \"mean_iou.txt\"\n",
        "with open(txt_path, \"w\") as f:\n",
        "    f.write(f\"Mean IoU over validation set: {mean_iou:.4f}\\n\")\n",
        "\n",
        "print(f\"[INFO] Mean IoU saved to {txt_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8HBAOmm6Otb"
      },
      "source": [
        "### PDLPR training function - IGFE, Encoder, Parallel decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tpAVEZy99zv"
      },
      "source": [
        "Validation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfcbi0KMKTuY"
      },
      "outputs": [],
      "source": [
        "def validate(model_parts, evaluator, val_loader, char_idx, idx_char, device):\n",
        "    igfe, encoder, decoder = model_parts\n",
        "\n",
        "    igfe.eval()\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    evaluator = Evaluator(idx_char)\n",
        "    pbar = tqdm(val_loader, desc=f\"Validating\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in pbar:\n",
        "            images = batch[\"cropped_image\"].to(device)\n",
        "            label_strs = batch[\"pdlpr_plate_string\"]\n",
        "\n",
        "            unknown = set(c for s in label_strs for c in s if c not in char_idx)\n",
        "            if unknown:\n",
        "                # updating vocabulary\n",
        "                for c in ''.join(label_strs):\n",
        "                    if c not in char_idx:\n",
        "                        idx = len(char_idx)\n",
        "                        char_idx[c] = idx\n",
        "                        idx_char[idx] = c\n",
        "                print(f\"unknown character {c}. Vocabulary updated\")\n",
        "                decoder.update_vocab_size(len(char_idx))\n",
        "\n",
        "            # Forward\n",
        "            features = igfe(images)\n",
        "            encoded = encoder(features)\n",
        "            logits = decoder(encoded)\n",
        "\n",
        "            evaluator.update(logits, label_strs)\n",
        "\n",
        "    metrics = evaluator.compute()\n",
        "    evaluator.print()\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9A4ZOTHKW_D"
      },
      "source": [
        "Training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2xL6_eu68TY"
      },
      "outputs": [],
      "source": [
        "def train(model_parts, evaluator, train_loader, val_loader, char_idx, idx_char, num_epochs, optimizer ,device):\n",
        "\n",
        "    igfe, encoder, decoder = model_parts\n",
        "    total_loss = 0\n",
        "\n",
        "    train_losses = []\n",
        "    train_seq_accs = []\n",
        "    train_char_accs = []\n",
        "\n",
        "    val_char_accs = []\n",
        "    val_seq_accs = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        evaluator = Evaluator(idx2char=idx_char)\n",
        "\n",
        "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        for batch in loop:\n",
        "            images = batch[\"cropped_image\"].to(device)\n",
        "            label_strs = batch[\"pdlpr_plate_string\"]\n",
        "\n",
        "            # update the vocabulary if unkwon character found\n",
        "            unknown = set(c for s in label_strs for c in s if c not in char_idx)\n",
        "            if unknown:\n",
        "                # updating vocabulary\n",
        "                for c in ''.join(label_strs):\n",
        "                    if c not in char_idx:\n",
        "                        idx = len(char_idx)\n",
        "                        char_idx[c] = idx\n",
        "                        idx_char[idx] = c\n",
        "                print(f\"unknown character {c}. Vocabulary updated\")\n",
        "                # update the decoder with the new vocabulary size but keeping the old weights\n",
        "                decoder.update_vocab_size(len(char_idx))\n",
        "\n",
        "            # lable encoding in ordr to compute loss\n",
        "            targets = torch.tensor([char_idx[c] for s in label_strs for c in s], dtype=torch.long).to(device)\n",
        "            target_lengths = torch.tensor([len(s) for s in label_strs], dtype=torch.long).to(device)\n",
        "            input_lengths = torch.full((images.size(0),), 18, dtype=torch.long).to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            features = igfe(images)\n",
        "            encoded = encoder(features)\n",
        "            logits = decoder(encoded)\n",
        "            log_probs = logits.log_softmax(2).permute(1, 0, 2)  # [T, B, C]\n",
        "\n",
        "            loss = ctc_loss(log_probs, targets, input_lengths, target_lengths)\n",
        "            total_loss += loss.item()\n",
        "            train_losses.append(loss)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # metrics updating using the evaluator\n",
        "            evaluator.update(logits, label_strs)\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        print(f\"Metrics at epoch {epoch+1}\")\n",
        "        evaluator.print()\n",
        "        metrics = evaluator.compute()\n",
        "        train_seq_accs.append(metrics[\"seq_accuracy\"])\n",
        "        train_char_accs.append(metrics[\"char_accuracy\"])\n",
        "\n",
        "\n",
        "        #saving the new vocabulary\n",
        "        with open(f\"vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(char_idx, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        # VALIDATION\n",
        "        val_evaluator = Evaluator(idx_char)\n",
        "        val_metrics = validate(\n",
        "        model_parts=(igfe, encoder, decoder),\n",
        "        evaluator=val_evaluator,\n",
        "        val_loader=val_loader,\n",
        "        char_idx=char_idx,\n",
        "        idx_char=idx_char,\n",
        "        device=device\n",
        "    )\n",
        "        val_seq_accs.append(val_metrics[\"seq_accuracy\"])\n",
        "        val_char_accs.append(val_metrics[\"char_accuracy\"])\n",
        "\n",
        "    #Saving the model for testing, the models will have as input the images cropped by YOLO\n",
        "    torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'igfe_state_dict': igfe.state_dict(),\n",
        "            'encoder_state_dict': encoder.state_dict(),\n",
        "            'decoder_state_dict': decoder.state_dict(),\n",
        "            'loss': total_loss,\n",
        "            'train_losses': train_losses,\n",
        "            'train_seq_accs': train_seq_accs,\n",
        "            'train_char_accs': train_char_accs\n",
        "        }, f'models/pdlpr_{NUM_EPOCHS_PDLPR}_{LR_PDLPR}_{BATCH_SIZE_PDLPR}.pt')\n",
        "    print(f\"Model saved in models/pdlpr_final.pt\")\n",
        "\n",
        "    print(\"END OF TRAINING, results:\\n\")\n",
        "\n",
        "    print(f\"number of epochs: {num_epochs}\")\n",
        "    print(f\"learning rate: {LR_PDLPR}\")\n",
        "    print(f\"batch size: {BATCH_SIZE_PDLPR}\")\n",
        "    print(f\"Loss: {total_loss / len(train_loader):.4f}\")\n",
        "    evaluator.print()\n",
        "\n",
        "\n",
        "    train_seq_accuracy = metrics['seq_accuracy']\n",
        "    val_seq_accuracy = val_metrics[\"seq_accuracy\"]\n",
        "    train_char_accuracy = metrics['char_accuracy']\n",
        "    val_char_accuracy = metrics['char_accuracy']\n",
        "\n",
        "    with open(f\"results/PDLPR-{NUM_EPOCHS_PDLPR}_{LR_PDLPR}_{BATCH_SIZE_PDLPR}.txt\", \"w\") as f:\n",
        "        f.write(f\"Final train accuracy: {train_seq_accuracy:.4f}\\n\")\n",
        "        f.write(f\"Final validation accuracy: {val_seq_accuracy:.4f}\\n\")\n",
        "        f.write(f\"Final character train accuracy: {train_char_accuracy:.4f}\\n\")\n",
        "        f.write(f\"Final character validation accuracy: {val_char_accuracy:.4f}\\n\")\n",
        "\n",
        "    print(f\"results saved in results/PDLPR-{NUM_EPOCHS_PDLPR}_{LR_PDLPR}_{BATCH_SIZE_PDLPR}.txt\")\n",
        "\n",
        "    # plot loss over epochs\n",
        "    epochs = range(1, len(train_losses)+1)\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, [l.detach().cpu().item() if torch.is_tensor(l) else l for l in train_losses], label=\"Train Loss\")\n",
        "    plt.title(\"Loss over Epochs\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"CTC Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(f\"metrics_images/loss_plot_{num_epochs}_{LR_PDLPR}_{BATCH_SIZE_PDLPR}.png\", dpi=300)\n",
        "\n",
        "    # plot train and validation metrics\n",
        "    print(\"Plotting metrics.........\")\n",
        "    plot_metrics(train_seq_accs, val_seq_accs, train_char_accs, val_char_accs)\n",
        "\n",
        "    return train_losses, train_seq_accs, train_char_accs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co8kt5kR-Fu-"
      },
      "source": [
        "Defining transformations to apply on the images and load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BREUnDd-E-c"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((48, 144)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
        "    ])\n",
        "\n",
        "    # loading data\n",
        "\n",
        "    dataset = CCPDDataset(base_dir=f\"dataset\", transform=transform)\n",
        "    train_loader, val_loader, test_loader = CCPDDataset.get_dataloaders(\n",
        "        base_dir=f\"dataset\",\n",
        "        batch_size=BATCH_SIZE_PDLPR,\n",
        "        transform=transform,\n",
        "        collate_fn= custom_collate\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXB6dwfp-ezK"
      },
      "source": [
        "Create the vocabulary if it does not already exist and defining mapping char to index and index to char"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loU_5ZrT-O7L",
        "outputId": "f0a83c41-2646-4726-86b4-0edb5ab57fcf"
      },
      "outputs": [],
      "source": [
        "# loading vocabulary\n",
        "if os.path.exists(f'vocab.json'):\n",
        "    print(f\"vocab.json found — loading...\")\n",
        "    char_idx, idx_char = load_vocab(f'vocab.json')\n",
        "else:\n",
        "    print(\"vocab.json not found — building it from labels...\")\n",
        "    char_idx, idx_char = build_vocab(f\"dataset/labels_pdlpr/train\", \"vocab.json\")\n",
        "\n",
        "vocab_size = len(char_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vfur3BaB-sO8"
      },
      "source": [
        "Defining training arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4BHpAS3-rTy"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "evaluator = Evaluator(idx_char)\n",
        "ctc_loss = nn.CTCLoss(blank=0)\n",
        "decoder_seq_len = 18  # From ParallelDecoder\n",
        "decoder = ParallelDecoder(dim=512, vocab_size=vocab_size, seq_len=decoder_seq_len).to(device).train()\n",
        "encoder = PDLPR_Encoder().to(device).train()\n",
        "igfe = IGFE().to(device).train()\n",
        "params = list(igfe.parameters()) + list(encoder.parameters()) + list(decoder.parameters())\n",
        "optimizer = optim.Adam(params, lr=LR_PDLPR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1jEU-ER_B6m"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "cnxUdYfW_Dym",
        "outputId": "fe1888a7-1a59-4b11-e01e-1d4a612a9a25"
      },
      "outputs": [],
      "source": [
        "print(\"Starting training..........\")\n",
        "train_char_accs, train_seq_accs, train_losses = train(\n",
        "    model_parts=(igfe, encoder, decoder),\n",
        "    evaluator=evaluator,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    char_idx=char_idx,\n",
        "    idx_char=idx_char,\n",
        "    num_epochs=NUM_EPOCHS_PDLPR,\n",
        "    optimizer=optimizer,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOa4Xuipb0wd"
      },
      "source": [
        "### train Baseline method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MSm6h7qV867"
      },
      "source": [
        "Training of the second part of character recognition, since the first part of licence plate detection was implemented using traditional techniques it doesn't make sense to distinguish between train and testing.\n",
        "\n",
        "So this is the code for the first part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "collapsed": true,
        "id": "r_n1gOR9WiE8",
        "outputId": "6b368e7d-b037-4e8b-d348-f79ae7537803"
      },
      "outputs": [],
      "source": [
        "# Initialize the reader just once: simplified chinese and english\n",
        "reader = easyocr.Reader(['ch_sim', 'en'])\n",
        "\n",
        "images_dir = Path(f\"dataset/images/train/\")\n",
        "labels_dir = Path(f\"dataset/labels/train/\")\n",
        "\n",
        "diff_results_dir = Path(\"results\")\n",
        "diff_results_dir.mkdir(parents=True, exist_ok=True)\n",
        "diff_results_txt = diff_results_dir / f\"BL_iou_ocr6.txt\"\n",
        "open(diff_results_txt, \"w\").close()\n",
        "\n",
        "total_iou = 0.0\n",
        "num_iou = 0\n",
        "num_passed_iou = 0      # counts values >= 0.7\n",
        "\n",
        "\n",
        "for image_path in tqdm(images_dir.glob(\"*.jpg\"), desc=\"Processing images\", unit=\"img\"):\n",
        "    image_name = image_path.name\n",
        "    label_path = labels_dir / (image_path.stem + \".txt\")\n",
        "\n",
        "    if not label_path.exists():\n",
        "        print(f\"NO label for {image_name}\")\n",
        "        continue\n",
        "\n",
        "    pil_image = Image.open(image_path).convert(\"RGB\")\n",
        "    yolo_tensor = get_label_yolo(label_path)\n",
        "    width, height = pil_image.size\n",
        "\n",
        "    true_coordinates = get_ground_truth_coordinates(yolo_tensor, width, height)\n",
        "\n",
        "    candidate_bounding_box = plate_detector(image_path, true_coordinates)\n",
        "\n",
        "    if not candidate_bounding_box:\n",
        "        with open(diff_results_txt, \"a\") as f:\n",
        "            f.write(f\"{image_name}\\n\")\n",
        "            f.write(\"IoU: 0.000\\n\")\n",
        "            f.write(\"OCR: NONE\\n\")\n",
        "            f.write(f\"Box GT: {true_coordinates}\\n\")\n",
        "            f.write(\"Box Pred: NONE\\n\")\n",
        "            f.write(\"---\\n\")\n",
        "\n",
        "        # count it as missing\n",
        "        total_iou += 0.0\n",
        "        num_iou += 1\n",
        "        continue\n",
        "\n",
        "    predict_bbox, ocr_text  = candidate_bounding_box\n",
        "    iou_diff = compute_iou(predict_bbox, true_coordinates)\n",
        "\n",
        "    total_iou += iou_diff\n",
        "    num_iou += 1\n",
        "    if iou_diff >= IOU_THRESHOLD:\n",
        "        num_passed_iou += 1\n",
        "\n",
        "    with open(diff_results_txt,  \"a\", encoding = \"utf-8\") as f:\n",
        "        f.write(f\"{image_name}\\n\")\n",
        "        f.write(f\"IoU: {iou_diff:.3f}\\n\")\n",
        "        f.write(f\"OCR: {ocr_text}\\n\")\n",
        "        f.write(f\"Box GT: {true_coordinates}\\n\")\n",
        "        f.write(f\"Box Pred: {predict_bbox}\\n\")\n",
        "        f.write(\"---\\n\")\n",
        "\n",
        "if num_iou > 0:\n",
        "    avg_iou = total_iou / num_iou\n",
        "    pass_rate = (num_passed_iou / num_iou) * 100\n",
        "else:\n",
        "    avg_iou = 0.0\n",
        "    pass_rate = 0.0\n",
        "\n",
        "\n",
        "with open(diff_results_txt, \"a\") as f:\n",
        "    f.write(f\"\\n AVERAGE IoU over {num_iou} predictions: {avg_iou:0.4f}\")\n",
        "    f.write(f\"\\n IoU pass rate (>= 0.7) {pass_rate:0.2f}\")\n",
        "\n",
        "print(f\"\\n AVERAGE IoU over {num_iou} predictions: {avg_iou:0.4f}\")\n",
        "print(f\"\\n IoU pass rate (>= 0.7) {pass_rate:0.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80HVEI5TWiw0"
      },
      "source": [
        "In the training of CNN + CTC we try different hyperparameters combinations to select the best ones. The resulting images and txt files are saved in local folders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "collapsed": true,
        "id": "NRlmVpiWUrqC",
        "outputId": "7ee0389d-359a-407d-968b-aabc887165cf"
      },
      "outputs": [],
      "source": [
        "#Hyperparameters combination\n",
        "batch_sizes = [64, 32]  \n",
        "learning_rates = [0.001]\n",
        "weight_decays = [1e-4, 5e-4]\n",
        "epochs = [40, 60]\n",
        "\n",
        "CHAR_LIST = sorted(set(PROVINCES+ALPHABETS+ADS))\n",
        "PLATE_LENGTH = 8\n",
        "\n",
        "NUM_CHAR = len(CHAR_LIST) + 1 #since we include the blank character\n",
        "\n",
        "combinations = product(batch_sizes, learning_rates, weight_decays, epochs)\n",
        "\n",
        "#executing the training and testing for all the possible combinations to get the best one\n",
        "for bs, lr, wd, ne in combinations:\n",
        "\n",
        "    #Hyperparameters\n",
        "    BATCH_SIZE = bs\n",
        "    LR = lr\n",
        "    WEIGHT_DECAY = wd\n",
        "    NUM_EPOCHS = ne\n",
        "\n",
        "    SAVE_NAME = f\"n_epochs_{NUM_EPOCHS}_bs_{BATCH_SIZE}_LR_{LR}_wd_{WEIGHT_DECAY}\"\n",
        "\n",
        "    print(f\"training with {SAVE_NAME}\")\n",
        "\n",
        "    model = CNN_CTC_model(num_char=NUM_CHAR, hidden_size=256)\n",
        "    ctc_loss = nn.CTCLoss(blank=0)\n",
        "\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Grayscale(),              # converte in 1 canale\n",
        "        transforms.Resize((48, 144)),       # adatta a H=48, W=144\n",
        "        transforms.ToTensor(),              # [C, H, W]\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "\n",
        "    train_dataloader, val_dataloader, test_dataloader = CCPDDataset.get_dataloaders(base_dir=DATASET_PATH_Y, batch_size=BATCH_SIZE, transform=preprocess, collate_fn = custom_collate_2)\n",
        "     #this optimizer uses stochastic gradient descent and has in input the parameters (weights) from\n",
        "    #the pretrained model\n",
        "    optimizer = Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "    #optimizer = SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    #initialize the device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    accuracy_val=[]\n",
        "    accuracy_train=[]\n",
        "    total_train_loss=[]\n",
        "    char_accuracy_train =[]\n",
        "    char_accuracy_val =[]\n",
        "\n",
        "    epsilon = 1e-6\n",
        "    #TRAIN LOOP we are doing fine tuning on the task of recognizing plate\n",
        "    for e in range(NUM_EPOCHS):\n",
        "        evaluator = Evaluator()\n",
        "        model.train()\n",
        "        train_loss= 0.0\n",
        "        train_acc = []\n",
        "        val_acc = []\n",
        "        train_char_acc =[]\n",
        "        val_char_acc = []\n",
        "        B_size = 0\n",
        "        i=0\n",
        "        #does the for loop for all the items in the same batch\n",
        "        for batch in train_dataloader:\n",
        "            print(f\"Batch {i + 1}/{len(train_dataloader)}\")\n",
        "            images = batch[\"cropped_image\"]\n",
        "            labels = batch[\"pdlpr_plate_idx\"]\n",
        "\n",
        "            images = [img.to(device) for img in images]\n",
        "            labels = [lab.to(device) for lab in labels]\n",
        "\n",
        "            # Stack per batch processing\n",
        "            images = torch.stack(images)\n",
        "            labels = torch.stack(labels)\n",
        "\n",
        "            #Ctc loss expects a simple list not a 2 dimensional tensor, so all the batch\n",
        "            #index are flattened into one single list\n",
        "            flat_labels_list = labels.view(-1)\n",
        "            #we get the output of the models and apply softmax to turn it into probability\n",
        "            output_logits = model(images)\n",
        "            output_probabilities = F.log_softmax(output_logits, dim=2)\n",
        "            #the output of the model are T vectors for the batch size\n",
        "            T = output_logits.size(0)\n",
        "            #get the current batch size\n",
        "            B_size = images.size(0)\n",
        "            #creates a tensor the length of the batch size filled with the dimention of the input\n",
        "            #and the dimension of the output, since ctc requires the lengths because it uses one big\n",
        "            #vector\n",
        "            input_lengths = torch.full((B_size,), T, dtype=torch.long).to(device)\n",
        "            target_lengths = torch.full((B_size,), PLATE_LENGTH, dtype=torch.long).to(device)\n",
        "\n",
        "            #CTC loss\n",
        "            loss = ctc_loss(output_probabilities, flat_labels_list, input_lengths, target_lengths)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            evaluator.reset()\n",
        "            evaluator.update_baseline(output_logits, labels)\n",
        "\n",
        "            ##we take the index of words with the highest probabilities\n",
        "            metrics = evaluator.compute()\n",
        "            #metrics for the whole batch\n",
        "            mean_batch_train_char_acc = metrics[\"char_accuracy\"]\n",
        "            mean_batch_train_acc = metrics[\"seq_accuracy\"]\n",
        "            #print(mean_batch_train_char_acc)\n",
        "            #print(mean_batch_train_acc)\n",
        "            train_acc.append(mean_batch_train_acc)\n",
        "            train_char_acc.append(mean_batch_train_char_acc)\n",
        "\n",
        "            i+=1\n",
        "\n",
        "        #compute the mean of the full and character accuracy for training\n",
        "        #for the whole epoch\n",
        "        mean_train_acc = sum(train_acc)/len(train_acc)\n",
        "        mean_train_char_acc = sum(train_char_acc)/len(train_char_acc)\n",
        "        train_loss = train_loss/B_size\n",
        "\n",
        "        #append the result to lists in order to plot them\n",
        "        accuracy_train.append(mean_train_acc)\n",
        "        char_accuracy_train.append(mean_train_char_acc)\n",
        "        total_train_loss.append(train_loss)\n",
        "\n",
        "        j=0\n",
        "        #Validation phase\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in val_dataloader:\n",
        "                print(f\"Batch {j + 1}/{len(val_dataloader)}\")\n",
        "                images = batch[\"cropped_image\"]\n",
        "                labels = batch[\"pdlpr_plate_idx\"]\n",
        "\n",
        "                images = [img.to(device) for img in images]\n",
        "                labels = [lab.to(device) for lab in labels]\n",
        "\n",
        "                images = torch.stack(images)\n",
        "                labels = torch.stack(labels)\n",
        "\n",
        "                flat_labels_list = labels.view(-1)\n",
        "\n",
        "                output_logits = model(images)\n",
        "\n",
        "                evaluator.reset()\n",
        "                evaluator.update_baseline(output_logits, labels)\n",
        "                metrics = evaluator.compute()\n",
        "\n",
        "                #metrics for the whole batch\n",
        "                mean_batch_val_char_acc = metrics[\"char_accuracy\"]\n",
        "                mean_batch_val_acc = metrics[\"seq_accuracy\"]\n",
        "                #print(mean_batch_val_char_acc)\n",
        "                #print(mean_batch_val_acc)\n",
        "                val_acc.append(mean_batch_val_acc)\n",
        "                val_char_acc.append(mean_batch_val_char_acc)\n",
        "\n",
        "                j+=1\n",
        "\n",
        "        #compute the mean of the iou validation score\n",
        "        mean_val_acc = sum(val_acc)/len(val_acc)\n",
        "        mean_val_char_acc = sum(val_char_acc)/len(val_char_acc)\n",
        "\n",
        "        accuracy_val.append(mean_val_acc)\n",
        "        char_accuracy_val.append(mean_val_char_acc)\n",
        "\n",
        "        print(f\"Epoch {e +1}/{NUM_EPOCHS} - train loss: {train_loss} - train acc: {mean_train_acc} - train char acc: {mean_train_char_acc} - val acc: {mean_val_acc} --  val char acc: {mean_val_char_acc}\" )\n",
        "\n",
        "    #Saving the model\n",
        "    torch.save(model.state_dict(), f\"models/CNNCTC-{SAVE_NAME}.pth\")\n",
        "\n",
        "    #Plotting the figure for the train and validation\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(range(1, NUM_EPOCHS + 1), accuracy_train, label=\"train acc\", marker='o')\n",
        "    plt.plot(range(1, NUM_EPOCHS + 1), accuracy_val, label=\"validation acc\", marker='s')\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(\"accuracy\")\n",
        "    plt.title(\"Train and validation plate accuracy per epoch\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    #plt.show()\n",
        "    plt.savefig(f\"metrics_images/train_validation_CNNCTC-{SAVE_NAME}.png\")\n",
        "\n",
        "    #Plotting the figure for the train and validation\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(range(1, NUM_EPOCHS + 1), char_accuracy_train, label=\"char train acc\", marker='o')\n",
        "    plt.plot(range(1, NUM_EPOCHS + 1), char_accuracy_val, label=\"char validation acc\", marker='s')\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(\"accuracy\")\n",
        "    plt.title(\"Train and validation character accuracy per epoch\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    #plt.show()\n",
        "    plt.savefig(f\"metrics_images/char_train_validation_CNNCTC-{SAVE_NAME}.png\")\n",
        "\n",
        "    #getting the last iou value for train and validation\n",
        "    final_train_acc = accuracy_train[-1]\n",
        "    final_val_acc = accuracy_val[-1]\n",
        "\n",
        "    final_char_train_acc = char_accuracy_train[-1]\n",
        "    final_char_val_acc = char_accuracy_val[-1]\n",
        "\n",
        "    with open(f\"results/CNNCTC-{SAVE_NAME}.txt\", \"w\") as f:\n",
        "        f.write(f\"Final train accuracy: {final_train_acc:.4f}\\n\")\n",
        "        f.write(f\"Final validation accuracy: {final_val_acc:.4f}\\n\")\n",
        "        f.write(f\"Final character train accuracy: {final_char_train_acc:.4f}\\n\")\n",
        "        f.write(f\"Final character validation accuracy: {final_char_val_acc:.4f}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cncz11IbUnpg"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrQVCVnYb770"
      },
      "source": [
        "### test Yolo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "LR08RINVUuUa",
        "outputId": "07e05e6d-7360-4fa6-f868-37562d58c361"
      },
      "outputs": [],
      "source": [
        "def get_model_name():\n",
        "    return f\"yolov5_epochs{EPOCHS_TRAIN_Y}_bs{BATCH_SIZE_TRAIN_Y}_lr{LR_INIT_Y}_imgs{IMAGE_SIZE_Y}.pt\"\n",
        "\n",
        "\n",
        "def get_run_name():\n",
        "    return get_model_name().replace(\".pt\", \"\")\n",
        "\n",
        "\n",
        "def test_yolo():\n",
        "    model_name = get_model_name()\n",
        "    # Load the actual trained model weights --> this ensures that testing is run on the best version of the model\n",
        "    model_path = Path(f\"runs/train\") / model_name.replace(\".pt\", \"\") / \"weights\" / \"best.pt\"\n",
        "\n",
        "    if not model_path.exists():\n",
        "        raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
        "\n",
        "\n",
        "    # Retrive the name of the training run\n",
        "    run_name = get_run_name()\n",
        "\n",
        "    output_name = f\"{Path(model_path).stem}_TEST_iou{int(IOU_THRESHOLD * 100)}\"\n",
        "    output_dir = Path(f\"runs/test\") / output_name\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    model = YOLO(model_path)\n",
        "\n",
        "    results = model.val(\n",
        "        data    = f\"ccpd.yaml\",\n",
        "        split   = \"test\",\n",
        "        iou     = IOU_THRESHOLD,\n",
        "        device  = \"cpu\",\n",
        "        name    = f\"{run_name}_TEST_iou70\",\n",
        "        project = f\"runs/test\"\n",
        "    )\n",
        "\n",
        "    return results, model, output_dir\n",
        "\n",
        "\n",
        "print(f\"\")\n",
        "model_path = f\"runs/train/yolov5_epochs30_bs12_lr0.001_imgs640/weights\"\n",
        "\n",
        "run_name = Path(model_path).parent.parent.name\n",
        "\n",
        "output_name = f\"{Path(model_path).stem}_TEST_iou{int(IOU_THRESHOLD * 100)}\"\n",
        "\n",
        "# TESTING\n",
        "results, model_test, test_output_dir = test_yolo()\n",
        "\n",
        "# Save metrics in a file\n",
        "metrics_path = test_output_dir / \"test_metrics.txt\"\n",
        "test_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "with open(metrics_path, \"w\") as f:\n",
        "    f.write(f\"Model: {model_path}\\n\")\n",
        "    f.write(f\"IoU Threshold: {IOU_THRESHOLD}\\n\\n\")\n",
        "    f.write(f\"mAP@0.5:      {results.box.map50:.4f}\\n\")\n",
        "    f.write(f\"mAP@0.5:0.95: {results.box.map:.4f}\\n\")\n",
        "    f.write(f\"Precision:    {results.box.mp:.4f}\\n\")\n",
        "    f.write(f\"Recall:       {results.box.mr:.4f}\\n\")\n",
        "\n",
        "\n",
        "# Compute IoU\n",
        "image_dir = Path(f\"dataset/images/test\")\n",
        "\n",
        "iou_list = []\n",
        "\n",
        "# Loop over images\n",
        "for image_path in sorted(image_dir.glob(\"*.jpg\")):\n",
        "    # Predict\n",
        "    result = model_test(str(image_path), max_det=5, verbose = False)[0]\n",
        "    predictions = result.boxes.xyxy.cpu().numpy()  # shape: (N, 4)\n",
        "\n",
        "    real_box = load_gt_box_from_label_test(image_path)\n",
        "    if real_box is None:\n",
        "        # skip image if no GT or invalid\n",
        "        continue\n",
        "\n",
        "    # Compute IoU between every predicted box and the true one\n",
        "    for predicted_box in predictions:\n",
        "        iou = compute_iou(predicted_box, real_box)\n",
        "        iou_list.append(iou)\n",
        "\n",
        "# Compute average among all iou values\n",
        "if iou_list:\n",
        "    mean_iou = sum(iou_list) / len(iou_list)\n",
        "else:\n",
        "    mean_iou = 0.0\n",
        "\n",
        "# Save in test_metrics.txt\n",
        "with open(metrics_path, \"a\") as f:\n",
        "    f.write(f\"Mean IoU over test set: {mean_iou:.4f}\\n\")\n",
        "\n",
        "print(f\"[INFO] Mean IoU saved to {metrics_path}\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n TESTING complete!\")\n",
        "print(f\"Results saved to: runs/test/{test_output_dir}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W58PEoMpSRgV"
      },
      "source": [
        "### Test PDLPR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2sOXzwhWLdU"
      },
      "source": [
        "Initializing parameters and load trained pdlpr model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((48, 144)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
        "    ])\n",
        "\n",
        "    # loading data\n",
        "\n",
        "    dataset = CCPDDataset(base_dir=f\"dataset\", transform=transform)\n",
        "    train_loader, val_loader, test_loader = CCPDDataset.get_dataloaders(\n",
        "        base_dir=f\"dataset\",\n",
        "        batch_size=BATCH_SIZE_PDLPR,\n",
        "        transform=transform,\n",
        "        collate_fn= custom_collate\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AO0S5XkUV10v",
        "outputId": "06511b3a-7876-4a48-b872-f87dc9719de8"
      },
      "outputs": [],
      "source": [
        "# loading vocabulary\n",
        "if os.path.exists(f'vocab.json'):\n",
        "    print(f\"vocab.json found — loading...\")\n",
        "    char_idx, idx_char = load_vocab(f'vocab.json')\n",
        "else:\n",
        "    print(\"vocab.json not found — building it from labels...\")\n",
        "    char_idx, idx_char = build_vocab(f\"dataset/labels_pdlpr/train\", \"vocab.json\")\n",
        "\n",
        "vocab_size = len(char_idx)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "evaluator = Evaluator(idx_char)\n",
        "ctc_loss = nn.CTCLoss(blank=0)\n",
        "decoder_seq_len = 18  # From ParallelDecoder\n",
        "decoder = ParallelDecoder(dim=512, vocab_size=vocab_size, seq_len=decoder_seq_len).to(device).train()\n",
        "encoder = PDLPR_Encoder().to(device).train()\n",
        "igfe = IGFE().to(device).train()\n",
        "\n",
        "# load pre trained model if needed\n",
        "if os.path.exists(f'models/pdlpr_10_0.0001_16.pt'):\n",
        "    print(\"checkpoint found. Loading state dict......\")\n",
        "    checkpoint = torch.load( f'models/pdlpr_10_0.0001_16.pt', map_location=device)\n",
        "    igfe.load_state_dict(checkpoint[\"igfe_state_dict\"])\n",
        "    encoder.load_state_dict(checkpoint[\"encoder_state_dict\"])\n",
        "    decoder.load_state_dict(checkpoint[\"decoder_state_dict\"])\n",
        "    #optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "\n",
        "else:\n",
        "    print(\"checkpoint not found. Please train the model first\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmNnKtGJOCXa"
      },
      "source": [
        "Test function PDLPR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XTJIp2bOBxW"
      },
      "outputs": [],
      "source": [
        "def test(model_parts, evaluator, test_loader, char_idx, idx_char, device):\n",
        "    igfe, encoder, decoder = model_parts\n",
        "\n",
        "    # keep track of metrics for plot\n",
        "    test_seq_accs = []\n",
        "    test_char_accs = []\n",
        "\n",
        "    igfe.eval()\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    evaluator = Evaluator(idx_char)\n",
        "    pbar = tqdm(test_loader, desc=f\"Testing\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in pbar:\n",
        "            images = batch[\"cropped_image\"].to(device)\n",
        "            label_strs = batch[\"pdlpr_plate_string\"]\n",
        "\n",
        "            unknown = set(c for s in label_strs for c in s if c not in char_idx)\n",
        "            if unknown:\n",
        "                # updating vocabulary\n",
        "                for c in ''.join(label_strs):\n",
        "                    if c not in char_idx:\n",
        "                        idx = len(char_idx)\n",
        "                        char_idx[c] = idx\n",
        "                        idx_char[idx] = c\n",
        "                print(f\"unknown character {c}. Vocabulary updated\")\n",
        "                # update the decoder with the new vocabulary size but keeping the old weights\n",
        "                decoder.update_vocab_size(len(char_idx))\n",
        "\n",
        "\n",
        "            # Forward\n",
        "            features = igfe(images)\n",
        "            encoded = encoder(features)\n",
        "            logits = decoder(encoded)\n",
        "\n",
        "            evaluator.update(logits, label_strs)\n",
        "            metrics = evaluator.compute()\n",
        "            test_seq_accs.append(metrics[\"seq_accuracy\"])\n",
        "            test_char_accs.append(metrics[\"char_accuracy\"])\n",
        "\n",
        "\n",
        "    #saving the new vocabulary\n",
        "    with open(\"vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(char_idx, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    metrics = evaluator.compute()\n",
        "    evaluator.print()\n",
        "    return metrics, test_char_accs, test_seq_accs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_01CMcIO-q9"
      },
      "source": [
        "Start testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "V9n9ag1GPCqB",
        "outputId": "b27a93d2-f0de-48ce-dcd4-6bf2297d209f"
      },
      "outputs": [],
      "source": [
        "print(\"Starting testing............\")\n",
        "test_metrics, test_char_accs, test_seq_accs = test(\n",
        "    model_parts=(igfe, encoder, decoder),\n",
        "    evaluator=evaluator,\n",
        "    test_loader=test_loader,\n",
        "    char_idx=char_idx,\n",
        "    idx_char=idx_char,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7HqGhcYSZXr"
      },
      "source": [
        "### Test pipeline yolo + PDLPR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPqPyw86SsO2"
      },
      "source": [
        "Function that crops the images using trained yolo model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPYOZqqcSqFU"
      },
      "outputs": [],
      "source": [
        "def crop_image_yolo(yolo_model, image):\n",
        "    #image = Image.open(image_path).convert(\"RGB\")\n",
        "    results = yolo_model(image, verbose=False)[0]  # Results object\n",
        "    boxes = results.boxes\n",
        "\n",
        "    if boxes is None or len(boxes) == 0:\n",
        "        print(\"No car plate detected\")\n",
        "        return None\n",
        "\n",
        "    bbox = boxes.xyxy[0].cpu().numpy().astype(int)\n",
        "    x1, y1, x2, y2 = bbox\n",
        "\n",
        "    cropped_img = image.crop((x1, y1, x2, y2))\n",
        "\n",
        "    #plt.imshow(cropped_img)\n",
        "    #plt.title(\"Targa rilevata (crop YOLO)\")\n",
        "    #plt.axis(\"off\")\n",
        "    #plt.show()\n",
        "\n",
        "    return cropped_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DerfPu2TSwxT"
      },
      "source": [
        "Function that tests the total pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Sv5fj8zSeSS"
      },
      "outputs": [],
      "source": [
        "def test(model_parts, yolo_model, transform, evaluator, test_loader, char_idx, idx_char, device):\n",
        "    igfe, encoder, decoder = model_parts\n",
        "\n",
        "    igfe.eval()\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    predicted_strings = []\n",
        "\n",
        "    evaluator = Evaluator(idx_char)\n",
        "    pbar = tqdm(test_loader, desc=f\"Testing\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in pbar:\n",
        "            # cropping images with yolo\n",
        "            sample = batch[0]\n",
        "            img = sample[\"full_image_original\"]\n",
        "            label_strs = sample[\"pdlpr_plate_string\"]\n",
        "\n",
        "            images = []\n",
        "\n",
        "            # Crop using YOLO\n",
        "            cropped_img = crop_image_yolo(yolo_model, img)\n",
        "            if cropped_img is None:\n",
        "                print(\"No plate detected — skipping image\")\n",
        "                continue\n",
        "            # Transform cropped image\n",
        "            tensor = transform(cropped_img).unsqueeze(0).to(device)\n",
        "            images.append(tensor)\n",
        "\n",
        "            if len(images) == 0:\n",
        "                continue  # skip batch if all images failed\n",
        "\n",
        "            images = torch.cat(images, dim=0)\n",
        "\n",
        "            unknown = set(c for s in label_strs for c in s if c not in char_idx)\n",
        "            if unknown:\n",
        "                # updating vocabulary\n",
        "                for c in ''.join(label_strs):\n",
        "                    if c not in char_idx:\n",
        "                        idx = len(char_idx)\n",
        "                        char_idx[c] = idx\n",
        "                        idx_char[idx] = c\n",
        "                print(f\"unknown character {c}. Vocabulary updated\")\n",
        "                # update the decoder with the new vocabulary size but keeping the old weights\n",
        "                decoder.update_vocab_size(len(char_idx))\n",
        "\n",
        "\n",
        "            # Forward\n",
        "            features = igfe(images)\n",
        "            encoded = encoder(features)\n",
        "            logits = decoder(encoded)\n",
        "\n",
        "            evaluator.update(logits, [label_strs])\n",
        "            pred_str = evaluator.greedy_decode(logits)\n",
        "            predicted_strings.append(pred_str)\n",
        "            #print(f\"traget string: {label_strs},  Predicted: {pred_str}\")\n",
        "\n",
        "        metrics = evaluator.compute()\n",
        "        evaluator.print()\n",
        "\n",
        "    return metrics, predicted_strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_GXdpR1TMoB",
        "outputId": "00ee1124-1832-4762-a052-9bbbb62cdc6d"
      },
      "outputs": [],
      "source": [
        "# loading yolo model\n",
        "yolo_model = YOLO(f\"runs/train/yolov5_epochs20_bs8_lr0.001_imgs640/weights/best.pt\")\n",
        "\n",
        "# Load data\n",
        "dataset = CCPDDataset(base_dir=f\"dataset\", transform=transform)\n",
        "_, _, test_loader = CCPDDataset.get_dataloaders(\n",
        "    base_dir=f\"dataset\",\n",
        "    batch_size=1,\n",
        "    transform=transform,\n",
        "    collate_fn=custom_collate_simple\n",
        ")\n",
        "# loading vocabulary\n",
        "if os.path.exists(f'vocab.json'):\n",
        "    print(f\"vocab.json found — loading...\")\n",
        "    char2idx, idx2char = load_vocab(f'vocab.json')\n",
        "else:\n",
        "    print(\"vocab.json not found — building it from labels...\")\n",
        "    char2idx, idx2char = build_vocab(f\"dataset/labels_pdlpr/train\", \"vocab.json\")\n",
        "\n",
        "vocab_size = len(char2idx)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "evaluator = Evaluator(idx2char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "8xrNPWYxVXL6",
        "outputId": "0ec10c1b-0115-49b0-c21f-462e9384123f"
      },
      "outputs": [],
      "source": [
        "print(\"start testing.........\")\n",
        "metrics, predicted_strings = test(\n",
        "    model_parts=(igfe, encoder, decoder),\n",
        "    yolo_model = yolo_model,\n",
        "    transform=transform,\n",
        "    evaluator=evaluator,\n",
        "    test_loader=test_loader,\n",
        "    char_idx=char2idx,\n",
        "    idx_char=idx2char,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(\"First 5 predicted strings:\", predicted_strings[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGug41Nsb-8M"
      },
      "source": [
        "### test Baseline method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbJpxW8yW50E"
      },
      "source": [
        "testing for the second part of the baseline method that uses CNN CTC, in order to execute this is important to initialize the Evaluator class which is found in Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "collapsed": true,
        "id": "2QqysWZSgry-",
        "outputId": "4e5951e6-dab9-4105-f1f2-a99cf59cf0cc"
      },
      "outputs": [],
      "source": [
        "CHAR_LIST = sorted(set(PROVINCES+ALPHABETS+ADS))\n",
        "PLATE_LENGTH = 8\n",
        "\n",
        "NUM_CHAR = len(CHAR_LIST) + 1 #since we include the blank character\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "LR = 0.001\n",
        "WEIGHT_DECAY = 0.0001\n",
        "NUM_EPOCHS = 60\n",
        "\n",
        "SAVE_NAME = f\"n_epochs_{NUM_EPOCHS}_bs_{BATCH_SIZE}_LR_{LR}_wd_{WEIGHT_DECAY}\"\n",
        "model = CNN_CTC_model(num_char=NUM_CHAR, hidden_size=256)\n",
        "ctc_loss = nn.CTCLoss(blank=0)\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Grayscale(),\n",
        "    transforms.Resize((48, 144)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "_, _, test_dataloader = CCPDDataset.get_dataloaders(base_dir=DATASET_PATH_Y, batch_size=BATCH_SIZE, transform=preprocess, collate_fn= custom_collate_2)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#TESTING PHASE\n",
        "if os.path.exists(f\"models/CNNCTC-{SAVE_NAME}.pth\"):\n",
        "    model.load_state_dict(torch.load(f\"models/CNNCTC-{SAVE_NAME}.pth\"))\n",
        "    model.to(device)\n",
        "else:\n",
        "    print(\"model not found. Please train the model first\")\n",
        "model.eval()\n",
        "test_acc = []\n",
        "char_test_acc = []\n",
        "\n",
        "evaluator = Evaluator()\n",
        "#here we just loop throught the test data and compute the accuracy scores\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        images = batch[\"cropped_image\"]\n",
        "        labels = batch[\"pdlpr_plate_idx\"]\n",
        "        images = [img.to(device) for img in images]\n",
        "        labels = [lab.to(device) for lab in labels]\n",
        "        images = torch.stack(images)\n",
        "        labels = torch.stack(labels)\n",
        "        flat_labels_list = labels.view(-1)\n",
        "\n",
        "        output_logits = model(images)\n",
        "        output_probabilities = F.log_softmax(output_logits, dim=2)\n",
        "        evaluator.reset()\n",
        "        evaluator.update_baseline(output_logits, labels)\n",
        "        metrics = evaluator.compute()\n",
        "        #metrics for the whole batch\n",
        "        mean_batch_test_char_acc = metrics[\"char_accuracy\"]\n",
        "        mean_batch_test_acc = metrics[\"seq_accuracy\"]\n",
        "        test_acc.append(mean_batch_test_acc)\n",
        "        char_test_acc.append(mean_batch_test_char_acc)\n",
        "\n",
        "mean_acc = sum(test_acc) / len(test_acc)\n",
        "mean_char_acc = sum(char_test_acc)/len(char_test_acc)\n",
        "print(f\"Test result accuracy: {mean_acc:.4f}\")\n",
        "print(f\"Test result char accuracy: {mean_char_acc:.4f}\")\n",
        "#saving the iou result of the training, validation (last step) and testing\n",
        "with open(f\"results/CNNCTC-test-{SAVE_NAME}.txt\", \"w\") as f:\n",
        "    f.write(f\"Final testing accuracy: {mean_acc:.4f}\\n\")\n",
        "    f.write(f\"Final testing character accuracy: {mean_char_acc:.4f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhGg5Gg6xSG4"
      },
      "source": [
        "### Test pipeline baseline method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxGPfANZc49u"
      },
      "source": [
        "Here we implement the pipeline testing, that tests the combination between the two parts for each method, in this code, the baseline so the function that computes the plate box using traditional methods then crops the images and does character recognition using cnn ctc model.\n",
        "\n",
        "The function plate_dectector is defined in utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cT9BfFzcxX3o",
        "outputId": "9e864cba-9524-4948-a3c2-3162ab673f10"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "reader = easyocr.Reader(['ch_sim', 'en'])\n",
        "\n",
        "CHAR_LIST = sorted(set(PROVINCES+ALPHABETS+ADS))\n",
        "PLATE_LENGTH = 8\n",
        "\n",
        "for idx, char in enumerate(CHAR_LIST):\n",
        "    CHAR_IDX[char] = idx + 1  # start from 1\n",
        "    IDX_CHAR[idx + 1] = char\n",
        "IDX_CHAR[0] = '_'  # blank character for CTC\n",
        "\n",
        "NUM_CHAR = len(CHAR_LIST) + 1 #since we include the blank character\n",
        "\n",
        "#Hyperparameters\n",
        "BATCH_SIZE = 32\n",
        "LR = 0.001\n",
        "WEIGHT_DECAY = 0.0001\n",
        "NUM_EPOCHS = 60\n",
        "SAVE_NAME = f\"n_epochs_{NUM_EPOCHS}_bs_{BATCH_SIZE}_LR_{LR}_wd_{WEIGHT_DECAY}\"\n",
        "\n",
        "cnn_ctc_model = CNN_CTC_model(num_char=NUM_CHAR, hidden_size=256)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Grayscale(),\n",
        "    transforms.Resize((48, 144)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "preprocess_dataset = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "#Load the cnnctc model for the second part\n",
        "print(f\"models/CNNCTC-{SAVE_NAME}.pth\")\n",
        "\n",
        "if os.path.exists(f\"models/CNNCTC-{SAVE_NAME}.pth\"):\n",
        "    cnn_ctc_model.load_state_dict(torch.load(f\"models/CNNCTC-{SAVE_NAME}.pth\"))\n",
        "    cnn_ctc_model.to(device)\n",
        "else:\n",
        "    print(\"model not found. Please train the model first\")\n",
        "\n",
        "#Initialize the path for the languages\n",
        "image_paths = Path(f\"dataset/images/test\")\n",
        "\n",
        "evaluator = Evaluator(idx2char=IDX_CHAR)\n",
        "cnn_ctc_model.eval()\n",
        "plate_accuracies = []\n",
        "char_accuracies = []\n",
        "iou_scores = []\n",
        "\n",
        "i=0\n",
        "for image_path in image_paths.glob(\"*.jpg\"):\n",
        "    if i % 10 == 0:\n",
        "        print(f\"processing image {i+1}/{55000}\")\n",
        "    image_name = image_path.name\n",
        "    plate_label_path = Path(f\"dataset/labels_pdlpr/test/\") / (image_path.stem + \".txt\")\n",
        "\n",
        "    with open(plate_label_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        pdlpr_plate_str = f.readline().strip()\n",
        "    #print(pdlpr_plate_str)\n",
        "\n",
        "    fields = image_path.stem.split(\"-\")\n",
        "    bbox_part = fields[2]\n",
        "    corners = bbox_part.split(\"_\")\n",
        "    x1, y1 = map(int, corners[0].split(\"&\"))\n",
        "    x2, y2 = map(int, corners[1].split(\"&\"))\n",
        "\n",
        "    true_box = [x1, y1, x2, y2]\n",
        "    #print(true_box)\n",
        "    plate_number = fields[4]\n",
        "    character_id_list = plate_number.split(\"_\")\n",
        "    plate_id = []\n",
        "    for c in character_id_list:\n",
        "        plate_id.append(int(c))\n",
        "\n",
        "    #converting the index from the name to the index from the\n",
        "    #unified vocabulary\n",
        "    plate_id= target_to_index(plate_id)\n",
        "    #print(plate_id)\n",
        "\n",
        "    true_plate_idx = torch.tensor(plate_id, dtype=torch.long).to(device)\n",
        "\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    bbx =[]\n",
        "    result_detector = plate_detector(image_path, true_box)\n",
        "    if result_detector is None:\n",
        "        print(f\"image {i}: No plate detected\")\n",
        "        i+=1\n",
        "        continue\n",
        "\n",
        "    bbx, text = result_detector\n",
        "    print(i, bbx, text)\n",
        "    iou = compute_iou(bbx, true_box)\n",
        "    print(f\"image {i}: iou score {iou}\")\n",
        "    iou_scores.append(iou)\n",
        "    x1,y1, x2, y2 = bbx\n",
        "    cropped_image = image.crop((x1, y1, x2, y2))\n",
        "\n",
        "    #import matplotlib.pyplot as plt\n",
        "    plt.imshow(cropped_image, cmap=\"gray\")\n",
        "    plt.title(\"Cropped Plate\")\n",
        "    plt.axis(\"off\")\n",
        "    #plt.show()\n",
        "\n",
        "    processed_image =preprocess(cropped_image).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        #computing the predictions wiht the cnn ctc model\n",
        "        logits_model_output = cnn_ctc_model(processed_image)\n",
        "        evaluator.reset()\n",
        "        evaluator.update_baseline(logits_model_output, [true_plate_idx])\n",
        "        metrics = evaluator.compute()\n",
        "\n",
        "        char_acc = metrics[\"char_accuracy\"]\n",
        "        plate_acc = metrics[\"seq_accuracy\"]\n",
        "        char_accuracies.append(char_acc)\n",
        "        plate_accuracies.append(plate_acc)\n",
        "        print(f\"  Char acc: {char_acc:.2f}, Seq acc: {plate_acc:.2f}\\n\")\n",
        "        plate_prediction = evaluator.greedy_decode_idx(logits_model_output)[0]\n",
        "        print(plate_prediction[:8])\n",
        "        plate_string = index_to_target(plate_prediction[:8])\n",
        "        print(f\"predicted_plate: {''.join(plate_string)}, original plate: {pdlpr_plate_str}\")\n",
        "\n",
        "    i+=1\n",
        "\n",
        "mean_char_acc = sum(char_accuracies) / len(char_accuracies)\n",
        "mean_plate_acc = sum(plate_accuracies)/len(plate_accuracies)\n",
        "mean_iou = sum(iou_scores)/len(iou_scores)\n",
        "print(f\"Pipeline test result plate accuracy: {mean_plate_acc:.4f}\")\n",
        "print(f\"Pipeline test result char accuracy: {mean_char_acc:.4f}\")\n",
        "print(f\"Pipeline test result iou score: {mean_iou:.4f}\")\n",
        "#saving the iou result of the training, validation (last step) and testing\n",
        "with open(f\"results/pipeline-baseline-test-{SAVE_NAME}.txt\", \"w\") as f:\n",
        "    f.write(f\"Final testing plate accuracy: {mean_plate_acc:.4f}\\n\")\n",
        "    f.write(f\"Final testing character accuracy: {mean_char_acc:.4f}\\n\")\n",
        "    f.write(f\"Final testing iou score: {mean_iou:.4f}\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "bRoLLAHCMhTG",
        "E_PZ6aiSU_Jk",
        "_mR_Eir2Mhju",
        "L1KTU3FkMhmW",
        "1gHQ8bVQxCkG",
        "DA_c11Er4uEj",
        "KnbuZU1I5ZGa",
        "Hnmq-soL5osZ",
        "JUYEDzyHxIDn",
        "d8HBAOmm6Otb",
        "W58PEoMpSRgV",
        "t7HqGhcYSZXr",
        "pGug41Nsb-8M",
        "AhGg5Gg6xSG4"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
